{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import movingpandas as mpd\n",
    "from datetime import timedelta\n",
    "from pyproj import CRS\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "logging.basicConfig(filename='log_read_freemove.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data read.\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "raw_points_gdf = gp.read_file(\"W:/Master-Thesis-Repository/data/freemove_dlr_data/od_points.geojson\")\n",
    "print('Data read.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to match geolife point dataset\n",
    "raw_points_gdf = raw_points_gdf.rename(columns={'tid': 'traj_id', \n",
    "                                                'datetime': 'time', \n",
    "                                                'lng': 'lon',\n",
    "                                                'lat': 'lat',\n",
    "                                                'uid': 'user'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geojson(gdf, path):\n",
    "    assert isinstance(gdf, gp.GeoDataFrame)\n",
    "    gdf.to_file(path, driver='GeoJSON')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing points outside valid lon and lat range...\n",
      "points removed.\n"
     ]
    }
   ],
   "source": [
    "print('removing points outside valid lon and lat range...')\n",
    "# Remove points that fall outside the valid lon and lat range (-90 to 90 for latitude and -180 to 180 for longitude)\n",
    "raw_points_gdf = raw_points_gdf[(raw_points_gdf.lat >= -90) & (raw_points_gdf.lat <= 90) & (raw_points_gdf.lon >= -180) & (raw_points_gdf.lon <= 180)]\n",
    "print('points removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping NA values...\n",
      "NA values dropped.\n"
     ]
    }
   ],
   "source": [
    "print('Dropping NA values...')\n",
    "len_before = len(raw_points_gdf)\n",
    "logging.info('Rows before dropping NA values: {}'.format(len(raw_points_gdf)))\n",
    "raw_points_gdf.dropna(inplace=True)\n",
    "logging.info('Rows after dropping NA values: {}'.format(len(raw_points_gdf)))\n",
    "logging.info('Rows dropped: {}'.format(len_before - len(raw_points_gdf)))\n",
    "print('NA values dropped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trajectory collection...\n",
      "trajectory collection created.\n"
     ]
    }
   ],
   "source": [
    "print('creating trajectory collection...')\n",
    "# Create trajectory collection\n",
    "raw_full_trip_collection = mpd.TrajectoryCollection(raw_points_gdf, traj_id_col='traj_id', obj_id_col ='user', t='time', x='lon', y='lat')\n",
    "print('trajectory collection created.')\n",
    "\n",
    "logging.info(f'This is a test log of traj id: {raw_full_trip_collection.trajectories[0].id}')\n",
    "logging.info(f'Number of trajectories in data: {len(raw_full_trip_collection.trajectories)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to EPSG:3035...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0465s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.1229s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-2)]: Done  22 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done  40 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-2)]: Done  74 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-2)]: Done 118 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-2)]: Done 162 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-2)]: Done 214 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-2)]: Done 266 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-2)]: Done 326 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-2)]: Done 386 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-2)]: Done 454 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-2)]: Done 522 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-2)]: Done 598 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-2)]: Done 674 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-2)]: Done 758 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-2)]: Done 842 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-2)]: Done 934 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-2)]: Done 1026 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-2)]: Done 1126 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-2)]: Done 1226 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-2)]: Done 1334 tasks      | elapsed:   13.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted to EPSG:3035 (Berlin).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done 1408 out of 1408 | elapsed:   14.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Convert to EPSG\n",
    "def convert_epsg(traj):\n",
    "    result = traj.to_crs(CRS(3035))\n",
    "    result.obj_id = traj.obj_id\n",
    "    return result\n",
    "print('converting to EPSG:3035...')\n",
    "raw_full_trip_collection.trajectories = Parallel(n_jobs=-2, verbose=10)(delayed(convert_epsg)(traj) for traj in raw_full_trip_collection.trajectories)\n",
    "print('converted to EPSG:3035 (Berlin).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:07<00:00, 192.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories split.\n"
     ]
    }
   ],
   "source": [
    "# Split trajectories\n",
    "\n",
    "def split_traj(traj, MAX_DIAMETER=100, MIN_DURATION=timedelta(minutes=15), MIN_LENGTH=500):\n",
    "    try:\n",
    "        split = mpd.StopSplitter(traj).split(max_diameter=MAX_DIAMETER, min_duration=MIN_DURATION, min_length=MIN_LENGTH)\n",
    "        for i in range(len(split.trajectories)):\n",
    "            split.trajectories[i].obj_id = traj.obj_id\n",
    "        return split.trajectories\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        logging.warning(f'{e} Error at traj: {traj.id}')\n",
    "\n",
    "        return []\n",
    "\n",
    "print('splitting trajectories...')\n",
    "# split_trajs = Parallel(n_jobs=4, verbose=10)(delayed(split_traj)(traj) for traj in geolife_raw_collection.trajectories)\n",
    "split_trajs = []\n",
    "for traj in tqdm(raw_full_trip_collection.trajectories):\n",
    "    try:\n",
    "        split_trajs.append(split_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "    \n",
    "split_trajs = [traj for sublist in split_trajs for traj in sublist]\n",
    "freemove_splitted_collection = mpd.TrajectoryCollection(split_trajs)\n",
    "print('trajectories split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:19<00:00, 74.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories smoothed.\n"
     ]
    }
   ],
   "source": [
    "# Smooth trajectories\n",
    "def smooth_traj(traj, PROCESS_NOISE_STD=0.1, MEASUREMENT_NOISE_STD=10):\n",
    "    try:\n",
    "        result = mpd.KalmanSmootherCV(traj).smooth(process_noise_std=PROCESS_NOISE_STD, measurement_noise_std=MEASUREMENT_NOISE_STD)\n",
    "        result.obj_id = traj.obj_id\n",
    "        return result\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        logging.warning(f'{e} Error at traj: {traj.id}')\n",
    "        return traj\n",
    "\n",
    "print('smoothing trajectories...')\n",
    "# geolife_splitted_smooth_collection = Parallel(n_jobs=4, verbose=10)(delayed(smooth_traj)(traj) for traj in geolife_splitted_collection.trajectories)\n",
    "freemove_smooth_collection = []\n",
    "for traj in tqdm(raw_full_trip_collection.trajectories):\n",
    "    try:\n",
    "        freemove_smooth_collection.append(smooth_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "\n",
    "freemove_smooth_collection = mpd.TrajectoryCollection(freemove_smooth_collection)\n",
    "print('trajectories smoothed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalizing trajectories... (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:05<00:00, 280.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories generalized. (1)\n"
     ]
    }
   ],
   "source": [
    "# Generalize trajectories\n",
    "def generalize_traj(traj, TOLERANCE=1.0):\n",
    "    try:\n",
    "        result = mpd.DouglasPeuckerGeneralizer(traj).generalize(tolerance=TOLERANCE)\n",
    "        result.obj_id = traj.obj_id\n",
    "        return result\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        logging.warning(f'{e} Error at traj: {traj.id}')\n",
    "        return traj\n",
    "\n",
    "# Douglas-Peucker generalization for non-smoothed trajectories\n",
    "print('generalizing trajectories... (1)')\n",
    "logging.info('generalizing trajectories... (1)')\n",
    "# geolife_splitted_generalized_collection = Parallel(n_jobs=4, verbose=10)(delayed(generalize_traj)(traj) for traj in geolife_splitted_collection.trajectories)\n",
    "freemove_generalized_collection = []\n",
    "for traj in tqdm(raw_full_trip_collection.trajectories):\n",
    "    try:\n",
    "        freemove_generalized_collection.append(generalize_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "freemove_generalized_collection = mpd.TrajectoryCollection(freemove_generalized_collection)\n",
    "print('trajectories generalized. (1)')\n",
    "logging.info('trajectories generalized. (1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalizing trajectories... (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:03<00:00, 415.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories generalized. (2)\n"
     ]
    }
   ],
   "source": [
    "# Douglas-Peucker generalization for smoothed trajectories\n",
    "print('generalizing trajectories... (2)')\n",
    "logging.info('generalizing trajectories... (2)')\n",
    "# geolife_splitted_smooth_generalized_collection = Parallel(n_jobs=4, verbose=10)(delayed(generalize_traj)(traj) for traj in geolife_splitted_smooth_collection.trajectories)\n",
    "freemove_smooth_generalized_collection = []\n",
    "for traj in tqdm(freemove_smooth_collection.trajectories):\n",
    "    try:\n",
    "        freemove_smooth_generalized_collection.append(generalize_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "\n",
    "freemove_smooth_generalized_collection = mpd.TrajectoryCollection(freemove_smooth_generalized_collection)\n",
    "print('trajectories generalized. (2)')\n",
    "logging.info('trajectories generalized. (2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trajcollection_to_gdf(trajcollection):\n",
    "    gdfs = []\n",
    "    for traj in tqdm(trajcollection.trajectories):\n",
    "        traj_gdf = traj.to_traj_gdf()\n",
    "        traj_gdf['user_id'] = traj.obj_id\n",
    "        gdfs.append(traj_gdf)\n",
    "\n",
    "    gdf = gp.GeoDataFrame(pd.concat(gdfs), crs='EPSG:3035')\n",
    "\n",
    "    return gp.GeoDataFrame(pd.concat(gdfs), crs='EPSG:3035')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:02<00:00, 508.66it/s]\n",
      "100%|██████████| 1024/1024 [00:01<00:00, 618.34it/s]\n",
      "100%|██████████| 1408/1408 [00:02<00:00, 549.02it/s]\n",
      "100%|██████████| 1408/1408 [00:02<00:00, 584.38it/s]\n",
      "100%|██████████| 1408/1408 [00:02<00:00, 526.18it/s]\n"
     ]
    }
   ],
   "source": [
    "logging.info('converting to gdf...')\n",
    "freemove_raw = convert_trajcollection_to_gdf(raw_full_trip_collection)\n",
    "freemove_splitted = convert_trajcollection_to_gdf(freemove_splitted_collection)\n",
    "freemove_smooth = convert_trajcollection_to_gdf(freemove_smooth_collection)\n",
    "freemove_generalized = convert_trajcollection_to_gdf(freemove_generalized_collection)\n",
    "freemove_smooth_generalized = convert_trajcollection_to_gdf(freemove_smooth_generalized_collection)\n",
    "logging.info('converted to gdf.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "# Write gdf to pickle file to load fast for further processing\n",
    "logging.info('writing to geojson...')\n",
    "write_geojson(freemove_raw, '../data/freemove/freemove_raw.geojson')\n",
    "write_geojson(freemove_splitted, '../data/freemove/freemove_splitted.geojson')\n",
    "write_geojson(freemove_smooth, '../data/freemove/freemove_smooth.geojson')\n",
    "write_geojson(freemove_generalized, '../data/freemove/freemove_generalized.geojson')\n",
    "write_geojson(freemove_smooth_generalized, '../data/freemove/freemove_smooth_generalized.geojson')\n",
    "\n",
    "# Save point gdf as geojson\n",
    "freemove_point_gdf = raw_full_trip_collection.to_point_gdf().reset_index()\n",
    "write_geojson(freemove_point_gdf, '../data/freemove/freemove_raw_point.geojson')\n",
    "freemove_smoothed_generalized_point = freemove_smooth_generalized_collection.to_point_gdf().reset_index()\n",
    "write_geojson(freemove_smoothed_generalized_point, '../data/freemove/freemove_smooth_generalized_point.geojson')\n",
    "logging.info('written to geojson.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "543daf12f525df94f20bbdd448da69881f98a71c963c44c9c7818e0113666227"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

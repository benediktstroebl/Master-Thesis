{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import movingpandas as mpd\n",
    "from datetime import timedelta\n",
    "from pyproj import CRS\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "from shapely import Point\n",
    "from datetime import datetime\n",
    "logging.basicConfig(filename='log_read_freemove.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data read.\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "raw_points_gdf = gp.read_file('../data/freemove/raw_full.geojson', crs='EPSG:4326')\n",
    "print('Data read.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4958/4958 [00:51<00:00, 96.58it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>TIME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16348</td>\n",
       "      <td>985222</td>\n",
       "      <td>2022-10-31 09:15:57</td>\n",
       "      <td>52.454451</td>\n",
       "      <td>13.504967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16348</td>\n",
       "      <td>985222</td>\n",
       "      <td>2022-10-31 09:15:58</td>\n",
       "      <td>52.454463</td>\n",
       "      <td>13.504983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16348</td>\n",
       "      <td>985222</td>\n",
       "      <td>2022-10-31 09:15:59</td>\n",
       "      <td>52.454454</td>\n",
       "      <td>13.505009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16348</td>\n",
       "      <td>985222</td>\n",
       "      <td>2022-10-31 09:16:00</td>\n",
       "      <td>52.454422</td>\n",
       "      <td>13.505042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16348</td>\n",
       "      <td>985222</td>\n",
       "      <td>2022-10-31 09:16:01</td>\n",
       "      <td>52.454391</td>\n",
       "      <td>13.505098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637556</th>\n",
       "      <td>17273</td>\n",
       "      <td>1012443</td>\n",
       "      <td>2022-11-15 10:00:46</td>\n",
       "      <td>52.501937</td>\n",
       "      <td>13.355693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637557</th>\n",
       "      <td>17273</td>\n",
       "      <td>1012443</td>\n",
       "      <td>2022-11-15 10:00:47</td>\n",
       "      <td>52.501903</td>\n",
       "      <td>13.355708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637558</th>\n",
       "      <td>17273</td>\n",
       "      <td>1012443</td>\n",
       "      <td>2022-11-15 10:00:48</td>\n",
       "      <td>52.501874</td>\n",
       "      <td>13.355723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637559</th>\n",
       "      <td>17273</td>\n",
       "      <td>1012443</td>\n",
       "      <td>2022-11-15 10:00:49</td>\n",
       "      <td>52.501847</td>\n",
       "      <td>13.355738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637560</th>\n",
       "      <td>17273</td>\n",
       "      <td>1012443</td>\n",
       "      <td>2022-11-15 10:00:50</td>\n",
       "      <td>52.501821</td>\n",
       "      <td>13.355753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1637561 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PERSON_ID  TRIP_ID                TIME        LAT        LON\n",
       "0            16348   985222 2022-10-31 09:15:57  52.454451  13.504967\n",
       "1            16348   985222 2022-10-31 09:15:58  52.454463  13.504983\n",
       "2            16348   985222 2022-10-31 09:15:59  52.454454  13.505009\n",
       "3            16348   985222 2022-10-31 09:16:00  52.454422  13.505042\n",
       "4            16348   985222 2022-10-31 09:16:01  52.454391  13.505098\n",
       "...            ...      ...                 ...        ...        ...\n",
       "1637556      17273  1012443 2022-11-15 10:00:46  52.501937  13.355693\n",
       "1637557      17273  1012443 2022-11-15 10:00:47  52.501903  13.355708\n",
       "1637558      17273  1012443 2022-11-15 10:00:48  52.501874  13.355723\n",
       "1637559      17273  1012443 2022-11-15 10:00:49  52.501847  13.355738\n",
       "1637560      17273  1012443 2022-11-15 10:00:50  52.501821  13.355753\n",
       "\n",
       "[1637561 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create geojson of points\n",
    "rows = []\n",
    "for i, row in tqdm(raw_points_gdf.iterrows(), total=len(raw_points_gdf), miniters=10):\n",
    "    person_id = row.PERSON_ID\n",
    "    trip_id = row.TRIP_ID\n",
    "    leg_id = row.LEG_ID\n",
    "    leg_start = datetime.strptime(str(row.LEG_START), '%Y-%m-%dT%H:%M:%S')\n",
    "    leg_end = datetime.strptime(str(row.LEG_END), '%Y-%m-%dT%H:%M:%S')\n",
    "    trip_purpose_ids = row.TRIP_PURPOSE_IDS\n",
    "    traffic_mode = row.TRAFFIC_MODE\n",
    "    leg_len_in_mtrs = row.LEG_LEN_IN_MTRS\n",
    "    leg_duration_in_secs = row.LEG_DURATION_IN_SECS\n",
    "\n",
    "    # interpolate time of points of a leg since we only have start and end time\n",
    "    total_points_of_leg = len(row.geometry.coords)\n",
    "    interpolattion_increment = leg_duration_in_secs / total_points_of_leg\n",
    "\n",
    "    for point_i, point in enumerate(row.geometry.coords):\n",
    "        new_row = {\"PERSON_ID\": person_id, \n",
    "                   \"TRIP_ID\": trip_id, \n",
    "                   \"LEG_ID\": leg_id, \n",
    "                   \"LEG_START\": leg_start, \n",
    "                   \"LEG_END\": leg_end,\n",
    "                   \"TIME\": leg_start + timedelta(seconds=round(interpolattion_increment * point_i, ndigits=0)),\n",
    "                   \"TRIP_PURPOSE_IDS\": trip_purpose_ids, \n",
    "                   \"TRAFFIC_MODE\": traffic_mode, \n",
    "                   \"LEG_LEN_IN_MTRS\": leg_len_in_mtrs, \n",
    "                   \"LEG_DURATION_IN_SECS\": leg_duration_in_secs, \n",
    "                   \"LAT\": Point(point).y,\n",
    "                   \"LON\": Point(point).x}\n",
    "        rows.append(new_row)\n",
    "\n",
    "\n",
    "raw_points_gdf = pd.DataFrame(rows)[[\"PERSON_ID\", \"TRIP_ID\", \"TIME\", \"LAT\", \"LON\"]]\n",
    "raw_points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to match geolife point dataset\n",
    "raw_points_gdf = raw_points_gdf.rename(columns={'TRIP_ID': 'traj_id', \n",
    "                                                'TIME': 'time', \n",
    "                                                'LON': 'lon',\n",
    "                                                'LAT': 'lat',\n",
    "                                                'PERSON_ID': 'user'}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geojson(gdf, path):\n",
    "    assert isinstance(gdf, gp.GeoDataFrame)\n",
    "    gdf.to_file(path, driver='GeoJSON')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing points outside valid lon and lat range...\n",
      "points removed.\n"
     ]
    }
   ],
   "source": [
    "print('removing points outside valid lon and lat range...')\n",
    "# Remove points that fall outside the valid lon and lat range (-90 to 90 for latitude and -180 to 180 for longitude)\n",
    "raw_points_gdf = raw_points_gdf[(raw_points_gdf.lat >= -90) & (raw_points_gdf.lat <= 90) & (raw_points_gdf.lon >= -180) & (raw_points_gdf.lon <= 180)]\n",
    "print('points removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping NA values...\n",
      "NA values dropped.\n"
     ]
    }
   ],
   "source": [
    "print('Dropping NA values...')\n",
    "len_before = len(raw_points_gdf)\n",
    "logging.info('Rows before dropping NA values: {}'.format(len(raw_points_gdf)))\n",
    "raw_points_gdf.dropna(inplace=True)\n",
    "logging.info('Rows after dropping NA values: {}'.format(len(raw_points_gdf)))\n",
    "logging.info('Rows dropped: {}'.format(len_before - len(raw_points_gdf)))\n",
    "print('NA values dropped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trajectory collection...\n",
      "trajectory collection created.\n"
     ]
    }
   ],
   "source": [
    "print('creating trajectory collection...')\n",
    "# Create trajectory collection\n",
    "raw_full_trip_collection = mpd.TrajectoryCollection(raw_points_gdf, traj_id_col='traj_id', obj_id_col ='user', t='time', x='lon', y='lat')\n",
    "print('trajectory collection created.')\n",
    "\n",
    "logging.info(f'This is a test log of traj id: {raw_full_trip_collection.trajectories[0].id}')\n",
    "logging.info(f'Number of trajectories in data: {len(raw_full_trip_collection.trajectories)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to EPSG:3035...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-2)]: Done  18 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-2)]: Done  27 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-2)]: Done  47 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-2)]: Done  58 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-2)]: Done  71 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-2)]: Done  84 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-2)]: Done  99 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-2)]: Done 114 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-2)]: Done 131 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-2)]: Done 148 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-2)]: Done 167 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-2)]: Done 186 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-2)]: Done 207 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-2)]: Done 228 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-2)]: Done 251 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-2)]: Done 274 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-2)]: Done 299 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-2)]: Done 324 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-2)]: Done 351 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-2)]: Done 378 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-2)]: Done 407 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-2)]: Done 436 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-2)]: Done 467 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-2)]: Done 498 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-2)]: Done 531 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-2)]: Done 564 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-2)]: Done 599 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-2)]: Done 634 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-2)]: Done 671 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-2)]: Done 708 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-2)]: Done 747 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-2)]: Done 786 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-2)]: Done 827 tasks      | elapsed:   50.8s\n",
      "[Parallel(n_jobs=-2)]: Done 868 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-2)]: Done 911 tasks      | elapsed:   55.1s\n",
      "[Parallel(n_jobs=-2)]: Done 954 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-2)]: Done 999 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=-2)]: Done 1044 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-2)]: Done 1091 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-2)]: Done 1138 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-2)]: Done 1187 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-2)]: Done 1236 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-2)]: Done 1287 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-2)]: Done 1338 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-2)]: Done 1391 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-2)]: Done 1408 out of 1408 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted to EPSG:3035 (Berlin).\n"
     ]
    }
   ],
   "source": [
    "# Convert to EPSG\n",
    "def convert_epsg(traj):\n",
    "    result = traj.to_crs(CRS(3035))\n",
    "    result.obj_id = traj.obj_id\n",
    "    return result\n",
    "print('converting to EPSG:3035...')\n",
    "raw_full_trip_collection.trajectories = Parallel(n_jobs=-2, verbose=10)(delayed(convert_epsg)(traj) for traj in raw_full_trip_collection.trajectories)\n",
    "print('converted to EPSG:3035 (Berlin).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1408 [00:40<58:30,  2.51s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error at traj:  985127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 18/1408 [01:13<1:12:34,  3.13s/it]"
     ]
    }
   ],
   "source": [
    "# Split trajectories\n",
    "\n",
    "def split_traj(traj, MAX_DIAMETER=100, MIN_DURATION=timedelta(minutes=15), MIN_LENGTH=500):\n",
    "    try:\n",
    "        split = mpd.StopSplitter(traj).split(max_diameter=MAX_DIAMETER, min_duration=MIN_DURATION, min_length=MIN_LENGTH)\n",
    "        for i in range(len(split.trajectories)):\n",
    "            split.trajectories[i].obj_id = traj.obj_id\n",
    "        return split.trajectories\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        logging.warning(f'{e} Error at traj: {traj.id}')\n",
    "\n",
    "        return []\n",
    "\n",
    "print('splitting trajectories...')\n",
    "# split_trajs = Parallel(n_jobs=4, verbose=10)(delayed(split_traj)(traj) for traj in geolife_raw_collection.trajectories)\n",
    "split_trajs = []\n",
    "for traj in tqdm(raw_full_trip_collection.trajectories):\n",
    "    try:\n",
    "        split_trajs.append(split_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "    \n",
    "split_trajs = [traj for sublist in split_trajs for traj in sublist]\n",
    "freemove_splitted_collection = mpd.TrajectoryCollection(split_trajs)\n",
    "print('trajectories split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:19<00:00, 74.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories smoothed.\n"
     ]
    }
   ],
   "source": [
    "# Smooth trajectories\n",
    "def smooth_traj(traj, PROCESS_NOISE_STD=0.1, MEASUREMENT_NOISE_STD=10):\n",
    "    try:\n",
    "        result = mpd.KalmanSmootherCV(traj).smooth(process_noise_std=PROCESS_NOISE_STD, measurement_noise_std=MEASUREMENT_NOISE_STD)\n",
    "        result.obj_id = traj.obj_id\n",
    "        return result\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        logging.warning(f'{e} Error at traj: {traj.id}')\n",
    "        return traj\n",
    "\n",
    "print('smoothing trajectories...')\n",
    "# geolife_splitted_smooth_collection = Parallel(n_jobs=4, verbose=10)(delayed(smooth_traj)(traj) for traj in geolife_splitted_collection.trajectories)\n",
    "freemove_smooth_collection = []\n",
    "for traj in tqdm(raw_full_trip_collection.trajectories):\n",
    "    try:\n",
    "        freemove_smooth_collection.append(smooth_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "\n",
    "freemove_smooth_collection = mpd.TrajectoryCollection(freemove_smooth_collection)\n",
    "print('trajectories smoothed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalizing trajectories... (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:05<00:00, 280.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories generalized. (1)\n"
     ]
    }
   ],
   "source": [
    "# Generalize trajectories\n",
    "def generalize_traj(traj, TOLERANCE=1.0):\n",
    "    try:\n",
    "        result = mpd.DouglasPeuckerGeneralizer(traj).generalize(tolerance=TOLERANCE)\n",
    "        result.obj_id = traj.obj_id\n",
    "        return result\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        logging.warning(f'{e} Error at traj: {traj.id}')\n",
    "        return traj\n",
    "\n",
    "# Douglas-Peucker generalization for non-smoothed trajectories\n",
    "print('generalizing trajectories... (1)')\n",
    "logging.info('generalizing trajectories... (1)')\n",
    "# geolife_splitted_generalized_collection = Parallel(n_jobs=4, verbose=10)(delayed(generalize_traj)(traj) for traj in geolife_splitted_collection.trajectories)\n",
    "freemove_generalized_collection = []\n",
    "for traj in tqdm(raw_full_trip_collection.trajectories):\n",
    "    try:\n",
    "        freemove_generalized_collection.append(generalize_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "freemove_generalized_collection = mpd.TrajectoryCollection(freemove_generalized_collection)\n",
    "print('trajectories generalized. (1)')\n",
    "logging.info('trajectories generalized. (1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalizing trajectories... (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:03<00:00, 415.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories generalized. (2)\n"
     ]
    }
   ],
   "source": [
    "# Douglas-Peucker generalization for smoothed trajectories\n",
    "print('generalizing trajectories... (2)')\n",
    "logging.info('generalizing trajectories... (2)')\n",
    "# geolife_splitted_smooth_generalized_collection = Parallel(n_jobs=4, verbose=10)(delayed(generalize_traj)(traj) for traj in geolife_splitted_smooth_collection.trajectories)\n",
    "freemove_smooth_generalized_collection = []\n",
    "for traj in tqdm(freemove_smooth_collection.trajectories):\n",
    "    try:\n",
    "        freemove_smooth_generalized_collection.append(generalize_traj(traj))\n",
    "    except BaseException as e:\n",
    "        print(e, 'Error at traj: ', traj.id)\n",
    "        continue\n",
    "\n",
    "freemove_smooth_generalized_collection = mpd.TrajectoryCollection(freemove_smooth_generalized_collection)\n",
    "print('trajectories generalized. (2)')\n",
    "logging.info('trajectories generalized. (2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trajcollection_to_gdf(trajcollection):\n",
    "    gdfs = []\n",
    "    for traj in tqdm(trajcollection.trajectories):\n",
    "        traj_gdf = traj.to_traj_gdf()\n",
    "        traj_gdf['user_id'] = traj.obj_id\n",
    "        gdfs.append(traj_gdf)\n",
    "\n",
    "    gdf = gp.GeoDataFrame(pd.concat(gdfs), crs='EPSG:3035')\n",
    "\n",
    "    return gp.GeoDataFrame(pd.concat(gdfs), crs='EPSG:3035')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1408/1408 [00:02<00:00, 508.66it/s]\n",
      "100%|██████████| 1024/1024 [00:01<00:00, 618.34it/s]\n",
      "100%|██████████| 1408/1408 [00:02<00:00, 549.02it/s]\n",
      "100%|██████████| 1408/1408 [00:02<00:00, 584.38it/s]\n",
      "100%|██████████| 1408/1408 [00:02<00:00, 526.18it/s]\n"
     ]
    }
   ],
   "source": [
    "logging.info('converting to gdf...')\n",
    "freemove_raw = convert_trajcollection_to_gdf(raw_full_trip_collection)\n",
    "freemove_splitted = convert_trajcollection_to_gdf(freemove_splitted_collection)\n",
    "freemove_smooth = convert_trajcollection_to_gdf(freemove_smooth_collection)\n",
    "freemove_generalized = convert_trajcollection_to_gdf(freemove_generalized_collection)\n",
    "freemove_smooth_generalized = convert_trajcollection_to_gdf(freemove_smooth_generalized_collection)\n",
    "logging.info('converted to gdf.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "# Write gdf to pickle file to load fast for further processing\n",
    "logging.info('writing to geojson...')\n",
    "write_geojson(freemove_raw, '../data/freemove/freemove_raw.geojson')\n",
    "write_geojson(freemove_splitted, '../data/freemove/freemove_splitted.geojson')\n",
    "write_geojson(freemove_smooth, '../data/freemove/freemove_smooth.geojson')\n",
    "write_geojson(freemove_generalized, '../data/freemove/freemove_generalized.geojson')\n",
    "write_geojson(freemove_smooth_generalized, '../data/freemove/freemove_smooth_generalized.geojson')\n",
    "\n",
    "# Save point gdf as geojson\n",
    "freemove_point_gdf = raw_full_trip_collection.to_point_gdf().reset_index()\n",
    "write_geojson(freemove_point_gdf, '../data/freemove/freemove_raw_point.geojson')\n",
    "freemove_smoothed_generalized_point = freemove_smooth_generalized_collection.to_point_gdf().reset_index()\n",
    "write_geojson(freemove_smoothed_generalized_point, '../data/freemove/freemove_smooth_generalized_point.geojson')\n",
    "logging.info('written to geojson.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "543daf12f525df94f20bbdd448da69881f98a71c963c44c9c7818e0113666227"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

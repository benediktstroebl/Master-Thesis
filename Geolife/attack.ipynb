{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import libpysal\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data frames (~ 40 secs)\n",
    "import load_geolife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data \n",
    "raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf, tesselation_gdf = load_geolife.geolife_raw_full_trip_gdf, load_geolife.geolife_raw_sp_gdf, load_geolife.geolife_raw_ep_gdf, load_geolife.geolife_tesselation_gdf\n",
    "assert len(raw_full_trip_gdf) == len(raw_trip_sp_gdf) == len(raw_trip_ep_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_random_users_from_dataframes(n, raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf):\n",
    "    \"\"\"Select n random users from the dataset.\n",
    "\n",
    "    Args:\n",
    "        n (_type_): Number of users to select.\n",
    "    \"\"\"\n",
    "    # Select n random users\n",
    "    users = np.random.choice(raw_full_trip_gdf['PERSON_ID'].unique(), n, replace=False)\n",
    "    # Filter dataframes\n",
    "    raw_full_trip_gdf = raw_full_trip_gdf[raw_full_trip_gdf['PERSON_ID'].isin(users)]\n",
    "    raw_trip_sp_gdf = raw_trip_sp_gdf[raw_trip_sp_gdf['PERSON_ID'].isin(users)]\n",
    "    raw_trip_ep_gdf = raw_trip_ep_gdf[raw_trip_ep_gdf['PERSON_ID'].isin(users)]\n",
    "    return raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select n random person ids from the dataset\n",
    "n_users = 5\n",
    "raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf = select_n_random_users_from_dataframes(n_users, raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def getGroundTruth():\n",
    "    # Get ground truth labels\n",
    "    df = full_trip_gdf\n",
    "    df['ID'] = df.sort_values('TRIP_ID').groupby('PERSON_ID').ngroup() # Sort TRIP ID ascending and set cluster id corresponding to PERSON_ID\n",
    "    ground_truth = df.sort_values('TRIP_ID').ID.to_list()\n",
    "    return ground_truth\n",
    "\n",
    "def getContingencyMatrix(clustering):\n",
    "    # Get ground truth labels\n",
    "    ground_truth = getGroundTruth()\n",
    "    cm = metrics.cluster.contingency_matrix(ground_truth, clustering)\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm) # resort contingency matrix to maximize sum of diagonal\n",
    "    cm = cm[:, col_idx] # reorder columns based on indices\n",
    "    cm = cm[row_idx, :] # reorder rows based on indices\n",
    "    return cm\n",
    "\n",
    "def getConfusionMatrix(clustering):\n",
    "    # Get ground truth labels\n",
    "    ground_truth = getGroundTruth()\n",
    "    cm = metrics.confusion_matrix(ground_truth, clustering)\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm) # resort confusion matrix to maximize sum of diagonal\n",
    "    cm = cm[:, col_idx] # reorder columns based on indices\n",
    "    cm = cm[row_idx, :] # reorder rows based on indices\n",
    "    return cm\n",
    "\n",
    "def getAccuracyAtOne(ground_truth, clustering):\n",
    "    cm = getContingencyMatrix(clustering)\n",
    "    # Reorder rows and columns to maximize sum of diagonal\n",
    "    # Vgl: https://smorbieu.gitlab.io/accuracy-from-classification-to-clustering-evaluation/#:~:text=Computing%20accuracy%20for%20clustering%20can,the%20accuracy%20for%20clustering%20results.\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm)\n",
    "    return cm[row_idx,col_idx].sum()/cm.sum()\n",
    "\n",
    "def getPrecision(ground_truth, clustering):\n",
    "    cm = getContingencyMatrix(clustering)\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm) # resort confusion matrix to maximize sum of diagonal\n",
    "    cm = cm[:, col_idx] # reorder columns based on indices\n",
    "    cm = cm[row_idx, :] # reorder rows based on indices\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "    # Return unweighted mean of precision\n",
    "    return np.mean(precision)\n",
    "\n",
    "def getRecall(ground_truth, clustering):\n",
    "    cm = getContingencyMatrix(clustering)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "\n",
    "    # Return unweighted mean of recall\n",
    "    return np.mean(recall)\n",
    "\n",
    "def getF1Score(ground_truth, clustering):\n",
    "    precision = getPrecision(ground_truth, clustering)\n",
    "    recall = getRecall(ground_truth, clustering)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate(clustering):\n",
    "    # Get ground truth labels\n",
    "    ground_truth = getGroundTruth()\n",
    "\n",
    "    # Not symmetric and not accounting for chance\n",
    "    print(f\"Accuracy@1: {getAccuracyAtOne(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Precision: {getPrecision(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Recall: {getRecall(ground_truth, clustering):.3f}\")\n",
    "    print(f\"F1: {getF1Score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Homogeneity: {metrics.homogeneity_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Completeness: {metrics.completeness_score(ground_truth, clustering):.3f}\")\n",
    "\n",
    "    # All of these metrics are symmetric and some of them are accounting for chance depending on the number of classes and clusters present in the data\n",
    "    print(f\"V-measure: {metrics.v_measure_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Rand index: {metrics.rand_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"ARI: {metrics.adjusted_rand_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"MI: {metrics.mutual_info_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"NMI: {metrics.normalized_mutual_info_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"AMI: {metrics.adjusted_mutual_info_score(ground_truth, clustering):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn.metrics\n",
    "\n",
    "def LCSS(traj1_linestr, traj2_linestr, eps=10):\n",
    "    \"\"\"This function takes in two GeoSeries and takes the top entry linestring. It then calculates the Least Common Sub-Sequence metric for these two and returns the value.\n",
    "\n",
    "    Args:\n",
    "        traj1_linestr (_type_): _description_\n",
    "        traj2_linestr (_type_): _description_\n",
    "        eps (int, optional): This can be interpreted as the distance in meters between two points compared of the subsequences. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        _type_: float\n",
    "    \"\"\"\n",
    "    assert isinstance(traj1_linestr, gp.GeoSeries), f\"traj1_linestr is of type {type(traj1_linestr)}, need to be GeoSeries\"\n",
    "    assert isinstance(traj2_linestr, gp.GeoSeries), f\"traj2_linestr is of type {type(traj2_linestr)}, need to be GeoSeries\"\n",
    "    assert len(traj1_linestr) > 0, \"traj1_linestr is empty\"\n",
    "    assert len(traj2_linestr) > 0, \"traj2_linestr is empty\"\n",
    "    \n",
    "    s1 = traj1_linestr.iloc[0].coords\n",
    "    s2 = traj2_linestr.iloc[0].coords\n",
    "\n",
    "    return tslearn.metrics.lcss(s1, s2, eps=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Start Points (SP) and End Points (EP) with Tessellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_boundary_points_with_tessellation(raw_trip_sp_gdf, raw_trip_ep_gdf, tesselation_gdf):\n",
    "    \"\"\"This function matches the boundary points of the raw trips with the tesselation. \n",
    "\n",
    "    Args:\n",
    "        raw_trip_sp_gdf (_type_): _description_\n",
    "        raw_trip_ep_gdf (_type_): _description_\n",
    "        tesselation_gdf (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: This function returns two data frames, one for the start points and one for the end points. These data frames contain the tile_id of the tesselation that the point is located in.\n",
    "    \"\"\"\n",
    "    # SP\n",
    "    # Spatial join points to polygons\n",
    "    gdf_sp = gp.sjoin(\n",
    "        tesselation_gdf[[\"tile_id\", \"geometry\"]],\n",
    "        raw_trip_sp_gdf,\n",
    "        how=\"inner\"\n",
    "    ).drop('index_right', axis=1)\n",
    "\n",
    "    # Spatial join points to polygons\n",
    "    gdf_ep = gp.sjoin(\n",
    "        tesselation_gdf[[\"tile_id\", \"geometry\"]],\n",
    "        raw_trip_ep_gdf,\n",
    "        how=\"inner\"\n",
    "    ).drop('index_right', axis=1)\n",
    "\n",
    "    return gdf_sp, gdf_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_sp, gdf_ep = match_boundary_points_with_tessellation(raw_trip_sp_gdf, raw_trip_ep_gdf, tesselation_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Full Trips that Start and End within Tessellation Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips that start and end wihin tessellation area: 246\n",
      "Number of trips outside and therefore dropped: 39\n"
     ]
    }
   ],
   "source": [
    "gdf_sp_ids = gdf_sp.TRIP_ID\n",
    "gdf_ep_ids = gdf_ep.TRIP_ID\n",
    "\n",
    "full_trip_gdf = raw_full_trip_gdf.query(\"TRIP_ID in @gdf_sp_ids and TRIP_ID in @gdf_ep_ids\")\n",
    "trip_sp_gdf = raw_trip_sp_gdf.query(\"TRIP_ID in @gdf_ep_ids\")\n",
    "trip_ep_gdf = raw_trip_ep_gdf.query(\"TRIP_ID in @gdf_sp_ids\")\n",
    "\n",
    "gdf_sp = gdf_sp.query(\"TRIP_ID in @gdf_ep_ids\")\n",
    "gdf_ep = gdf_ep.query(\"TRIP_ID in @gdf_sp_ids\")\n",
    "\n",
    "assert len(full_trip_gdf) == len(trip_sp_gdf) == len(trip_ep_gdf) == len(gdf_sp) == len(gdf_ep) == len(set(trip_sp_gdf.TRIP_ID).intersection(set(trip_ep_gdf.TRIP_ID))) # this last intersection checks that for all unique trip ids we have exactly ONE SP and EP\n",
    "\n",
    "print(f\"Number of trips that start and end wihin tessellation area: {len(full_trip_gdf)}\")\n",
    "print(f\"Number of trips outside and therefore dropped: {len(raw_full_trip_gdf) - len(full_trip_gdf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = raw_trip_sp_gdf.query(\"TRIP_ID not in @gdf_ep_ids and TRIP_ID not in @gdf_sp_ids\").explore(color='red')\n",
    "raw_trip_ep_gdf.query(\"TRIP_ID not in @gdf_ep_ids and TRIP_ID not in @gdf_sp_ids\").explore(m=m, color='blue')\n",
    "raw_full_trip_gdf.query(\"TRIP_ID not in @gdf_ep_ids and TRIP_ID not in @gdf_sp_ids\").explore(m=m, color='black')\n",
    "tesselation_gdf.explore(m=m, color='green', style_kwds=dict(color=\"green\",weight=1, opacity=0.1, fillOpacity=.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build mapping of trip chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trip_chain_mapping(gdf_sp, gdf_ep, INFLOW_HR_DIFF_THRESHOLD=4, HR_DIFF_THRESHOLD=8):\n",
    "    \"\"\"This function returns a list of trip chains that are continued trips that happened subsequent to and from same tile within a given time threshold.\n",
    "\n",
    "    Args:\n",
    "        gdf_sp (_type_): _description_\n",
    "        gdf_ep (_type_): _description_\n",
    "        inflow_hr_diff_threshold (int, optional): _description_. Defaults to 4.\n",
    "        hr_diff_threshold (int, optional): _description_. Defaults to 8.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate mapping of continued trips that happened subsequent to and from same tile\n",
    "    mapping_cont_trips = []\n",
    "    for index, trip in tqdm(gdf_ep.sort_values('TRIP_ID').iterrows(), total=len(gdf_ep)):\n",
    "        te_1_id = trip.TRIP_ID\n",
    "        te_1_tid = trip.tile_id\n",
    "        te_1_dt = pd.to_datetime(trip['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "        ts_1_dt = pd.to_datetime(trip['TRIP_START'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        inflow = gdf_ep.query(\"tile_id == @te_1_tid\").copy()\n",
    "        inflow['TRIP_END'] = inflow.TRIP_END.astype('datetime64[ns]')\n",
    "        inflow['TRIP_START'] = inflow.TRIP_START.astype('datetime64[ns]')\n",
    "        inflow['INFLOW_HR_DIFF'] = inflow.TRIP_END.apply(lambda x: (x - te_1_dt).total_seconds()/3600)\n",
    "        inflow = inflow.query(\"(INFLOW_HR_DIFF <= @INFLOW_HR_DIFF_THRESHOLD) and (INFLOW_HR_DIFF >= @INFLOW_HR_DIFF_THRESHOLD)\") # Take trips \n",
    "        inflow = inflow.query(\"(TRIP_START > @te_1_dt) or (@ts_1_dt > TRIP_END)\") # Ignore trips that have happened simultaneously\n",
    "\n",
    "        # if more than one trip has arrived in +- hour window, then do not merge this trip\n",
    "        if len(inflow) > 1:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Get all trips that started from same tile as t_1 has ended in\n",
    "        ts_2 = gdf_sp.query(\"tile_id == @te_1_tid\").copy()\n",
    "\n",
    "        # get difference between two trips hours (seconds divided by 3600 gets hours)\n",
    "        ts_2['TRIP_START'] = ts_2.TRIP_START.astype('datetime64[ns]')\n",
    "        ts_2['TRIP_END'] = ts_2.TRIP_END.astype('datetime64[ns]')\n",
    "    \n",
    "        ts_2['hr_diff'] = ts_2['TRIP_START'].apply(lambda x: (x - te_1_dt).total_seconds()/3600)\n",
    "\n",
    "        # Only consider trips that started within a certain time after the initial trip ended in the same tessellation tile\n",
    "        ts_2 = ts_2[(ts_2['hr_diff'].astype(str).astype(float) <= HR_DIFF_THRESHOLD) & (ts_2['hr_diff'].astype(str).astype(float) >= 0)]\n",
    "\n",
    "        # Only consider trips that are not simultaneously\n",
    "        ts_2 = ts_2.query(\"(TRIP_START > @te_1_dt) or (@ts_1_dt > TRIP_END)\")\n",
    "\n",
    "        # Only consider connection if exactly one trip started from same tile in time window\n",
    "        if len(ts_2) == 1:\n",
    "            mapping_cont_trips.append({\n",
    "                'TRIP_ID': te_1_id,\n",
    "                'TRIP_ID_CONT': ts_2.TRIP_ID.iloc[0]\n",
    "            })\n",
    "\n",
    "    return mapping_cont_trips\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:04<00:00, 53.76it/s]\n"
     ]
    }
   ],
   "source": [
    "mapping_cont_trips = build_trip_chain_mapping(gdf_sp, gdf_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trip_chaining(mapping_cont_trips, full_trip_gdf):\n",
    "    \"\"\"This function evaluates the trip chaining by checking if the chained trips are from the same person.\n",
    "\n",
    "    Args:\n",
    "        mapping_cont_trips (_type_): Dictionary of trip ids that are chained. Output of build_trip_chain_mapping()\n",
    "        full_trip_gdf (_type_): The full trip gdf that contains all trips.\n",
    "    Returns:\n",
    "        _type_: None\n",
    "    \"\"\"\n",
    "    mistakes = []\n",
    "    for conn in mapping_cont_trips:\n",
    "        trip_ids = [conn['TRIP_ID'],  conn['TRIP_ID_CONT']]\n",
    "        unique_person = full_trip_gdf.query(\"TRIP_ID in @trip_ids\").PERSON_ID.nunique()\n",
    "\n",
    "        if unique_person > 1:\n",
    "            mistakes.append(full_trip_gdf.query(\"TRIP_ID in @trip_ids\"))\n",
    "\n",
    "\n",
    "    print(f\"Number of edges (matched) between trips: {len(mapping_cont_trips)}\")\n",
    "    print(f\"Number of wrong matches: {len(mistakes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges (matched) between trips: 74\n",
      "Number of wrong matches: 0\n"
     ]
    }
   ],
   "source": [
    "evaluate_trip_chaining(mapping_cont_trips, full_trip_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge trips according to matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTripChain(trip_id, mapping_cont_trips, chain=[]):\n",
    "    \"\"\" Recursive function that returns a list for all chained trips for a give orig trip_id\n",
    "\n",
    "\n",
    "    Args:\n",
    "        trip_id (_type_): _description_\n",
    "        chain (list, optional): _description_. Defaults to [].\n",
    "        mapping_cont_trips (_type_): Mapping of continued trips. Output of build_trip_chain_mapping().\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if type(trip_id) == str:\n",
    "        trip_id = int(trip_id)\n",
    "\n",
    "    # add orig trip_id to output list\n",
    "    if len(chain) == 0:\n",
    "        chain.append(trip_id)\n",
    "\n",
    "    # recursively find all chained trips originating from the orig trip_id\n",
    "    for edge in mapping_cont_trips:\n",
    "        if edge['TRIP_ID'] == trip_id:\n",
    "            chain.append(edge['TRIP_ID_CONT'])\n",
    "            getTripChain(edge['TRIP_ID_CONT'], mapping_cont_trips, chain)\n",
    "            \n",
    "        \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trips_from_matching(gdf_sp, mapping_cont_trips, full_trip_gdf):\n",
    "    \"\"\"This function merges trips that are chained together from the matching done in build_trip_chain_mapping().\n",
    "\n",
    "    Args:\n",
    "        gdf_sp (_type_): GeoDataFrame of start points.\n",
    "        mapping_cont_trips (_type_): Mapping of continued trips. Output of build_trip_chain_mapping().\n",
    "        full_trip_gdf (_type_): GeoDataFrame of all trips.\n",
    "\n",
    "    Returns:\n",
    "        _type_: GeoDataFrame of merged trips.\n",
    "    \"\"\"\n",
    "    # Get trip chain for each trip (Start Point)\n",
    "    print(\"Building trip chains...\")\n",
    "    trip_chains = [getTripChain(trip, mapping_cont_trips, chain=[]) for trip in tqdm(gdf_sp.TRIP_ID)]\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Sort for longest chain first\n",
    "    trip_chains.sort(key = len, reverse = True)\n",
    "\n",
    "    # Create dictionary to store mappings for evaluation\n",
    "    trip_concat_dict = {}\n",
    "\n",
    "    covered_trips = []\n",
    "    merged_trips_gdf = []\n",
    "    print(\"Merging trips...\")\n",
    "    for chain in tqdm(trip_chains, total=len(trip_chains)):\n",
    "        # Check if any of the trips in the current chain has already been merged as part of another chain\n",
    "        # Since we start with the longest chain and iterate through descending sorted list, we only retain the complete chains\n",
    "        if set(chain).intersection(set(covered_trips)):\n",
    "            continue\n",
    "\n",
    "        # add trip chain to dict for evaluation later\n",
    "        trip_concat_dict[chain[0]] = chain[1:]\n",
    "        \n",
    "        # add all trip ids part of current chain to list so that every trip is only contained in longest chain of it\n",
    "        covered_trips += chain\n",
    "\n",
    "        trips = full_trip_gdf.query(\"TRIP_ID in @chain\").sort_values(\"TRIP_START\")\n",
    "        trips[\"temp\"] = 1\n",
    "\n",
    "        trips = trips.groupby('temp').agg(list).reset_index(drop=True).rename(columns={'TRIP_ID': 'TRIP_ID_CHAIN'})\n",
    "\n",
    "        trips[\"wkt_trip\"] = trips['geometry'].apply(lambda x: \", \".join([str(i) for i in x]).replace(\"), LINESTRING (\", \", \"))\n",
    "        trips['TRIP_START'] = trips['TRIP_START'].apply(lambda x: min(x))\n",
    "        trips['TRIP_END'] = trips['TRIP_END'].apply(lambda x: max(x))\n",
    "        trips['TRIP_LEN_IN_MTRS'] = trips['TRIP_LEN_IN_MTRS'].apply(lambda x: sum(x))\n",
    "        #trips['TRIP_DURATION_IN_SECS'] = trips['TRIP_DURATION_IN_SECS'].apply(lambda x: sum(x))\n",
    "        trips['TRIP_WD'] = trips['TRIP_WD'].apply(lambda x: x[0]) # see below\n",
    "        trips['TRIP_DATE'] = trips['TRIP_DATE'].apply(lambda x: x[0]) # see below\n",
    "        trips['TRIP_ID'] = trips['TRIP_ID_CHAIN'].apply(lambda x: x[0]) # assign trip_id of first trip in chain to concatenated trip\n",
    "        # This is the TRIP_ID of the last trip in the chain to be concatenated\n",
    "        trips['TRIP_ID_LAST'] = trips['TRIP_ID_CHAIN'].apply(lambda x: x[-1]) \n",
    "\n",
    "        # Note: Here we are assigning the PERSON_ID of the first trip to the concatenated trip. This of course can be erroneous if the concatenation itself is wrong\n",
    "        trips['PERSON_ID'] = trips['PERSON_ID'].apply(lambda x: x[0])\n",
    "        trips = trips.drop(['geometry', 'TRIP_ID_CHAIN'], axis=1)\n",
    "\n",
    "        trips = gp.GeoDataFrame(trips, geometry=gp.GeoSeries.from_wkt(trips['wkt_trip'])).drop('wkt_trip', axis=1)\n",
    "\n",
    "        merged_trips_gdf.append(trips)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    trip_merged_gdf = pd.concat(merged_trips_gdf)\n",
    "\n",
    "    print(f\"Number of trips that were matched at least once: {len(set(covered_trips))}/{len(set(gdf_sp.TRIP_ID))}\")\n",
    "\n",
    "    # Concatenate all trips that were unmerged with the merged trips into a new gdf\n",
    "    print(\"Concatenating MERGED and UNMERGED trips...\")\n",
    "    unmerged_trips = full_trip_gdf.query(\"TRIP_ID not in @covered_trips\")\n",
    "    full_trips_concat_gdf = pd.concat([unmerged_trips, trip_merged_gdf])\n",
    "    full_trips_concat_gdf['TRIP_ID_FIRST'] = full_trips_concat_gdf['TRIP_ID'] # This is the same as TRIP_ID\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Assign TRIP_ID as TRIP_ID_LAST in case TRIP has not been merged and first and last TRIP_Id are in fact the same\n",
    "    full_trips_concat_gdf['TRIP_ID_LAST'] = np.where(full_trips_concat_gdf.TRIP_ID_LAST.isnull(), full_trips_concat_gdf.TRIP_ID, full_trips_concat_gdf.TRIP_ID_LAST)\n",
    "\n",
    "\n",
    "    return full_trips_concat_gdf.reset_index(drop=True), trip_concat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trip chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:00<00:00, 81973.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Merging trips...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:03<00:00, 75.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Number of trips that were matched at least once: 245/246\n",
      "Concatenating MERGED and UNMERGED trips...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_trips_concat_gdf, trip_concat_dict = merge_trips_from_matching(gdf_sp, mapping_cont_trips, full_trip_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for those trip_ids that are still the start of a trip even after the concatenation (of trip chains)\n",
    "t_id_sp = full_trips_concat_gdf.TRIP_ID_FIRST\n",
    "t_id_ep = full_trips_concat_gdf.TRIP_ID_LAST\n",
    "\n",
    "# Also filter dfs that contain points\n",
    "gdf_sp_concat = gdf_sp.query(\"TRIP_ID in @t_id_sp\")\n",
    "trip_sp_gdf_concat = trip_sp_gdf.query(\"TRIP_ID in @t_id_sp\")\n",
    "\n",
    "gdf_ep_concat = gdf_ep.query(\"TRIP_ID in @t_id_ep\")\n",
    "trip_ep_gdf_concat = trip_ep_gdf.query(\"TRIP_ID in @t_id_ep\")\n",
    "\n",
    "assert len(trip_sp_gdf_concat) == len(trip_ep_gdf_concat) == len(gdf_sp_concat) == len(gdf_ep_concat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Clustering after Concatenation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexInList(trip_id, full_trip_gdf):\n",
    "    \"\"\"This function takes in a trip_id and returns the list index of this trip's position in the ground truth clustering.\n",
    "\n",
    "    Args:\n",
    "        trip_id (int): TRIP_ID\n",
    "\n",
    "    Returns:\n",
    "        int: The index of this TRIP_ID in the ground truth clustering vector.\n",
    "    \"\"\"\n",
    "    index_list = full_trip_gdf.sort_values('TRIP_ID').TRIP_ID.to_list()\n",
    "\n",
    "    return index_list.index(trip_id)\n",
    "\n",
    "\n",
    "def build_clustering_after_concatenation(full_trips_concat_gdf, trip_concat_dict):\n",
    "    \"\"\"This function builds the clustering vector after the concatenation step.\n",
    "\n",
    "    Args:\n",
    "        full_trips_concat_gdf (GeoDataFrame): GeoDataFrame containing all trips after the concatenation step.\n",
    "        trip_concat_dict (dict): Dictionary containing the trip chains that were concatenated.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of this TRIP_ID in the ground truth clustering vector.\n",
    "    \"\"\"\n",
    "\n",
    "    # This creates the array with clustering IDs after the concatenation step\n",
    "    clustering_concat = {}\n",
    "    for index, trip in full_trips_concat_gdf.reset_index().sort_values('TRIP_ID').iterrows():\n",
    "        trip_order_index = getIndexInList(trip.TRIP_ID, full_trip_gdf)\n",
    "\n",
    "        clustering_concat[trip_order_index] = index\n",
    "\n",
    "        if trip.TRIP_ID in trip_concat_dict:\n",
    "            for t in trip_concat_dict[trip.TRIP_ID]:\n",
    "                clustering_concat[getIndexInList(t, full_trip_gdf)] = index\n",
    "\n",
    "    clustering_concat = list(dict(sorted(clustering_concat.items())).values())\n",
    "\n",
    "    print(f\"Number of unique clusters: {len(set(clustering_concat))}\")\n",
    "\n",
    "    return clustering_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique clusters: 173\n"
     ]
    }
   ],
   "source": [
    "clustering_concat = build_clustering_after_concatenation(full_trips_concat_gdf, trip_concat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Home Locations (HL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Start Points (SPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SP-tessellation matching that still contains all SP (and potential HL), and not just the SP and EP of the concatenated trips. We do this, because we do not want loose potential HL contributed of substrip concatenated in a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>TRIP_START</th>\n",
       "      <th>TRIP_END</th>\n",
       "      <th>TRIP_LEN_IN_MTRS</th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>TRIP_WD</th>\n",
       "      <th>TRIP_DATE</th>\n",
       "      <th>hl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>372</td>\n",
       "      <td>POLYGON ((442357.988 4424432.304, 442332.766 4...</td>\n",
       "      <td>1552</td>\n",
       "      <td>2008-03-02 04:39:11</td>\n",
       "      <td>2008-03-02 06:04:24</td>\n",
       "      <td>4438.259538</td>\n",
       "      <td>101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2008-03-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>372</td>\n",
       "      <td>POLYGON ((442357.988 4424432.304, 442332.766 4...</td>\n",
       "      <td>1558</td>\n",
       "      <td>2008-03-02 12:39:11</td>\n",
       "      <td>2008-03-02 14:04:24</td>\n",
       "      <td>4438.259538</td>\n",
       "      <td>101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2008-03-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>528</td>\n",
       "      <td>POLYGON ((438032.778 4423436.306, 438007.573 4...</td>\n",
       "      <td>22914</td>\n",
       "      <td>2009-08-19 14:17:38</td>\n",
       "      <td>2009-08-19 15:13:07</td>\n",
       "      <td>12814.433108</td>\n",
       "      <td>50</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-08-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>528</td>\n",
       "      <td>POLYGON ((438032.778 4423436.306, 438007.573 4...</td>\n",
       "      <td>22895</td>\n",
       "      <td>2009-08-19 06:17:38</td>\n",
       "      <td>2009-08-19 07:13:07</td>\n",
       "      <td>12814.428804</td>\n",
       "      <td>50</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-08-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528</td>\n",
       "      <td>POLYGON ((438032.778 4423436.306, 438007.573 4...</td>\n",
       "      <td>22676</td>\n",
       "      <td>2009-08-04 03:10:59</td>\n",
       "      <td>2009-08-04 04:47:08</td>\n",
       "      <td>13890.512295</td>\n",
       "      <td>50</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2009-08-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>45834</td>\n",
       "      <td>POLYGON ((440561.829 4425552.124, 440536.612 4...</td>\n",
       "      <td>1579</td>\n",
       "      <td>2008-03-09 13:29:17</td>\n",
       "      <td>2008-03-09 13:32:06</td>\n",
       "      <td>257.057315</td>\n",
       "      <td>101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2008-03-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>45959</td>\n",
       "      <td>POLYGON ((454783.637 4417690.904, 454758.380 4...</td>\n",
       "      <td>22977</td>\n",
       "      <td>2009-08-22 15:41:43</td>\n",
       "      <td>2009-08-22 17:22:56</td>\n",
       "      <td>8557.043065</td>\n",
       "      <td>50</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2009-08-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>45959</td>\n",
       "      <td>POLYGON ((454783.637 4417690.904, 454758.380 4...</td>\n",
       "      <td>22962</td>\n",
       "      <td>2009-08-22 07:41:43</td>\n",
       "      <td>2009-08-22 09:22:56</td>\n",
       "      <td>8557.055436</td>\n",
       "      <td>50</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2009-08-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>46562</td>\n",
       "      <td>POLYGON ((437507.092 4413698.900, 437481.903 4...</td>\n",
       "      <td>22707</td>\n",
       "      <td>2009-08-06 05:56:33</td>\n",
       "      <td>2009-08-06 07:32:58</td>\n",
       "      <td>17811.876606</td>\n",
       "      <td>50</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>46562</td>\n",
       "      <td>POLYGON ((437507.092 4413698.900, 437481.903 4...</td>\n",
       "      <td>22716</td>\n",
       "      <td>2009-08-06 13:55:44</td>\n",
       "      <td>2009-08-06 15:32:58</td>\n",
       "      <td>17865.269180</td>\n",
       "      <td>50</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tile_id                                           geometry  TRIP_ID  \\\n",
       "0       372  POLYGON ((442357.988 4424432.304, 442332.766 4...     1552   \n",
       "1       372  POLYGON ((442357.988 4424432.304, 442332.766 4...     1558   \n",
       "2       528  POLYGON ((438032.778 4423436.306, 438007.573 4...    22914   \n",
       "3       528  POLYGON ((438032.778 4423436.306, 438007.573 4...    22895   \n",
       "4       528  POLYGON ((438032.778 4423436.306, 438007.573 4...    22676   \n",
       "..      ...                                                ...      ...   \n",
       "241   45834  POLYGON ((440561.829 4425552.124, 440536.612 4...     1579   \n",
       "242   45959  POLYGON ((454783.637 4417690.904, 454758.380 4...    22977   \n",
       "243   45959  POLYGON ((454783.637 4417690.904, 454758.380 4...    22962   \n",
       "244   46562  POLYGON ((437507.092 4413698.900, 437481.903 4...    22707   \n",
       "245   46562  POLYGON ((437507.092 4413698.900, 437481.903 4...    22716   \n",
       "\n",
       "              TRIP_START             TRIP_END  TRIP_LEN_IN_MTRS  PERSON_ID  \\\n",
       "0    2008-03-02 04:39:11  2008-03-02 06:04:24       4438.259538        101   \n",
       "1    2008-03-02 12:39:11  2008-03-02 14:04:24       4438.259538        101   \n",
       "2    2009-08-19 14:17:38  2009-08-19 15:13:07      12814.433108         50   \n",
       "3    2009-08-19 06:17:38  2009-08-19 07:13:07      12814.428804         50   \n",
       "4    2009-08-04 03:10:59  2009-08-04 04:47:08      13890.512295         50   \n",
       "..                   ...                  ...               ...        ...   \n",
       "241  2008-03-09 13:29:17  2008-03-09 13:32:06        257.057315        101   \n",
       "242  2009-08-22 15:41:43  2009-08-22 17:22:56       8557.043065         50   \n",
       "243  2009-08-22 07:41:43  2009-08-22 09:22:56       8557.055436         50   \n",
       "244  2009-08-06 05:56:33  2009-08-06 07:32:58      17811.876606         50   \n",
       "245  2009-08-06 13:55:44  2009-08-06 15:32:58      17865.269180         50   \n",
       "\n",
       "       TRIP_WD   TRIP_DATE hl  \n",
       "0       Sunday  2008-03-02  0  \n",
       "1       Sunday  2008-03-02  0  \n",
       "2    Wednesday  2009-08-19  0  \n",
       "3    Wednesday  2009-08-19  1  \n",
       "4      Tuesday  2009-08-04  0  \n",
       "..         ...         ... ..  \n",
       "241     Sunday  2008-03-09  0  \n",
       "242   Saturday  2009-08-22  0  \n",
       "243   Saturday  2009-08-22  1  \n",
       "244   Thursday  2009-08-06  0  \n",
       "245   Thursday  2009-08-06  0  \n",
       "\n",
       "[246 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate home locations (HL) from SPs\n",
    "gdf_sp.index=pd.to_datetime(gdf_sp.TRIP_START)\n",
    "gdf_sp['hl'] = gdf_sp['TRIP_START'].apply(lambda x: 1 if x in gdf_sp.between_time('6:00', '10:00').TRIP_START else 0).astype(object)\n",
    "gdf_sp.reset_index(inplace=True, drop=True)\n",
    "gdf_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only those cells that are HL\n",
    "gdf_hl_sp = gdf_sp[gdf_sp['hl'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\libpysal\\weights\\weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 33 disconnected components.\n",
      " There are 21 islands with ids: 2, 5, 9, 17, 20, 22, 23, 25, 26, 27, 30, 34, 38, 40, 43, 52, 53, 54, 55, 56, 57.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# create spatial weights matrix\n",
    "W = libpysal.weights.Queen.from_dataframe(gdf_hl_sp)\n",
    "\n",
    "# get component labels\n",
    "components = W.component_labels\n",
    "\n",
    "gdf_hl_combined_sp = pd.merge(gp.sjoin(\n",
    "    gdf_hl_sp,\n",
    "    gdf_hl_sp.dissolve(by=components)[[\"geometry\"]],\n",
    "    how=\"left\"\n",
    "), gdf_hl_sp.dissolve(by=components)[[\"geometry\"]].reset_index(), left_on=\"index_right\", right_on='index', suffixes=(\"__drop\", \"\")).drop(['index', 'index_right', 'geometry__drop'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute count of unique HL per Peson (HL here is already the merged tiles)\n",
    "gdf_hl_combined_sp = pd.merge(gdf_hl_combined_sp, gdf_hl_combined_sp.astype({'geometry': 'string'}).groupby('PERSON_ID')[['geometry']].nunique().reset_index().rename(columns={'geometry': 'CNT_UNIQUE_HL'}), how=\"left\")\n",
    "\n",
    "# TODO: Add number of trips per merged tile\n",
    "#gdf_hl_combined_sp = pd.merge(gdf_hl_combined_sp, gdf_hl_combined_sp.groupby(['PERSON_ID', 'geometry']).nunique().reset_index()[['PERSON_ID', 'TRIP_ID']].rename(columns={'TRIP_ID': 'CNT_TRIPS_PER_HL'}), suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "# using dictionary to convert specific columns\n",
    "convert_dict = {'PERSON_ID': object,\n",
    "                'CNT_UNIQUE_HL': int\n",
    "                }\n",
    " \n",
    "gdf_hl_combined_sp = gdf_hl_combined_sp.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From End Points (EPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>TRIP_START</th>\n",
       "      <th>TRIP_END</th>\n",
       "      <th>TRIP_LEN_IN_MTRS</th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>TRIP_WD</th>\n",
       "      <th>TRIP_DATE</th>\n",
       "      <th>hl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>282</td>\n",
       "      <td>POLYGON ((456551.254 4426308.699, 456525.980 4...</td>\n",
       "      <td>23321</td>\n",
       "      <td>2009-09-12 14:33:10</td>\n",
       "      <td>2009-09-12 15:06:50</td>\n",
       "      <td>1635.279132</td>\n",
       "      <td>143</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2009-09-12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>528</td>\n",
       "      <td>POLYGON ((438032.778 4423436.306, 438007.573 4...</td>\n",
       "      <td>22408</td>\n",
       "      <td>2009-07-24 11:11:47</td>\n",
       "      <td>2009-07-24 12:20:22</td>\n",
       "      <td>7190.321349</td>\n",
       "      <td>50</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2009-07-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>528</td>\n",
       "      <td>POLYGON ((438032.778 4423436.306, 438007.573 4...</td>\n",
       "      <td>22897</td>\n",
       "      <td>2009-08-19 09:43:26</td>\n",
       "      <td>2009-08-19 09:48:03</td>\n",
       "      <td>1382.819472</td>\n",
       "      <td>50</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-08-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>528</td>\n",
       "      <td>POLYGON ((438032.778 4423436.306, 438007.573 4...</td>\n",
       "      <td>22916</td>\n",
       "      <td>2009-08-19 17:43:26</td>\n",
       "      <td>2009-08-19 17:48:03</td>\n",
       "      <td>1382.804454</td>\n",
       "      <td>50</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-08-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>887</td>\n",
       "      <td>POLYGON ((441143.960 4424727.727, 441118.742 4...</td>\n",
       "      <td>2193</td>\n",
       "      <td>2008-05-07 07:05:28</td>\n",
       "      <td>2008-05-07 07:11:49</td>\n",
       "      <td>384.703156</td>\n",
       "      <td>101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2008-05-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>45828</td>\n",
       "      <td>POLYGON ((440080.919 4426103.699, 440055.703 4...</td>\n",
       "      <td>22406</td>\n",
       "      <td>2009-07-24 09:46:35</td>\n",
       "      <td>2009-07-24 10:36:12</td>\n",
       "      <td>12641.469844</td>\n",
       "      <td>50</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2009-07-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>45834</td>\n",
       "      <td>POLYGON ((440561.829 4425552.124, 440536.612 4...</td>\n",
       "      <td>27637</td>\n",
       "      <td>2011-12-30 04:04:08</td>\n",
       "      <td>2011-12-30 04:24:13</td>\n",
       "      <td>3643.215028</td>\n",
       "      <td>102</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2011-12-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>45886</td>\n",
       "      <td>POLYGON ((444229.892 4420875.143, 444204.669 4...</td>\n",
       "      <td>1004</td>\n",
       "      <td>2007-12-03 07:12:39</td>\n",
       "      <td>2007-12-03 07:41:03</td>\n",
       "      <td>5681.841580</td>\n",
       "      <td>101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2007-12-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>45959</td>\n",
       "      <td>POLYGON ((454783.637 4417690.904, 454758.380 4...</td>\n",
       "      <td>22976</td>\n",
       "      <td>2009-08-22 14:53:03</td>\n",
       "      <td>2009-08-22 15:17:14</td>\n",
       "      <td>1634.041445</td>\n",
       "      <td>50</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2009-08-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>45959</td>\n",
       "      <td>POLYGON ((454783.637 4417690.904, 454758.380 4...</td>\n",
       "      <td>22961</td>\n",
       "      <td>2009-08-22 06:03:16</td>\n",
       "      <td>2009-08-22 07:17:14</td>\n",
       "      <td>18206.011775</td>\n",
       "      <td>50</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2009-08-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tile_id                                           geometry  TRIP_ID  \\\n",
       "0       282  POLYGON ((456551.254 4426308.699, 456525.980 4...    23321   \n",
       "1       528  POLYGON ((438032.778 4423436.306, 438007.573 4...    22408   \n",
       "2       528  POLYGON ((438032.778 4423436.306, 438007.573 4...    22897   \n",
       "3       528  POLYGON ((438032.778 4423436.306, 438007.573 4...    22916   \n",
       "4       887  POLYGON ((441143.960 4424727.727, 441118.742 4...     2193   \n",
       "..      ...                                                ...      ...   \n",
       "241   45828  POLYGON ((440080.919 4426103.699, 440055.703 4...    22406   \n",
       "242   45834  POLYGON ((440561.829 4425552.124, 440536.612 4...    27637   \n",
       "243   45886  POLYGON ((444229.892 4420875.143, 444204.669 4...     1004   \n",
       "244   45959  POLYGON ((454783.637 4417690.904, 454758.380 4...    22976   \n",
       "245   45959  POLYGON ((454783.637 4417690.904, 454758.380 4...    22961   \n",
       "\n",
       "              TRIP_START             TRIP_END  TRIP_LEN_IN_MTRS  PERSON_ID  \\\n",
       "0    2009-09-12 14:33:10  2009-09-12 15:06:50       1635.279132        143   \n",
       "1    2009-07-24 11:11:47  2009-07-24 12:20:22       7190.321349         50   \n",
       "2    2009-08-19 09:43:26  2009-08-19 09:48:03       1382.819472         50   \n",
       "3    2009-08-19 17:43:26  2009-08-19 17:48:03       1382.804454         50   \n",
       "4    2008-05-07 07:05:28  2008-05-07 07:11:49        384.703156        101   \n",
       "..                   ...                  ...               ...        ...   \n",
       "241  2009-07-24 09:46:35  2009-07-24 10:36:12      12641.469844         50   \n",
       "242  2011-12-30 04:04:08  2011-12-30 04:24:13       3643.215028        102   \n",
       "243  2007-12-03 07:12:39  2007-12-03 07:41:03       5681.841580        101   \n",
       "244  2009-08-22 14:53:03  2009-08-22 15:17:14       1634.041445         50   \n",
       "245  2009-08-22 06:03:16  2009-08-22 07:17:14      18206.011775         50   \n",
       "\n",
       "       TRIP_WD   TRIP_DATE hl  \n",
       "0     Saturday  2009-09-12  0  \n",
       "1       Friday  2009-07-24  0  \n",
       "2    Wednesday  2009-08-19  0  \n",
       "3    Wednesday  2009-08-19  0  \n",
       "4    Wednesday  2008-05-07  0  \n",
       "..         ...         ... ..  \n",
       "241     Friday  2009-07-24  0  \n",
       "242     Friday  2011-12-30  0  \n",
       "243     Monday  2007-12-03  0  \n",
       "244   Saturday  2009-08-22  0  \n",
       "245   Saturday  2009-08-22  0  \n",
       "\n",
       "[246 rows x 10 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate home locations (HL) from EPs\n",
    "gdf_ep.index=pd.to_datetime(gdf_ep.TRIP_END)\n",
    "gdf_ep['hl'] = gdf_ep['TRIP_END'].apply(lambda x: 1 if x in gdf_ep.between_time('18:00', '00:00').TRIP_END else 0).astype(object)\n",
    "gdf_ep.reset_index(inplace=True, drop=True)\n",
    "gdf_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only those cells that are HL\n",
    "gdf_hl_ep = gdf_ep[gdf_ep['hl'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\libpysal\\weights\\weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 15 disconnected components.\n",
      " There are 7 islands with ids: 3, 8, 9, 10, 12, 21, 26.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "### Merge hl cells that are adjacent (touching) to each other \n",
    "\n",
    "# create spatial weights matrix\n",
    "W = libpysal.weights.Queen.from_dataframe(gdf_hl_ep)\n",
    "\n",
    "# get component labels\n",
    "components = W.component_labels\n",
    "\n",
    "gdf_hl_combined_ep = pd.merge(gp.sjoin(\n",
    "    gdf_hl_ep,\n",
    "    gdf_hl_ep.dissolve(by=components)[[\"geometry\"]],\n",
    "    how=\"left\"\n",
    "), gdf_hl_ep.dissolve(by=components)[[\"geometry\"]].reset_index(), left_on=\"index_right\", right_on='index', suffixes=(\"__drop\", \"\")).drop(['index', 'index_right', 'geometry__drop'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hl_combined_ep = pd.merge(gdf_hl_combined_ep, gdf_hl_combined_ep.astype({'geometry': 'string'}).groupby('PERSON_ID')[['geometry']].nunique().reset_index().rename(columns={'geometry': 'CNT_UNIQUE_HL'}), how=\"left\")\n",
    "\n",
    "# using dictionary to convert specific columns\n",
    "convert_dict = {'PERSON_ID': object,\n",
    "                'CNT_UNIQUE_HL': int\n",
    "                }\n",
    " \n",
    "gdf_hl_combined_ep = gdf_hl_combined_ep.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge (concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_combined = pd.concat([gdf_hl_combined_ep, gdf_hl_combined_sp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\libpysal\\weights\\weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 38 disconnected components.\n",
      " There are 25 islands with ids: 3, 11, 12, 13, 20, 29, 46, 47, 58, 67, 68, 71, 72, 74, 75, 78, 82, 83, 84, 85, 91, 93, 94, 95, 96.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "### Merge hl cells that are adjacent (touching) to each other \n",
    "\n",
    "# create spatial weights matrix\n",
    "W = libpysal.weights.Queen.from_dataframe(gp_combined)\n",
    "\n",
    "# get component labels\n",
    "components = W.component_labels\n",
    "\n",
    "# We need to first join and then merge to first get the right index and then actually join the geometry \n",
    "gp_combined = pd.merge(gp.sjoin(\n",
    "    gp_combined,\n",
    "    gp_combined.dissolve(by=components)[[\"geometry\"]],\n",
    "    how=\"left\"\n",
    "), gp_combined.dissolve(by=components)[[\"geometry\"]].reset_index(), left_on=\"index_right\", right_on='index', suffixes=(\"__drop\", \"\")).drop(['index', 'index_right', 'geometry__drop'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_combined = pd.merge(gp_combined.drop('CNT_UNIQUE_HL', axis=1), gp_combined.astype({'geometry': 'string'}).groupby('PERSON_ID')[['geometry']].nunique().reset_index().rename(columns={'geometry': 'CNT_UNIQUE_HL'}), how=\"left\")\n",
    "\n",
    "# using dictionary to convert specific columns\n",
    "convert_dict = {'PERSON_ID': object,\n",
    "                'CNT_UNIQUE_HL': int\n",
    "                }\n",
    " \n",
    "gp_combined = gp_combined.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Frequency'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the number of unique users per HL tile (count of 1 means that the HL has only trips from one unique user, thus perfect clustering)\n",
    "gp_combined.astype({'geometry':'string'}).groupby(['geometry'])[['PERSON_ID']].nunique().PERSON_ID.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique users for which at least one HL was identified\n",
    "gp_combined.PERSON_ID.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique HL tiles: 38\n"
     ]
    }
   ],
   "source": [
    "# Assign ID to HL\n",
    "gp_combined['HL_ID'] = gp_combined.astype({'geometry': 'string'}).groupby('geometry').ngroup()\n",
    "\n",
    "HL_table = gp_combined[['geometry', 'HL_ID']].drop_duplicates()\n",
    "\n",
    "print(f\"Number of unique HL tiles: {len(HL_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match trips with Home Location tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match concatenated trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all start and enpoints of all trajectories with HL tiles\n",
    "# All successfully matched trips will have 0 in the \"matched_sp/ep\" column else NaN\n",
    "matched_sp = gp.sjoin(\n",
    "    trip_sp_gdf_concat, # This data frame contains all SPs of the trips that are at the end of a concatenated trip (end of a concatenated trip)\n",
    "    gp_combined.dissolve()[['geometry']], # Here we do the matching on the dissolved HL tiles since we only want to have one match per point to detect binary whether it is matched at all or not\n",
    "    how=\"left\"\n",
    ").rename(columns={\"index_right\": \"matched_sp\"})\n",
    "\n",
    "matched_ep = gp.sjoin(\n",
    "    trip_ep_gdf_concat, # This data frame contains all EPs of the trips that are at the end of a concatenated trip (end of a concatenated trip)\n",
    "    gp_combined.dissolve()[['geometry']], # same here, see above\n",
    "    how=\"left\"\n",
    ").rename(columns={\"index_right\": \"matched_ep\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Since we here match with the dissolved tile, we also can at max get ONE match per SP since overlapping HL tiles are dissolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge start and endpoints of all trajectories with HL tiles to get HL_IDs for each trip\n",
    "s = gp.sjoin(\n",
    "    trip_sp_gdf_concat,\n",
    "    HL_table,\n",
    "    how=\"right\").drop('index_left', axis=1).dropna()\n",
    "\n",
    "e = gp.sjoin(\n",
    "    trip_ep_gdf_concat, \n",
    "    HL_table, \n",
    "    how=\"right\").drop('index_left', axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched trajectories (concatenated) that do neither start nor end in a HL tile: 39/173\n",
      "Number of trajectories (concatenated) that start AND end in a HL tile: 64/173\n"
     ]
    }
   ],
   "source": [
    "# Get unmatched start and endpoints\n",
    "unmatched_sp_t_ids = matched_sp[matched_sp.matched_sp.isnull()].TRIP_ID.to_list()\n",
    "unmatched_ep_t_ids = matched_ep[matched_ep.matched_ep.isnull()].TRIP_ID.to_list()\n",
    "\n",
    "# Number of unmatched trajectories that do not start or end in an HL tile\n",
    "nr_unmatched = len(full_trips_concat_gdf.query('TRIP_ID_FIRST in @unmatched_sp_t_ids and TRIP_ID_LAST in @unmatched_ep_t_ids'))\n",
    "print(f\"Number of unmatched trajectories (concatenated) that do neither start nor end in a HL tile: {nr_unmatched}/{len(full_trips_concat_gdf)}\")\n",
    "\n",
    "# Get TRIP_IDs of matched start and endpoints\n",
    "matched_sp_t_ids = matched_sp[~matched_sp.matched_sp.isnull()].TRIP_ID.to_list()\n",
    "matched_ep_t_ids = matched_ep[~matched_ep.matched_ep.isnull()].TRIP_ID.to_list()\n",
    "\n",
    "print(f\"Number of trajectories (concatenated) that start AND end in a HL tile: {len(full_trips_concat_gdf.query('TRIP_ID_FIRST in @matched_sp_t_ids and TRIP_ID_LAST in @matched_ep_t_ids'))}/{len(full_trips_concat_gdf)}\")\n",
    "\n",
    "# check whether number of unmatched trajectories plus number of matched trajectories do line up with the total number of trips in data (in this case concatenated trips)\n",
    "assert (full_trips_concat_gdf.query(\"TRIP_ID_FIRST in @s.TRIP_ID or TRIP_ID_LAST in @e.TRIP_ID\").TRIP_ID.nunique() + nr_unmatched) == len(full_trips_concat_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips that match different HL tiles with their SP and EP: 36\n"
     ]
    }
   ],
   "source": [
    "# Merge matched SP and EP to get the HL_IDs for each trip and drop duplicates.\n",
    "HL_table_se_concat = pd.merge(full_trips_concat_gdf, s[['TRIP_ID', 'HL_ID']], left_on=\"TRIP_ID_FIRST\", right_on=\"TRIP_ID\", how=\"left\")\n",
    "HL_table_se_concat = pd.merge(HL_table_se_concat, e[['TRIP_ID', 'HL_ID']], left_on=\"TRIP_ID_LAST\", right_on=\"TRIP_ID\", how=\"left\").drop(['TRIP_ID_y', 'TRIP_ID'], axis=1).rename(columns={'TRIP_ID_x': 'TRIP_ID'})\n",
    "HL_table_se_concat = HL_table_se_concat[['TRIP_ID', 'HL_ID_x', 'HL_ID_y']].set_index('TRIP_ID').stack().droplevel(1).reset_index().rename(columns={0: 'HL_ID'}).drop_duplicates()\n",
    "\n",
    "# Get trips that match different HL tiles with their SP and EP\n",
    "double_assigned_trips = HL_table_se_concat.groupby('TRIP_ID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Get trips that are not assigned to any HL tile\n",
    "unmatched_trips = full_trips_concat_gdf.query('TRIP_ID_FIRST in @unmatched_sp_t_ids and TRIP_ID_LAST in @unmatched_ep_t_ids')[['TRIP_ID']].reset_index(drop=True)\n",
    "unmatched_trips['HL_ID'] = None # Get same format as HL_table_se_concat\n",
    "\n",
    "#HL_table_se_concat['HL_ID'] = HL_table_se_concat['HL_ID_x'].combine_first(HL_table_se_concat['HL_ID_y'])\n",
    "print(f\"Number of trips that match different HL tiles with their SP and EP: {double_assigned_trips.TRIP_ID.nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign double matched trips to one unique HL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all double matched trips and compare them to all other trips that are uniquely assigned in their respective potential HLs that they have been matched with. Then take the HL with the single maximum lcss score between the trip under question and any trip of the assigned HL tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-2)]: Done  18 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-2)]: Done  27 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-2)]: Done  47 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=-2)]: Done  58 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-2)]: Done  67 out of  72 | elapsed:   39.0s remaining:    2.8s\n",
      "[Parallel(n_jobs=-2)]: Done  72 out of  72 | elapsed:   41.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Get trips that match only one HL tile with their SP and EP\n",
    "uniquely_assigned_trips = HL_table_se_concat.groupby('TRIP_ID').filter(lambda x: len(x) == 1)\n",
    "\n",
    "def compute_lcss_scores(double_assigned_trip):\n",
    "    t_id = double_assigned_trip.TRIP_ID\n",
    "    hl_id = double_assigned_trip.HL_ID\n",
    "\n",
    "    # Create dict to store results for this trip and HL\n",
    "    result_dict = {t_id: {hl_id: list()}}\n",
    "\n",
    "    # Get trajectory linestring for this trip\n",
    "    trip = full_trips_concat_gdf.query(\"TRIP_ID == @t_id\")\n",
    "\n",
    "    # Get trips that are uniquely assigned to this HL\n",
    "    assigned_trips = uniquely_assigned_trips.query(\"HL_ID == @hl_id\")\n",
    "\n",
    "    # Loop through these trips and calc LCSS scores for each of them\n",
    "    for index, assigned_trip in assigned_trips.iterrows():\n",
    "        assigned_t_id = assigned_trip.TRIP_ID\n",
    "\n",
    "        # Skip the calc for the trip with itself\n",
    "        if assigned_t_id == t_id:\n",
    "            continue\n",
    "        # Get trajectory linestring for this trip (here we use the non-concated one since we are considering S and E points separately to match HL with the concated trips afterwards)\n",
    "        a_trip = full_trips_concat_gdf.query(\"TRIP_ID == @assigned_t_id\")\n",
    "\n",
    "        score = LCSS(trip.geometry, a_trip.geometry)\n",
    "\n",
    "        # save scores in list\n",
    "        result_dict[t_id][hl_id].append(score)\n",
    "    return result_dict\n",
    "\n",
    "parallel_scores = Parallel(n_jobs=-2, verbose=10)(delayed(compute_lcss_scores)(double_assigned_trip) for index, double_assigned_trip in double_assigned_trips.iterrows())\n",
    "\n",
    "# Flatten the list of dicts from parallel processing\n",
    "lcss_scores = {}\n",
    "for idx, trip in enumerate(parallel_scores):\n",
    "    for t_id in trip:\n",
    "\n",
    "        # create dict for this trip if not yet existing (it would if another HL this trip was joined with has already been checked)\n",
    "        if t_id not in lcss_scores:\n",
    "            lcss_scores[t_id] = {}\n",
    "\n",
    "        for hl_id in parallel_scores[idx][t_id]:\n",
    "            # create new list for this HL under the trip key\n",
    "            lcss_scores[t_id][hl_id] = parallel_scores[idx][t_id][hl_id]\n",
    "\n",
    "# Get and compare max scores across all matched HL for a trip and assign the HL with the max value of any trip\n",
    "for trip in lcss_scores:\n",
    "    for key in lcss_scores[trip]:\n",
    "        if len(lcss_scores[trip][key]) > 0:\n",
    "            lcss_scores[trip][key] = max(lcss_scores[trip][key])\n",
    "        else:\n",
    "            lcss_scores[trip][key] = 0\n",
    "    \n",
    "    lcss_scores[trip] = max(lcss_scores[trip], key=lcss_scores[trip].get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign resolved scores to se_HL_lookup table and drop duplicates\n",
    "HL_table_se_concat['HL_ID'] = HL_table_se_concat.apply(lambda x: lcss_scores[x['TRIP_ID']] if x['TRIP_ID'] in lcss_scores else x['HL_ID'], axis=1)\n",
    "HL_table_se_concat = HL_table_se_concat.drop_duplicates(['TRIP_ID', 'HL_ID']).reset_index(drop=True)\n",
    "\n",
    "# Assert that the number of trips that are matched to a HL tile plus the the nr of trips that are unmatched is equal to the total number of concatenated trips\n",
    "assert len(HL_table_se_concat) + nr_unmatched == len(full_trips_concat_gdf)\n",
    "\n",
    "# Combine resolved HL HL_table_se_concat with the unmatched trips to get full HL_table for all trips\n",
    "HL_table_trips_concat = pd.concat([HL_table_se_concat, unmatched_trips], ignore_index=True)\n",
    "HL_table_trips_concat.loc[HL_table_trips_concat.HL_ID.isnull(), 'HL_ID'] = -1 # assign HL_ID -1 to unmatched trips"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get trajectories that happened during the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTripOverlaps(gdf):\n",
    "    \"\"\"This function calculates whether two trips overlap in time.\n",
    "\n",
    "    Args:\n",
    "        gdf (_type_): The GeoDataFrame containing the trips.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Dictionary with trip IDs as keys and a list of trip IDs that overlap with the key trip as values.\n",
    "    \"\"\"\n",
    "    def getOverlaps(trip_x):\n",
    "        overlap_dict = {}\n",
    "        overlaps = []\n",
    "        ts_x = pd.to_datetime(trip_x['TRIP_START'], format='%Y-%m-%d %H:%M:%S')\n",
    "        te_x = pd.to_datetime(trip_x['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "        i = 0\n",
    "\n",
    "        for index_y, trip_y in gdf.iterrows():\n",
    "            ts_y = pd.to_datetime(trip_y['TRIP_START'], format='%Y-%m-%d %H:%M:%S')\n",
    "            te_y = pd.to_datetime(trip_y['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            if ts_x <= ts_y and te_x >= ts_y and (trip_x.TRIP_ID != trip_y.TRIP_ID): \n",
    "                overlaps.append(trip_y['TRIP_ID'])\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        overlap_dict[trip_x['TRIP_ID']] = overlaps\n",
    "\n",
    "        return overlap_dict\n",
    "\n",
    "    overlap_dicts = Parallel(n_jobs=-2, verbose=10)(delayed(getOverlaps)(trip_x) for index_x, trip_x in gdf.iterrows())\n",
    "\n",
    "    return {k: v for d in overlap_dicts for k, v in d.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.1162s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-2)]: Done  22 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-2)]: Done  40 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-2)]: Done  58 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-2)]: Done  80 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-2)]: Done 102 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-2)]: Done 128 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-2)]: Done 160 out of 173 | elapsed:    4.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-2)]: Done 173 out of 173 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "full_trips_concat_gdf_overlap_dict = getTripOverlaps(full_trips_concat_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clustering after HL assignment step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverlappingTrips(traj_id_list):\n",
    "    \"\"\"This function finds the overlapping trips for a list of trajectory IDs.\n",
    "\n",
    "    Args:\n",
    "        traj_id_list (_type_): List of trajectory IDs.\n",
    "    \"\"\"\n",
    "    overlapping_trips = [item for sublist in [full_trips_concat_gdf_overlap_dict[t] for t in traj_id_list] for item in sublist] # we first get a list of lists and then flatten it\n",
    "    return overlapping_trips\n",
    "\n",
    "def findLargestNonSimultaneousSubset(traj_id_list, RANDOMIZED_SEARCH_THRESHOLD=300, RANDOMIZED_SEARCH_ITERATIONS=1000):\n",
    "    \"\"\"This function finds the largest subset of trajectories that are not simultaneous. It uses a determinitic algorithm if the length of the trajectory ID list is smaller than a threshold and a randomized algorithm if the length is larger than the given threshold.\n",
    "\n",
    "    Args:\n",
    "        traj_id_list (list): List of trajectory IDs.\n",
    "\n",
    "    Returns:\n",
    "        list: List of trajectory IDs that are not simultaneous.\n",
    "    \"\"\"\n",
    "    len_traj_id_list = len(traj_id_list)\n",
    "\n",
    "    # Deterministic (optimal) algorithm\n",
    "    if len_traj_id_list <= RANDOMIZED_SEARCH_THRESHOLD:\n",
    "        # We create a list of all possible subsets of the trajectory ID list with decreasing length\n",
    "        # We do this iteratively to not overload the memory\n",
    "        for i in range(len_traj_id_list):\n",
    "            # Create a list of all possible subsets of the trajectory ID list with length len_traj_id_list - i\n",
    "            subsets = list(itertools.combinations(traj_id_list, len_traj_id_list - i))\n",
    "        \n",
    "            # Sort the list by length of the subsets\n",
    "            subsets.sort(key=len, reverse=True)\n",
    "\n",
    "            # Loop through the list of subsets\n",
    "            for subset in subsets:\n",
    "                # get all trips that do overlap in time with any of the trips in subset\n",
    "                overlapping_trips = getOverlappingTrips(subset)\n",
    "\n",
    "                # Check if the subset is not simultaneous\n",
    "                if all([t not in overlapping_trips for t in subset]):\n",
    "                    # If so, return the subset as list\n",
    "                    return list(subset)\n",
    "\n",
    "    # Randomized algorithm (numeric approximation)\n",
    "    else:\n",
    "        def randomized_subset_search(traj_id_list):\n",
    "            subsets = []\n",
    "            candidates = traj_id_list.copy()\n",
    "            id = candidates.pop(random.randrange(len(candidates)))\n",
    "            subset = [id]\n",
    "            while candidates:\n",
    "                next = candidates.pop(random.randrange(len(candidates)))\n",
    "\n",
    "                if all([t not in getOverlappingTrips(subset) for t in subset]):\n",
    "                    subset.append(next)\n",
    "            \n",
    "            subsets.append(subset)\n",
    "            return max(subsets, key=len)\n",
    "        \n",
    "        # We run the randomized subset search 100 times and return the longest subset\n",
    "        print(f'Running randomized subset for search for {RANDOMIZED_SEARCH_ITERATIONS} iterations with {len(traj_id_list)} trajectories...')\n",
    "        result = Parallel(n_jobs=-2, verbose=10)(delayed(randomized_subset_search)(traj_id_list) for _ in range(RANDOMIZED_SEARCH_ITERATIONS))\n",
    "        print('Done. Length of longest subset: ', len(max(result, key=len)))\n",
    "        return max(result, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 247.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# This creates the array with clustering IDs after the concatenation step\n",
    "clustering_after_HL = {}\n",
    "HL_table_dict = (HL_table_trips_concat.groupby('HL_ID')\n",
    "       .apply(lambda x: list(dict(x.TRIP_ID).values()))\n",
    "       .to_dict())\n",
    "\n",
    "for index, HL in tqdm(enumerate(HL_table_dict), total=len(HL_table_dict)):\n",
    "    # Skip HL_ID -1, we will assign a clustering ID to these trips later\n",
    "    if HL == -1:\n",
    "        continue\n",
    "\n",
    "    # find the largest subset of trips that are not simultaneous\n",
    "    non_simultaneous_subset = findLargestNonSimultaneousSubset(HL_table_dict[HL])\n",
    "\n",
    "    # assign hl_id -1 to all trips that are not part of the largest subset\n",
    "    for trip in HL_table_dict[HL]:\n",
    "        if trip not in non_simultaneous_subset:\n",
    "            HL_table_dict[-1].append(trip)\n",
    "\n",
    "    # Loop through all trips that are assigned to this HL_ID and assign the same clustering ID to all of them\n",
    "    for trip in non_simultaneous_subset:\n",
    "        clustering_after_HL[getIndexInList(trip, full_trip_gdf)] = index\n",
    "\n",
    "        # Check if this trip is a concatenated trip and assign the same clustering ID to all trips that are part of the concatenated trip\n",
    "        if trip in trip_concat_dict:\n",
    "            for t in trip_concat_dict[trip]:\n",
    "                clustering_after_HL[getIndexInList(t, full_trip_gdf)] = index\n",
    "\n",
    "# Assign clustering IDs to all hl_id -1 trips (these are the trips that were not successfully assigned to any HL_ID)\n",
    "for index, unm_trip in enumerate(HL_table_dict[-1]):\n",
    "    clustering_after_HL[getIndexInList(unm_trip, full_trip_gdf)] = index + len(HL_table_dict) - 1 # -1 because we don't want to count the -1 HL_ID in the length of HL_table_dict\n",
    "\n",
    "    # Check if this trip is a concatenated trip and assign the same clustering ID to all trips that are part of the concatenated trip\n",
    "    if unm_trip in trip_concat_dict:\n",
    "        # for t in trip_concat_dict[unm_trip]:\n",
    "            clustering_after_HL[getIndexInList(t, full_trip_gdf)] = index + len(HL_table_dict) - 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Trips Without Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing trips that were not assigned to any HL_ID with trips that were assigned to a HL_ID...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:31<04:50,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing match and assign cluster id 13.0 to trip 22705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:36<04:07,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing match and assign cluster id 31.0 to trip 22976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:37<02:50,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing match and assign cluster id 21.0 to trip 27571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:38<02:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match and assign new cluster id 65 to trips 27521 27637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [00:38<01:26,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match and assign new cluster id 66 to trips 1551 1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [00:40<00:51,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match and assign new cluster id 67 to trips 27617 27399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [00:47<00:33,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1.0: 0.2982456140350877, 0.0: 0.0, 1.0: 0.0, 3.0: 0.0, 4.0: 0.2982456140350877, 6.0: 0.18128654970760233, 10.0: 0.0, 12.0: 0.10526315789473684, 13.0: 0.011695906432748537, 14.0: 0.005847953216374269, 15.0: 0.0, 18.0: 0.0, 19.0: 0.10059171597633136, 21.0: 0.08187134502923976, 23.0: 0.0, 24.0: 0.0, 25.0: 0.0, 27.0: 0.08771929824561403, 28.0: 0.005847953216374269, 29.0: 0.0, 31.0: 0.0, 32.0: 0.0, 33.0: 0.0, 35.0: 0.0, 36.0: 0.0, 37.0: 0.005847953216374269, 65: 0.0, 66: 0.0, 67: 0.06432748538011696}\n",
      "27202\n",
      "22606\n",
      "22626\n",
      "There are two HL_IDs with the same LCSS score. Looking for second highest LCSS score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [00:48<00:31,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1.0: [0.0, 0.0, 0.0, 0.04093567251461988, 0.2982456140350877, 0.013071895424836602, 0.03205128205128205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05555555555555555, 0.0, 0.005847953216374269, 0.0, 0.08187134502923976, 0.0, 0.0, 0.0, 0.11940298507462686, 0.08, 0.0, 0.05847953216374269, 0.0, 0.008771929824561403, 0.0, 0.05555555555555555, 0.0], 0.0: 0.0, 1.0: 0.0, 3.0: 0.0, 4.0: [0.0, 0.005847953216374269, 0.005847953216374269, 0.0, 0.0, 0.0, 0.07017543859649122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005847953216374269, 0.005847953216374269, 0.2982456140350877], 6.0: 0.18128654970760233, 10.0: 0.0, 12.0: 0.10526315789473684, 13.0: 0.011695906432748537, 14.0: 0.005847953216374269, 15.0: 0.0, 18.0: 0.0, 19.0: 0.10059171597633136, 21.0: 0.08187134502923976, 23.0: 0.0, 24.0: 0.0, 25.0: 0.0, 27.0: 0.08771929824561403, 28.0: 0.005847953216374269, 29.0: 0.0, 31.0: 0.0, 32.0: 0.0, 33.0: 0.0, 35.0: 0.0, 36.0: 0.0, 37.0: 0.005847953216374269, 65: 0.0, 66: 0.0, 67: 0.06432748538011696}\n",
      "Done. Second highest LCSS score is for HL_ID -1.0\n",
      "no match and assign new cluster id 68 to trips 27202 22606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [00:58<00:25,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing match and assign cluster id 12.0 to trip 1579\n",
      "Done.\n",
      "Assigning clustering IDs to all trips that are part of a new cluster...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This compares all trips that were not assigned to any HL_ID with all trips that were assigned to a HL_ID and assigns the clustering ID of the trip with the highest LCSS score above a certain threshold\n",
    "SIM_THRESH_FOR_NO_MATCH = 0.25\n",
    "new_cluster_ids = []\n",
    "\n",
    "print('Comparing trips that were not assigned to any HL_ID with trips that were assigned to a HL_ID...')\n",
    "for unm_trip in tqdm(HL_table_dict[-1], total=len(HL_table_dict[-1])):\n",
    "    # compute LCSS for all HL_IDs in clustering_after_HL\n",
    "    scores = {}\n",
    "    most_similar_trip_per_hl_id = {}\n",
    "    for hl_id, trips in HL_table_dict.items():\n",
    "        scores[hl_id] = {}\n",
    "        for t in trips:\n",
    "            if t != unm_trip: # don't compare trip with itself in case for -1 hl_id\n",
    "                scores[hl_id][t] = LCSS(full_trips_concat_gdf.query(\"TRIP_ID == @unm_trip\").geometry, full_trips_concat_gdf.query(\"TRIP_ID == @t\").geometry)\n",
    "\n",
    "        # Store most similar trip per HL_ID\n",
    "        most_similar_trip_per_hl_id[hl_id] = max(scores[hl_id], key=scores[hl_id].get)\n",
    "\n",
    "        # find the trip with the highest LCSS score for current HL_ID\n",
    "        max_score = max(scores[hl_id].values())\n",
    "        scores[hl_id] = max_score\n",
    "        \n",
    "    # find the HL_ID with the highest LCSS score\n",
    "    max_score_across_hl_id = max(scores.values())\n",
    "    max_score_hl_id = [k for k, v in scores.items() if v == max_score_across_hl_id] #  in theory there could be two trips of different HL_ids with the same LCSS score. This is why this is a list\n",
    "\n",
    "    # throw error if length of max_score_hl_id is greater than 1 and the max_score_across_hl_id is above the threshold\n",
    "    if len(max_score_hl_id) > 1 and max_score_across_hl_id > SIM_THRESH_FOR_NO_MATCH:\n",
    "        print(scores)\n",
    "        print(unm_trip)\n",
    "        print(most_similar_trip_per_hl_id[max_score_hl_id[0]])\n",
    "        print(most_similar_trip_per_hl_id[max_score_hl_id[1]])\n",
    "        print(\"There are two HL_IDs with the same LCSS score. Looking for second highest LCSS score...\")\n",
    "\n",
    "        # find the second highest LCSS score\n",
    "        scores[max_score_hl_id[0]] = [LCSS(full_trips_concat_gdf.query(\"TRIP_ID == @unm_trip\").geometry, full_trips_concat_gdf.query(\"TRIP_ID == @t\").geometry) for t in HL_table_dict[max_score_hl_id[0]] if t != unm_trip]\n",
    "        scores[max_score_hl_id[1]] = [LCSS(full_trips_concat_gdf.query(\"TRIP_ID == @unm_trip\").geometry, full_trips_concat_gdf.query(\"TRIP_ID == @t\").geometry) for t in HL_table_dict[max_score_hl_id[1]] if t != unm_trip]\n",
    "        \n",
    "        # find the HL_ID with the second highest LCSS score and use that as the max_score_hl_id\n",
    "        if sorted(scores[max_score_hl_id[0]])[-2] > sorted(scores[max_score_hl_id[1]])[-2]:\n",
    "            max_score_hl_id = max_score_hl_id[0]\n",
    "        else:\n",
    "            max_score_hl_id = max_score_hl_id[1]\n",
    "        print(\"Done. Second highest LCSS score is for HL_ID\", max_score_hl_id)\n",
    "\n",
    "    else:\n",
    "        max_score_hl_id = max_score_hl_id[0]\n",
    "\n",
    "    # if the highest LCSS score is above the threshold and the trips that have the highest LCSS score do not overlap in time\n",
    "    if max_score_across_hl_id > SIM_THRESH_FOR_NO_MATCH and unm_trip not in getOverlappingTrips(HL_table_dict[max_score_hl_id]): \n",
    "        # if hl_id of matched hl_id is -1 then create new cluster for the two trips\n",
    "        if max_score_hl_id == -1:\n",
    "            # Create new cluster id that i one higher than the highest clustering ID and the number of new clusters\n",
    "            new_cluster_id = max(clustering_after_HL.values()) + 1 + len(new_cluster_ids)\n",
    "            print(\"no match and assign new cluster id\", new_cluster_id, \"to trips\", unm_trip, most_similar_trip_per_hl_id[max_score_hl_id])\n",
    "\n",
    "            # create new cluster with ID that is one higher than the highest clustering ID\n",
    "            HL_table_dict[new_cluster_id] = [unm_trip, most_similar_trip_per_hl_id[max_score_hl_id]]\n",
    "            new_cluster_ids.append(new_cluster_id)\n",
    "\n",
    "            # Remove trips from -1 hl_id to avoid two equal LCSS scores error\n",
    "            HL_table_dict[-1].remove(most_similar_trip_per_hl_id[max_score_hl_id])\n",
    "            HL_table_dict[-1].remove(unm_trip)\n",
    "\n",
    "        # if hl_id of matched hl_id is not -1 then assign the clustering ID of the matched HL_ID to the trip\n",
    "        else:\n",
    "            print(\"existing match and assign cluster id\", max_score_hl_id, \"to trip\", unm_trip)\n",
    "            # Append the trip to the list of trips that are part of the HL_ID with the highest LCSS score\n",
    "            HL_table_dict[max_score_hl_id].append(unm_trip)\n",
    "\n",
    "            # Remove trip from -1 hl_id to avoid two equal LCSS scores error\n",
    "            HL_table_dict[-1].remove(unm_trip)\n",
    "\n",
    "            # assign the clustering ID of the HL_ID with the highest LCSS score to the trip\n",
    "            # This overwrites the previous clustering ID of the trip that was assigned in the previous step\n",
    "            clustering_after_HL[getIndexInList(unm_trip, full_trip_gdf)] = clustering_after_HL[getIndexInList(HL_table_dict[max_score_hl_id][0], full_trip_gdf)] # This is a list of length one as asserted above\n",
    "\n",
    "            # Check if this trip is a concatenated trip and assign the same clustering ID to all trips that are part of the concatenated trip\n",
    "            if unm_trip in trip_concat_dict:\n",
    "                for t in trip_concat_dict[unm_trip]:\n",
    "                    clustering_after_HL[getIndexInList(t, full_trip_gdf)] = clustering_after_HL[getIndexInList(HL_table_dict[max_score_hl_id][0], full_trip_gdf)] # This is a list of length one as asserted above\n",
    "    else:\n",
    "        # if trip overlaps in time with other trips in the same HL_ID\n",
    "        if unm_trip in getOverlappingTrips(HL_table_dict[max_score_hl_id]):\n",
    "            print(\"trip overlaps in time with other trips in the same HL_ID\")\n",
    "print('Done.')        \n",
    "\n",
    "# Assign clustering IDs to all trips that are part of a new cluster\n",
    "print('Assigning clustering IDs to all trips that are part of a new cluster...')\n",
    "for cluster_id in new_cluster_ids:\n",
    "    for trip in HL_table_dict[cluster_id]:\n",
    "        clustering_after_HL[getIndexInList(trip, full_trip_gdf)] = cluster_id\n",
    "\n",
    "    # Assign clustering IDs to all trips that are part of a concatenated trip\n",
    "    if trip in trip_concat_dict:\n",
    "        for t in trip_concat_dict[trip]:\n",
    "            clustering_after_HL[getIndexInList(t, full_trip_gdf)] = cluster_id\n",
    "print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = full_trips_concat_gdf.query(\"TRIP_ID == 22460\").explore(color='purple')\n",
    "\n",
    "hl_trips = HL_table_dict[12.0]\n",
    "\n",
    "full_trips_concat_gdf.query(\"TRIP_ID in @hl_trips\").explore(m = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique clusters: 56\n"
     ]
    }
   ],
   "source": [
    "clustering_HL = list(dict(sorted(clustering_after_HL.items())).values())\n",
    "\n",
    "print(f\"Number of unique clusters: {len(set(clustering_HL))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering results after concatenation step:\n",
      "\n",
      "Clustering results after HL matching step:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "row, column, and data array must all be the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m#evaluate(clustering_concat)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mClustering results after HL matching step:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m evaluate(clustering_HL)\n",
      "Cell \u001b[1;32mIn[27], line 67\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(clustering)\u001b[0m\n\u001b[0;32m     64\u001b[0m ground_truth \u001b[39m=\u001b[39m getGroundTruth()\n\u001b[0;32m     66\u001b[0m \u001b[39m# Not symmetric and not accounting for chance\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy@1: \u001b[39m\u001b[39m{\u001b[39;00mgetAccuracyAtOne(ground_truth,\u001b[39m \u001b[39;49mclustering)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrecision: \u001b[39m\u001b[39m{\u001b[39;00mgetPrecision(ground_truth,\u001b[39m \u001b[39mclustering)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRecall: \u001b[39m\u001b[39m{\u001b[39;00mgetRecall(ground_truth,\u001b[39m \u001b[39mclustering)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 30\u001b[0m, in \u001b[0;36mgetAccuracyAtOne\u001b[1;34m(ground_truth, clustering)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetAccuracyAtOne\u001b[39m(ground_truth, clustering):\n\u001b[1;32m---> 30\u001b[0m     cm \u001b[39m=\u001b[39m getContingencyMatrix(clustering)\n\u001b[0;32m     31\u001b[0m     \u001b[39m# Reorder rows and columns to maximize sum of diagonal\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[39m# Vgl: https://smorbieu.gitlab.io/accuracy-from-classification-to-clustering-evaluation/#:~:text=Computing%20accuracy%20for%20clustering%20can,the%20accuracy%20for%20clustering%20results.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     row_idx, col_idx \u001b[39m=\u001b[39m linear_sum_assignment(\u001b[39m-\u001b[39mcm)\n",
      "Cell \u001b[1;32mIn[27], line 14\u001b[0m, in \u001b[0;36mgetContingencyMatrix\u001b[1;34m(clustering)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetContingencyMatrix\u001b[39m(clustering):\n\u001b[0;32m     12\u001b[0m     \u001b[39m# Get ground truth labels\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     ground_truth \u001b[39m=\u001b[39m getGroundTruth()\n\u001b[1;32m---> 14\u001b[0m     cm \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mcluster\u001b[39m.\u001b[39;49mcontingency_matrix(ground_truth, clustering)\n\u001b[0;32m     15\u001b[0m     row_idx, col_idx \u001b[39m=\u001b[39m linear_sum_assignment(\u001b[39m-\u001b[39mcm) \u001b[39m# resort contingency matrix to maximize sum of diagonal\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     cm \u001b[39m=\u001b[39m cm[:, col_idx] \u001b[39m# reorder columns based on indices\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:142\u001b[0m, in \u001b[0;36mcontingency_matrix\u001b[1;34m(labels_true, labels_pred, eps, sparse, dtype)\u001b[0m\n\u001b[0;32m    138\u001b[0m n_clusters \u001b[39m=\u001b[39m clusters\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    139\u001b[0m \u001b[39m# Using coo_matrix to accelerate simple histogram calculation,\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m# i.e. bins are consecutive integers\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m# Currently, coo_matrix is faster than histogram2d for simple cases\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m contingency \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39;49mcoo_matrix(\n\u001b[0;32m    143\u001b[0m     (np\u001b[39m.\u001b[39;49mones(class_idx\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]), (class_idx, cluster_idx)),\n\u001b[0;32m    144\u001b[0m     shape\u001b[39m=\u001b[39;49m(n_classes, n_clusters),\n\u001b[0;32m    145\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m sparse:\n\u001b[0;32m    148\u001b[0m     contingency \u001b[39m=\u001b[39m contingency\u001b[39m.\u001b[39mtocsr()\n",
      "File \u001b[1;32mc:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\scipy\\sparse\\_coo.py:197\u001b[0m, in \u001b[0;36mcoo_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check()\n",
      "File \u001b[1;32mc:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\scipy\\sparse\\_coo.py:282\u001b[0m, in \u001b[0;36mcoo_matrix._check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[0;32m    280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m to_native(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[1;32m--> 282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnnz \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m    284\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mrow index exceeds matrix dimensions\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\scipy\\sparse\\_base.py:299\u001b[0m, in \u001b[0;36mspmatrix.nnz\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnnz\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    293\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Number of stored values, including explicit zeros.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[0;32m    295\u001b[0m \u001b[39m    See also\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[39m    --------\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    count_nonzero : Number of non-zero entries\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetnnz()\n",
      "File \u001b[1;32mc:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\scipy\\sparse\\_coo.py:244\u001b[0m, in \u001b[0;36mcoo_matrix.getnnz\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    242\u001b[0m nnz \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m nnz \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow) \u001b[39mor\u001b[39;00m nnz \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol):\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mrow, column, and data array must all be the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    245\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39msame length\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \\\n\u001b[0;32m    248\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mrow, column, and data arrays must be 1-D\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: row, column, and data array must all be the same length"
     ]
    }
   ],
   "source": [
    "print(\"Clustering results after concatenation step:\")\n",
    "#evaluate(clustering_concat)\n",
    "print(\"\\nClustering results after HL matching step:\")\n",
    "evaluate(clustering_HL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "543daf12f525df94f20bbdd448da69881f98a71c963c44c9c7818e0113666227"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

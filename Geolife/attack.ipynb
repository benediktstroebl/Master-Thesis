{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import libpysal\n",
    "import itertools\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data frames (~ 40 secs)\n",
    "import load_geolife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data \n",
    "raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf, tesselation_gdf = load_geolife.geolife_raw_full_trip_gdf, load_geolife.geolife_raw_sp_gdf, load_geolife.geolife_raw_ep_gdf, load_geolife.geolife_tesselation_gdf\n",
    "assert len(raw_full_trip_gdf) == len(raw_trip_sp_gdf) == len(raw_trip_ep_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_random_users_from_dataframes(n, raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf):\n",
    "    \"\"\"Select n random users from the dataset.\n",
    "\n",
    "    Args:\n",
    "        n (_type_): Number of users to select.\n",
    "    \"\"\"\n",
    "    # Select n random users\n",
    "    users = np.random.choice(raw_full_trip_gdf['PERSON_ID'].unique(), n, replace=False)\n",
    "    # Filter dataframes\n",
    "    raw_full_trip_gdf = raw_full_trip_gdf[raw_full_trip_gdf['PERSON_ID'].isin(users)]\n",
    "    raw_trip_sp_gdf = raw_trip_sp_gdf[raw_trip_sp_gdf['PERSON_ID'].isin(users)]\n",
    "    raw_trip_ep_gdf = raw_trip_ep_gdf[raw_trip_ep_gdf['PERSON_ID'].isin(users)]\n",
    "    return raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select n random person ids from the dataset\n",
    "n_users = 30\n",
    "raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf = select_n_random_users_from_dataframes(n_users, raw_full_trip_gdf, raw_trip_sp_gdf, raw_trip_ep_gdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def getGroundTruth():\n",
    "    # Get ground truth labels\n",
    "    df = full_trip_gdf\n",
    "    df['ID'] = df.sort_values('TRIP_ID').groupby('PERSON_ID').ngroup() # Sort TRIP ID ascending and set cluster id corresponding to PERSON_ID\n",
    "    ground_truth = df.sort_values('TRIP_ID').ID.to_list()\n",
    "    return ground_truth\n",
    "\n",
    "def getContingencyMatrix(clustering):\n",
    "    # Get ground truth labels\n",
    "    ground_truth = getGroundTruth()\n",
    "    cm = metrics.cluster.contingency_matrix(ground_truth, clustering)\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm) # resort contingency matrix to maximize sum of diagonal\n",
    "    cm = cm[:, col_idx] # reorder columns based on indices\n",
    "    cm = cm[row_idx, :] # reorder rows based on indices\n",
    "    return cm\n",
    "\n",
    "def getConfusionMatrix(clustering):\n",
    "    # Get ground truth labels\n",
    "    ground_truth = getGroundTruth()\n",
    "    cm = metrics.confusion_matrix(ground_truth, clustering)\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm) # resort confusion matrix to maximize sum of diagonal\n",
    "    cm = cm[:, col_idx] # reorder columns based on indices\n",
    "    cm = cm[row_idx, :] # reorder rows based on indices\n",
    "    return cm\n",
    "\n",
    "def getAccuracyAtOne(ground_truth, clustering):\n",
    "    cm = getContingencyMatrix(clustering)\n",
    "    # Reorder rows and columns to maximize sum of diagonal\n",
    "    # Vgl: https://smorbieu.gitlab.io/accuracy-from-classification-to-clustering-evaluation/#:~:text=Computing%20accuracy%20for%20clustering%20can,the%20accuracy%20for%20clustering%20results.\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm)\n",
    "    return cm[row_idx,col_idx].sum()/cm.sum()\n",
    "\n",
    "def getPrecision(ground_truth, clustering):\n",
    "    cm = getContingencyMatrix(clustering)\n",
    "    row_idx, col_idx = linear_sum_assignment(-cm) # resort confusion matrix to maximize sum of diagonal\n",
    "    cm = cm[:, col_idx] # reorder columns based on indices\n",
    "    cm = cm[row_idx, :] # reorder rows based on indices\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "    # Return unweighted mean of precision\n",
    "    return np.mean(precision)\n",
    "\n",
    "def getRecall(ground_truth, clustering):\n",
    "    cm = getContingencyMatrix(clustering)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "\n",
    "    # Return unweighted mean of recall\n",
    "    return np.mean(recall)\n",
    "\n",
    "def getF1Score(ground_truth, clustering):\n",
    "    precision = getPrecision(ground_truth, clustering)\n",
    "    recall = getRecall(ground_truth, clustering)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate(clustering):\n",
    "    # Get ground truth labels\n",
    "    ground_truth = getGroundTruth()\n",
    "\n",
    "    # Not symmetric and not accounting for chance\n",
    "    print(f\"Accuracy@1: {getAccuracyAtOne(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Precision: {getPrecision(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Recall: {getRecall(ground_truth, clustering):.3f}\")\n",
    "    print(f\"F1: {getF1Score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Homogeneity: {metrics.homogeneity_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Completeness: {metrics.completeness_score(ground_truth, clustering):.3f}\")\n",
    "\n",
    "    # All of these metrics are symmetric and some of them are accounting for chance depending on the number of classes and clusters present in the data\n",
    "    print(f\"V-measure: {metrics.v_measure_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"Rand index: {metrics.rand_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"ARI: {metrics.adjusted_rand_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"MI: {metrics.mutual_info_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"NMI: {metrics.normalized_mutual_info_score(ground_truth, clustering):.3f}\")\n",
    "    print(f\"AMI: {metrics.adjusted_mutual_info_score(ground_truth, clustering):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn.metrics\n",
    "\n",
    "def LCSS(traj1_linestr, traj2_linestr, eps=10):\n",
    "    \"\"\"This function takes in two GeoSeries and takes the top entry linestring. It then calculates the Least Common Sub-Sequence metric for these two and returns the value.\n",
    "\n",
    "    Args:\n",
    "        traj1_linestr (_type_): _description_\n",
    "        traj2_linestr (_type_): _description_\n",
    "        eps (int, optional): This can be interpreted as the distance in meters between two points compared of the subsequences. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        _type_: float\n",
    "    \"\"\"\n",
    "    assert isinstance(traj1_linestr, gp.GeoSeries), f\"traj1_linestr is of type {type(traj1_linestr)}, need to be GeoSeries\"\n",
    "    assert isinstance(traj2_linestr, gp.GeoSeries), f\"traj2_linestr is of type {type(traj2_linestr)}, need to be GeoSeries\"\n",
    "\n",
    "    s1 = traj1_linestr.iloc[0].coords\n",
    "    s2 = traj2_linestr.iloc[0].coords\n",
    "\n",
    "    return tslearn.metrics.lcss(s1, s2, eps=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Start Points (SP) and End Points (EP) with Tessellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_boundary_points_with_tessellation(raw_trip_sp_gdf, raw_trip_ep_gdf, tesselation_gdf):\n",
    "    \"\"\"This function matches the boundary points of the raw trips with the tesselation. \n",
    "\n",
    "    Args:\n",
    "        raw_trip_sp_gdf (_type_): _description_\n",
    "        raw_trip_ep_gdf (_type_): _description_\n",
    "        tesselation_gdf (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: This function returns two data frames, one for the start points and one for the end points. These data frames contain the tile_id of the tesselation that the point is located in.\n",
    "    \"\"\"\n",
    "    # SP\n",
    "    # Spatial join points to polygons\n",
    "    gdf_sp = gp.sjoin(\n",
    "        tesselation_gdf[[\"tile_id\", \"geometry\"]],\n",
    "        raw_trip_sp_gdf,\n",
    "        how=\"inner\"\n",
    "    ).drop('index_right', axis=1)\n",
    "\n",
    "    # Spatial join points to polygons\n",
    "    gdf_ep = gp.sjoin(\n",
    "        tesselation_gdf[[\"tile_id\", \"geometry\"]],\n",
    "        raw_trip_ep_gdf,\n",
    "        how=\"inner\"\n",
    "    ).drop('index_right', axis=1)\n",
    "\n",
    "    return gdf_sp, gdf_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_sp, gdf_ep = match_boundary_points_with_tessellation(raw_trip_sp_gdf, raw_trip_ep_gdf, tesselation_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Full Trips that Start and End within Tessellation Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips that start and end wihin tessellation area: 4108\n",
      "Number of trips outside and therefore dropped: 639\n"
     ]
    }
   ],
   "source": [
    "gdf_sp_ids = gdf_sp.TRIP_ID\n",
    "gdf_ep_ids = gdf_ep.TRIP_ID\n",
    "\n",
    "full_trip_gdf = raw_full_trip_gdf.query(\"TRIP_ID in @gdf_sp_ids and TRIP_ID in @gdf_ep_ids\")\n",
    "trip_sp_gdf = raw_trip_sp_gdf.query(\"TRIP_ID in @gdf_ep_ids\")\n",
    "trip_ep_gdf = raw_trip_ep_gdf.query(\"TRIP_ID in @gdf_sp_ids\")\n",
    "\n",
    "gdf_sp = gdf_sp.query(\"TRIP_ID in @gdf_ep_ids\")\n",
    "gdf_ep = gdf_ep.query(\"TRIP_ID in @gdf_sp_ids\")\n",
    "\n",
    "assert len(full_trip_gdf) == len(trip_sp_gdf) == len(trip_ep_gdf) == len(gdf_sp) == len(gdf_ep) == len(set(trip_sp_gdf.TRIP_ID).intersection(set(trip_ep_gdf.TRIP_ID))) # this last intersection checks that for all unique trip ids we have exactly ONE SP and EP\n",
    "\n",
    "print(f\"Number of trips that start and end wihin tessellation area: {len(full_trip_gdf)}\")\n",
    "print(f\"Number of trips outside and therefore dropped: {len(raw_full_trip_gdf) - len(full_trip_gdf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build mapping of trip chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trip_chain_mapping(gdf_sp, gdf_ep, INFLOW_HR_DIFF_THRESHOLD=4, HR_DIFF_THRESHOLD=8):\n",
    "    \"\"\"This function returns a list of trip chains that are continued trips that happened subsequent to and from same tile within a given time threshold.\n",
    "\n",
    "    Args:\n",
    "        gdf_sp (_type_): _description_\n",
    "        gdf_ep (_type_): _description_\n",
    "        inflow_hr_diff_threshold (int, optional): _description_. Defaults to 4.\n",
    "        hr_diff_threshold (int, optional): _description_. Defaults to 8.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate mapping of continued trips that happened subsequent to and from same tile\n",
    "    mapping_cont_trips = []\n",
    "    for index, trip in tqdm(gdf_ep.sort_values('TRIP_ID').iterrows(), total=len(gdf_ep)):\n",
    "        te_1_id = trip.TRIP_ID\n",
    "        te_1_tid = trip.tile_id\n",
    "        te_1_dt = pd.to_datetime(trip['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "        ts_1_dt = pd.to_datetime(trip['TRIP_START'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        inflow = gdf_ep.query(\"tile_id == @te_1_tid\").copy()\n",
    "        inflow['TRIP_END'] = inflow.TRIP_END.astype('datetime64[ns]')\n",
    "        inflow['TRIP_START'] = inflow.TRIP_START.astype('datetime64[ns]')\n",
    "        inflow['INFLOW_HR_DIFF'] = inflow.TRIP_END.apply(lambda x: (x - te_1_dt).total_seconds()/3600)\n",
    "        inflow = inflow.query(\"(INFLOW_HR_DIFF <= @INFLOW_HR_DIFF_THRESHOLD) and (INFLOW_HR_DIFF >= @INFLOW_HR_DIFF_THRESHOLD)\") # Take trips \n",
    "        inflow = inflow.query(\"(TRIP_START > @te_1_dt) or (@ts_1_dt > TRIP_END)\") # Ignore trips that have happened simultaneously\n",
    "\n",
    "        # if more than one trip has arrived in +- hour window, then do not merge this trip\n",
    "        if len(inflow) > 1:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Get all trips that started from same tile as t_1 has ended in\n",
    "        ts_2 = gdf_sp.query(\"tile_id == @te_1_tid\").copy()\n",
    "\n",
    "        # get difference between two trips hours (seconds divided by 3600 gets hours)\n",
    "        ts_2['TRIP_START'] = ts_2.TRIP_START.astype('datetime64[ns]')\n",
    "        ts_2['TRIP_END'] = ts_2.TRIP_END.astype('datetime64[ns]')\n",
    "    \n",
    "        ts_2['hr_diff'] = ts_2['TRIP_START'].apply(lambda x: (x - te_1_dt).total_seconds()/3600)\n",
    "\n",
    "        # Only consider trips that started within a certain time after the initial trip ended in the same tessellation tile\n",
    "        ts_2 = ts_2[(ts_2['hr_diff'].astype(str).astype(float) <= HR_DIFF_THRESHOLD) & (ts_2['hr_diff'].astype(str).astype(float) >= 0)]\n",
    "\n",
    "        # Only consider trips that are not simultaneously\n",
    "        ts_2 = ts_2.query(\"(TRIP_START > @te_1_dt) or (@ts_1_dt > TRIP_END)\")\n",
    "\n",
    "        # Only consider connection if exactly one trip started from same tile in time window\n",
    "        if len(ts_2) == 1:\n",
    "            mapping_cont_trips.append({\n",
    "                'TRIP_ID': te_1_id,\n",
    "                'TRIP_ID_CONT': ts_2.TRIP_ID.iloc[0]\n",
    "            })\n",
    "\n",
    "    return mapping_cont_trips\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4108/4108 [01:02<00:00, 65.24it/s]\n"
     ]
    }
   ],
   "source": [
    "mapping_cont_trips = build_trip_chain_mapping(gdf_sp, gdf_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trip_chaining(mapping_cont_trips, full_trip_gdf):\n",
    "    \"\"\"This function evaluates the trip chaining by checking if the chained trips are from the same person.\n",
    "\n",
    "    Args:\n",
    "        mapping_cont_trips (_type_): Dictionary of trip ids that are chained. Output of build_trip_chain_mapping()\n",
    "        full_trip_gdf (_type_): The full trip gdf that contains all trips.\n",
    "    Returns:\n",
    "        _type_: None\n",
    "    \"\"\"\n",
    "    mistakes = []\n",
    "    for conn in mapping_cont_trips:\n",
    "        trip_ids = [conn['TRIP_ID'],  conn['TRIP_ID_CONT']]\n",
    "        unique_person = full_trip_gdf.query(\"TRIP_ID in @trip_ids\").PERSON_ID.nunique()\n",
    "\n",
    "        if unique_person > 1:\n",
    "            mistakes.append(full_trip_gdf.query(\"TRIP_ID in @trip_ids\"))\n",
    "\n",
    "\n",
    "    print(f\"Number of edges (matched) between trips: {len(mapping_cont_trips)}\")\n",
    "    print(f\"Number of wrong matches: {len(mistakes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges (matched) between trips: 1491\n",
      "Number of wrong matches: 18\n"
     ]
    }
   ],
   "source": [
    "evaluate_trip_chaining(mapping_cont_trips, full_trip_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge trips according to matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTripChain(trip_id, mapping_cont_trips, chain=[]):\n",
    "    \"\"\" Recursive function that returns a list for all chained trips for a give orig trip_id\n",
    "\n",
    "\n",
    "    Args:\n",
    "        trip_id (_type_): _description_\n",
    "        chain (list, optional): _description_. Defaults to [].\n",
    "        mapping_cont_trips (_type_): Mapping of continued trips. Output of build_trip_chain_mapping().\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if type(trip_id) == str:\n",
    "        trip_id = int(trip_id)\n",
    "\n",
    "    # add orig trip_id to output list\n",
    "    if len(chain) == 0:\n",
    "        chain.append(trip_id)\n",
    "\n",
    "    # recursively find all chained trips originating from the orig trip_id\n",
    "    for edge in mapping_cont_trips:\n",
    "        if edge['TRIP_ID'] == trip_id:\n",
    "            chain.append(edge['TRIP_ID_CONT'])\n",
    "            getTripChain(edge['TRIP_ID_CONT'], mapping_cont_trips, chain)\n",
    "            \n",
    "        \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trips_from_matching(gdf_sp, mapping_cont_trips, full_trip_gdf):\n",
    "    \"\"\"This function merges trips that are chained together from the matching done in build_trip_chain_mapping().\n",
    "\n",
    "    Args:\n",
    "        gdf_sp (_type_): GeoDataFrame of start points.\n",
    "        mapping_cont_trips (_type_): Mapping of continued trips. Output of build_trip_chain_mapping().\n",
    "        full_trip_gdf (_type_): GeoDataFrame of all trips.\n",
    "\n",
    "    Returns:\n",
    "        _type_: GeoDataFrame of merged trips.\n",
    "    \"\"\"\n",
    "    # Get trip chain for each trip (Start Point)\n",
    "    print(\"Building trip chains...\")\n",
    "    trip_chains = [getTripChain(trip, mapping_cont_trips, chain=[]) for trip in tqdm(gdf_sp.TRIP_ID)]\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Sort for longest chain first\n",
    "    trip_chains.sort(key = len, reverse = True)\n",
    "\n",
    "    # Create dictionary to store mappings for evaluation\n",
    "    trip_concat_dict = {}\n",
    "\n",
    "    covered_trips = []\n",
    "    merged_trips_gdf = []\n",
    "    print(\"Merging trips...\")\n",
    "    for chain in tqdm(trip_chains, total=len(trip_chains)):\n",
    "        # Check if any of the trips in the current chain has already been merged as part of another chain\n",
    "        # Since we start with the longest chain and iterate through descending sorted list, we only retain the complete chains\n",
    "        if set(chain).intersection(set(covered_trips)):\n",
    "            continue\n",
    "\n",
    "        # add trip chain to dict for evaluation later\n",
    "        trip_concat_dict[chain[0]] = chain[1:]\n",
    "        \n",
    "        # add all trip ids part of current chain to list so that every trip is only contained in longest chain of it\n",
    "        covered_trips += chain\n",
    "\n",
    "        trips = full_trip_gdf.query(\"TRIP_ID in @chain\").sort_values(\"TRIP_START\")\n",
    "        trips[\"temp\"] = 1\n",
    "\n",
    "        trips = trips.groupby('temp').agg(list).reset_index(drop=True).rename(columns={'TRIP_ID': 'TRIP_ID_CHAIN'})\n",
    "\n",
    "        trips[\"wkt_trip\"] = trips['geometry'].apply(lambda x: \", \".join([str(i) for i in x]).replace(\"), LINESTRING (\", \", \"))\n",
    "        trips['TRIP_START'] = trips['TRIP_START'].apply(lambda x: min(x))\n",
    "        trips['TRIP_END'] = trips['TRIP_END'].apply(lambda x: max(x))\n",
    "        trips['TRIP_LEN_IN_MTRS'] = trips['TRIP_LEN_IN_MTRS'].apply(lambda x: sum(x))\n",
    "        #trips['TRIP_DURATION_IN_SECS'] = trips['TRIP_DURATION_IN_SECS'].apply(lambda x: sum(x))\n",
    "        trips['TRIP_WD'] = trips['TRIP_WD'].apply(lambda x: x[0]) # see below\n",
    "        trips['TRIP_DATE'] = trips['TRIP_DATE'].apply(lambda x: x[0]) # see below\n",
    "        trips['TRIP_ID'] = trips['TRIP_ID_CHAIN'].apply(lambda x: x[0]) # assign trip_id of first trip in chain to concatenated trip\n",
    "        # This is the TRIP_ID of the last trip in the chain to be concatenated\n",
    "        trips['TRIP_ID_LAST'] = trips['TRIP_ID_CHAIN'].apply(lambda x: x[-1]) \n",
    "\n",
    "        # Note: Here we are assigning the PERSON_ID of the first trip to the concatenated trip. This of course can be erroneous if the concatenation itself is wrong\n",
    "        trips['PERSON_ID'] = trips['PERSON_ID'].apply(lambda x: x[0])\n",
    "        trips = trips.drop(['geometry', 'TRIP_ID_CHAIN'], axis=1)\n",
    "\n",
    "        trips = gp.GeoDataFrame(trips, geometry=gp.GeoSeries.from_wkt(trips['wkt_trip'])).drop('wkt_trip', axis=1)\n",
    "\n",
    "        merged_trips_gdf.append(trips)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    trip_merged_gdf = pd.concat(merged_trips_gdf)\n",
    "\n",
    "    print(f\"Number of trips that were matched at least once: {len(set(covered_trips))}/{len(set(gdf_sp.TRIP_ID))}\")\n",
    "\n",
    "    # Concatenate all trips that were unmerged with the merged trips into a new gdf\n",
    "    print(\"Concatenating MERGED and UNMERGED trips...\")\n",
    "    unmerged_trips = full_trip_gdf.query(\"TRIP_ID not in @covered_trips\")\n",
    "    full_trips_concat_gdf = pd.concat([unmerged_trips, trip_merged_gdf])\n",
    "    full_trips_concat_gdf['TRIP_ID_FIRST'] = full_trips_concat_gdf['TRIP_ID'] # This is the same as TRIP_ID\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Assign TRIP_ID as TRIP_ID_LAST in case TRIP has not been merged and first and last TRIP_Id are in fact the same\n",
    "    full_trips_concat_gdf['TRIP_ID_LAST'] = np.where(full_trips_concat_gdf.TRIP_ID_LAST.isnull(), full_trips_concat_gdf.TRIP_ID, full_trips_concat_gdf.TRIP_ID_LAST)\n",
    "\n",
    "\n",
    "    return full_trips_concat_gdf.reset_index(drop=True), trip_concat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trip chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4108/4108 [00:00<00:00, 11024.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Merging trips...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4108/4108 [00:33<00:00, 124.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Number of trips that were matched at least once: 4069/4108\n",
      "Concatenating MERGED and UNMERGED trips...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "full_trips_concat_gdf, trip_concat_dict = merge_trips_from_matching(gdf_sp, mapping_cont_trips, full_trip_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for those trip_ids that are still the start of a trip even after the concatenation (of trip chains)\n",
    "t_id_sp = full_trips_concat_gdf.TRIP_ID_FIRST\n",
    "t_id_ep = full_trips_concat_gdf.TRIP_ID_LAST\n",
    "\n",
    "# Also filter dfs that contain points\n",
    "gdf_sp_concat = gdf_sp.query(\"TRIP_ID in @t_id_sp\")\n",
    "trip_sp_gdf_concat = trip_sp_gdf.query(\"TRIP_ID in @t_id_sp\")\n",
    "\n",
    "gdf_ep_concat = gdf_ep.query(\"TRIP_ID in @t_id_ep\")\n",
    "trip_ep_gdf_concat = trip_ep_gdf.query(\"TRIP_ID in @t_id_ep\")\n",
    "\n",
    "assert len(trip_sp_gdf_concat) == len(trip_ep_gdf_concat) == len(gdf_sp_concat) == len(gdf_ep_concat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Clustering after Concatenation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexInList(trip_id, full_trip_gdf):\n",
    "    \"\"\"This function takes in a trip_id and returns the list index of this trip's position in the ground truth clustering.\n",
    "\n",
    "    Args:\n",
    "        trip_id (int): TRIP_ID\n",
    "\n",
    "    Returns:\n",
    "        int: The index of this TRIP_ID in the ground truth clustering vector.\n",
    "    \"\"\"\n",
    "    index_list = full_trip_gdf.sort_values('TRIP_ID').TRIP_ID.to_list()\n",
    "\n",
    "    return index_list.index(trip_id)\n",
    "\n",
    "\n",
    "def build_clustering_after_concatenation(full_trips_concat_gdf, trip_concat_dict):\n",
    "    \"\"\"This function builds the clustering vector after the concatenation step.\n",
    "\n",
    "    Args:\n",
    "        full_trips_concat_gdf (GeoDataFrame): GeoDataFrame containing all trips after the concatenation step.\n",
    "        trip_concat_dict (dict): Dictionary containing the trip chains that were concatenated.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of this TRIP_ID in the ground truth clustering vector.\n",
    "    \"\"\"\n",
    "\n",
    "    # This creates the array with clustering IDs after the concatenation step\n",
    "    clustering_concat = {}\n",
    "    for index, trip in full_trips_concat_gdf.reset_index().sort_values('TRIP_ID').iterrows():\n",
    "        trip_order_index = getIndexInList(trip.TRIP_ID, full_trip_gdf)\n",
    "\n",
    "        clustering_concat[trip_order_index] = index\n",
    "\n",
    "        if trip.TRIP_ID in trip_concat_dict:\n",
    "            for t in trip_concat_dict[trip.TRIP_ID]:\n",
    "                clustering_concat[getIndexInList(t, full_trip_gdf)] = index\n",
    "\n",
    "    clustering_concat = list(dict(sorted(clustering_concat.items())).values())\n",
    "\n",
    "    print(f\"Number of unique clusters: {len(set(clustering_concat))}\")\n",
    "\n",
    "    return clustering_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique clusters: 2656\n"
     ]
    }
   ],
   "source": [
    "clustering_concat = build_clustering_after_concatenation(full_trips_concat_gdf, trip_concat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Home Locations (HL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Start Points (SPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SP-tessellation matching that still contains all SP (and potential HL), and not just the SP and EP of the concatenated trips. We do this, because we do not want loose potential HL contributed of substrip concatenated in a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>TRIP_START</th>\n",
       "      <th>TRIP_END</th>\n",
       "      <th>TRIP_LEN_IN_MTRS</th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>TRIP_WD</th>\n",
       "      <th>TRIP_DATE</th>\n",
       "      <th>hl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>POLYGON ((116.32913 39.97292, 116.32885 39.971...</td>\n",
       "      <td>2746</td>\n",
       "      <td>2008-05-30 06:50:51</td>\n",
       "      <td>2008-05-30 07:41:05</td>\n",
       "      <td>26841.362904</td>\n",
       "      <td>81</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2008-05-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>POLYGON ((116.32913 39.97292, 116.32885 39.971...</td>\n",
       "      <td>17211</td>\n",
       "      <td>2009-04-14 01:13:04</td>\n",
       "      <td>2009-04-14 01:21:54</td>\n",
       "      <td>1095.477838</td>\n",
       "      <td>22</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2009-04-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87</td>\n",
       "      <td>POLYGON ((116.34883 39.92639, 116.34855 39.924...</td>\n",
       "      <td>23443</td>\n",
       "      <td>2009-09-20 15:55:00</td>\n",
       "      <td>2009-09-20 23:52:49</td>\n",
       "      <td>21171.250391</td>\n",
       "      <td>77</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009-09-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>234</td>\n",
       "      <td>POLYGON ((116.31312 39.94624, 116.31284 39.944...</td>\n",
       "      <td>12951</td>\n",
       "      <td>2009-02-14 03:31:24</td>\n",
       "      <td>2009-02-14 09:53:59</td>\n",
       "      <td>57720.799602</td>\n",
       "      <td>23</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2009-02-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>236</td>\n",
       "      <td>POLYGON ((116.30315 39.99269, 116.30287 39.991...</td>\n",
       "      <td>18696</td>\n",
       "      <td>2009-05-10 03:20:28</td>\n",
       "      <td>2009-05-10 03:25:58</td>\n",
       "      <td>294.604029</td>\n",
       "      <td>22</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009-05-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4103</th>\n",
       "      <td>46601</td>\n",
       "      <td>POLYGON ((116.41995 40.04515, 116.41967 40.043...</td>\n",
       "      <td>22339</td>\n",
       "      <td>2009-07-22 11:52:04</td>\n",
       "      <td>2009-07-22 12:05:29</td>\n",
       "      <td>1519.308675</td>\n",
       "      <td>144</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-07-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>46771</td>\n",
       "      <td>POLYGON ((116.42411 39.93782, 116.42382 39.936...</td>\n",
       "      <td>9799</td>\n",
       "      <td>2008-12-10 07:22:33</td>\n",
       "      <td>2008-12-10 10:03:40</td>\n",
       "      <td>21315.514878</td>\n",
       "      <td>4</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2008-12-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4105</th>\n",
       "      <td>46797</td>\n",
       "      <td>POLYGON ((116.44501 40.02048, 116.44473 40.018...</td>\n",
       "      <td>20077</td>\n",
       "      <td>2009-06-07 06:06:03</td>\n",
       "      <td>2009-06-07 06:29:09</td>\n",
       "      <td>7481.792024</td>\n",
       "      <td>144</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009-06-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>46797</td>\n",
       "      <td>POLYGON ((116.44501 40.02048, 116.44473 40.018...</td>\n",
       "      <td>22766</td>\n",
       "      <td>2009-08-09 09:03:22</td>\n",
       "      <td>2009-08-09 09:21:52</td>\n",
       "      <td>1819.083467</td>\n",
       "      <td>144</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009-08-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>46882</td>\n",
       "      <td>POLYGON ((116.33165 39.98753, 116.33137 39.985...</td>\n",
       "      <td>17286</td>\n",
       "      <td>2009-04-15 17:43:05</td>\n",
       "      <td>2009-04-15 18:50:05</td>\n",
       "      <td>5442.437097</td>\n",
       "      <td>43</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-04-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4108 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tile_id                                           geometry  TRIP_ID  \\\n",
       "0         61  POLYGON ((116.32913 39.97292, 116.32885 39.971...     2746   \n",
       "1         61  POLYGON ((116.32913 39.97292, 116.32885 39.971...    17211   \n",
       "2         87  POLYGON ((116.34883 39.92639, 116.34855 39.924...    23443   \n",
       "3        234  POLYGON ((116.31312 39.94624, 116.31284 39.944...    12951   \n",
       "4        236  POLYGON ((116.30315 39.99269, 116.30287 39.991...    18696   \n",
       "...      ...                                                ...      ...   \n",
       "4103   46601  POLYGON ((116.41995 40.04515, 116.41967 40.043...    22339   \n",
       "4104   46771  POLYGON ((116.42411 39.93782, 116.42382 39.936...     9799   \n",
       "4105   46797  POLYGON ((116.44501 40.02048, 116.44473 40.018...    20077   \n",
       "4106   46797  POLYGON ((116.44501 40.02048, 116.44473 40.018...    22766   \n",
       "4107   46882  POLYGON ((116.33165 39.98753, 116.33137 39.985...    17286   \n",
       "\n",
       "               TRIP_START             TRIP_END  TRIP_LEN_IN_MTRS  PERSON_ID  \\\n",
       "0     2008-05-30 06:50:51  2008-05-30 07:41:05      26841.362904         81   \n",
       "1     2009-04-14 01:13:04  2009-04-14 01:21:54       1095.477838         22   \n",
       "2     2009-09-20 15:55:00  2009-09-20 23:52:49      21171.250391         77   \n",
       "3     2009-02-14 03:31:24  2009-02-14 09:53:59      57720.799602         23   \n",
       "4     2009-05-10 03:20:28  2009-05-10 03:25:58        294.604029         22   \n",
       "...                   ...                  ...               ...        ...   \n",
       "4103  2009-07-22 11:52:04  2009-07-22 12:05:29       1519.308675        144   \n",
       "4104  2008-12-10 07:22:33  2008-12-10 10:03:40      21315.514878          4   \n",
       "4105  2009-06-07 06:06:03  2009-06-07 06:29:09       7481.792024        144   \n",
       "4106  2009-08-09 09:03:22  2009-08-09 09:21:52       1819.083467        144   \n",
       "4107  2009-04-15 17:43:05  2009-04-15 18:50:05       5442.437097         43   \n",
       "\n",
       "        TRIP_WD   TRIP_DATE hl  \n",
       "0        Friday  2008-05-30  1  \n",
       "1       Tuesday  2009-04-14  0  \n",
       "2        Sunday  2009-09-20  0  \n",
       "3      Saturday  2009-02-14  0  \n",
       "4        Sunday  2009-05-10  0  \n",
       "...         ...         ... ..  \n",
       "4103  Wednesday  2009-07-22  0  \n",
       "4104  Wednesday  2008-12-10  1  \n",
       "4105     Sunday  2009-06-07  1  \n",
       "4106     Sunday  2009-08-09  1  \n",
       "4107  Wednesday  2009-04-15  0  \n",
       "\n",
       "[4108 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate home locations (HL) from SPs\n",
    "gdf_sp.index=pd.to_datetime(gdf_sp.TRIP_START)\n",
    "gdf_sp['hl'] = gdf_sp['TRIP_START'].apply(lambda x: 1 if x in gdf_sp.between_time('6:00', '10:00').TRIP_START else 0).astype(object)\n",
    "gdf_sp.reset_index(inplace=True, drop=True)\n",
    "gdf_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only those cells that are HL\n",
    "gdf_hl_sp = gdf_sp[gdf_sp['hl'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\libpysal\\weights\\weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 150 disconnected components.\n",
      " There are 92 islands with ids: 4, 15, 20, 21, 25, 59, 95, 107, 109, 123, 124, 132, 137, 138, 142, 154, 162, 163, 166, 169, 205, 206, 207, 210, 212, 213, 218, 234, 235, 286, 287, 288, 293, 356, 357, 360, 363, 364, 372, 373, 391, 394, 402, 405, 448, 449, 451, 452, 453, 457, 462, 500, 502, 522, 526, 532, 537, 540, 545, 548, 563, 604, 607, 614, 618, 621, 643, 646, 647, 653, 655, 658, 668, 669, 673, 679, 680, 681, 684, 685, 686, 697, 810, 831, 834, 835, 838, 839, 840, 843, 846, 856.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# create spatial weights matrix\n",
    "W = libpysal.weights.Queen.from_dataframe(gdf_hl_sp)\n",
    "\n",
    "# get component labels\n",
    "components = W.component_labels\n",
    "\n",
    "gdf_hl_combined_sp = pd.merge(gp.sjoin(\n",
    "    gdf_hl_sp,\n",
    "    gdf_hl_sp.dissolve(by=components)[[\"geometry\"]],\n",
    "    how=\"left\"\n",
    "), gdf_hl_sp.dissolve(by=components)[[\"geometry\"]].reset_index(), left_on=\"index_right\", right_on='index', suffixes=(\"__drop\", \"\")).drop(['index', 'index_right', 'geometry__drop'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute count of unique HL per Peson (HL here is already the merged tiles)\n",
    "gdf_hl_combined_sp = pd.merge(gdf_hl_combined_sp, gdf_hl_combined_sp.astype({'geometry': 'string'}).groupby('PERSON_ID')[['geometry']].nunique().reset_index().rename(columns={'geometry': 'CNT_UNIQUE_HL'}), how=\"left\")\n",
    "\n",
    "# TODO: Add number of trips per merged tile\n",
    "#gdf_hl_combined_sp = pd.merge(gdf_hl_combined_sp, gdf_hl_combined_sp.groupby(['PERSON_ID', 'geometry']).nunique().reset_index()[['PERSON_ID', 'TRIP_ID']].rename(columns={'TRIP_ID': 'CNT_TRIPS_PER_HL'}), suffixes=(\"\", \"_drop\"))\n",
    "\n",
    "# using dictionary to convert specific columns\n",
    "convert_dict = {'PERSON_ID': object,\n",
    "                'CNT_UNIQUE_HL': int\n",
    "                }\n",
    " \n",
    "gdf_hl_combined_sp = gdf_hl_combined_sp.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From End Points (EPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>TRIP_START</th>\n",
       "      <th>TRIP_END</th>\n",
       "      <th>TRIP_LEN_IN_MTRS</th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>TRIP_WD</th>\n",
       "      <th>TRIP_DATE</th>\n",
       "      <th>hl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>POLYGON ((116.32913 39.97292, 116.32885 39.971...</td>\n",
       "      <td>21601</td>\n",
       "      <td>2009-07-07 04:55:30</td>\n",
       "      <td>2009-07-07 05:09:45</td>\n",
       "      <td>1278.079304</td>\n",
       "      <td>22</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2009-07-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>POLYGON ((116.34883 39.92639, 116.34855 39.924...</td>\n",
       "      <td>23442</td>\n",
       "      <td>2009-09-20 12:33:45</td>\n",
       "      <td>2009-09-20 15:39:58</td>\n",
       "      <td>17570.406856</td>\n",
       "      <td>77</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009-09-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87</td>\n",
       "      <td>POLYGON ((116.34883 39.92639, 116.34855 39.924...</td>\n",
       "      <td>4687</td>\n",
       "      <td>2008-08-23 03:30:32</td>\n",
       "      <td>2008-08-23 03:36:52</td>\n",
       "      <td>390.323060</td>\n",
       "      <td>167</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2008-08-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>236</td>\n",
       "      <td>POLYGON ((116.30315 39.99269, 116.30287 39.991...</td>\n",
       "      <td>18695</td>\n",
       "      <td>2009-05-10 02:09:07</td>\n",
       "      <td>2009-05-10 03:01:38</td>\n",
       "      <td>4756.662101</td>\n",
       "      <td>22</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009-05-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275</td>\n",
       "      <td>POLYGON ((116.39991 39.96732, 116.39963 39.965...</td>\n",
       "      <td>9251</td>\n",
       "      <td>2008-12-02 09:38:47</td>\n",
       "      <td>2008-12-02 09:43:33</td>\n",
       "      <td>2591.080868</td>\n",
       "      <td>144</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4103</th>\n",
       "      <td>46601</td>\n",
       "      <td>POLYGON ((116.41995 40.04515, 116.41967 40.043...</td>\n",
       "      <td>22339</td>\n",
       "      <td>2009-07-22 11:52:04</td>\n",
       "      <td>2009-07-22 12:05:29</td>\n",
       "      <td>1519.308675</td>\n",
       "      <td>144</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-07-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>46771</td>\n",
       "      <td>POLYGON ((116.42411 39.93782, 116.42382 39.936...</td>\n",
       "      <td>9798</td>\n",
       "      <td>2008-12-10 06:10:43</td>\n",
       "      <td>2008-12-10 07:07:03</td>\n",
       "      <td>2873.847650</td>\n",
       "      <td>4</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2008-12-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4105</th>\n",
       "      <td>46797</td>\n",
       "      <td>POLYGON ((116.44501 40.02048, 116.44473 40.018...</td>\n",
       "      <td>10876</td>\n",
       "      <td>2009-01-08 01:34:32</td>\n",
       "      <td>2009-01-08 01:45:39</td>\n",
       "      <td>650.239584</td>\n",
       "      <td>144</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>46845</td>\n",
       "      <td>POLYGON ((116.20342 39.90573, 116.20314 39.904...</td>\n",
       "      <td>11636</td>\n",
       "      <td>2009-01-20 04:56:47</td>\n",
       "      <td>2009-01-20 05:02:19</td>\n",
       "      <td>3322.760745</td>\n",
       "      <td>144</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2009-01-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>46882</td>\n",
       "      <td>POLYGON ((116.33165 39.98753, 116.33137 39.985...</td>\n",
       "      <td>17285</td>\n",
       "      <td>2009-04-15 14:38:46</td>\n",
       "      <td>2009-04-15 17:10:40</td>\n",
       "      <td>28919.814553</td>\n",
       "      <td>43</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2009-04-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4108 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tile_id                                           geometry  TRIP_ID  \\\n",
       "0         61  POLYGON ((116.32913 39.97292, 116.32885 39.971...    21601   \n",
       "1         87  POLYGON ((116.34883 39.92639, 116.34855 39.924...    23442   \n",
       "2         87  POLYGON ((116.34883 39.92639, 116.34855 39.924...     4687   \n",
       "3        236  POLYGON ((116.30315 39.99269, 116.30287 39.991...    18695   \n",
       "4        275  POLYGON ((116.39991 39.96732, 116.39963 39.965...     9251   \n",
       "...      ...                                                ...      ...   \n",
       "4103   46601  POLYGON ((116.41995 40.04515, 116.41967 40.043...    22339   \n",
       "4104   46771  POLYGON ((116.42411 39.93782, 116.42382 39.936...     9798   \n",
       "4105   46797  POLYGON ((116.44501 40.02048, 116.44473 40.018...    10876   \n",
       "4106   46845  POLYGON ((116.20342 39.90573, 116.20314 39.904...    11636   \n",
       "4107   46882  POLYGON ((116.33165 39.98753, 116.33137 39.985...    17285   \n",
       "\n",
       "               TRIP_START             TRIP_END  TRIP_LEN_IN_MTRS  PERSON_ID  \\\n",
       "0     2009-07-07 04:55:30  2009-07-07 05:09:45       1278.079304         22   \n",
       "1     2009-09-20 12:33:45  2009-09-20 15:39:58      17570.406856         77   \n",
       "2     2008-08-23 03:30:32  2008-08-23 03:36:52        390.323060        167   \n",
       "3     2009-05-10 02:09:07  2009-05-10 03:01:38       4756.662101         22   \n",
       "4     2008-12-02 09:38:47  2008-12-02 09:43:33       2591.080868        144   \n",
       "...                   ...                  ...               ...        ...   \n",
       "4103  2009-07-22 11:52:04  2009-07-22 12:05:29       1519.308675        144   \n",
       "4104  2008-12-10 06:10:43  2008-12-10 07:07:03       2873.847650          4   \n",
       "4105  2009-01-08 01:34:32  2009-01-08 01:45:39        650.239584        144   \n",
       "4106  2009-01-20 04:56:47  2009-01-20 05:02:19       3322.760745        144   \n",
       "4107  2009-04-15 14:38:46  2009-04-15 17:10:40      28919.814553         43   \n",
       "\n",
       "        TRIP_WD   TRIP_DATE hl  \n",
       "0       Tuesday  2009-07-07  0  \n",
       "1        Sunday  2009-09-20  0  \n",
       "2      Saturday  2008-08-23  0  \n",
       "3        Sunday  2009-05-10  0  \n",
       "4       Tuesday  2008-12-02  0  \n",
       "...         ...         ... ..  \n",
       "4103  Wednesday  2009-07-22  0  \n",
       "4104  Wednesday  2008-12-10  0  \n",
       "4105   Thursday  2009-01-08  0  \n",
       "4106    Tuesday  2009-01-20  0  \n",
       "4107  Wednesday  2009-04-15  0  \n",
       "\n",
       "[4108 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate home locations (HL) from EPs\n",
    "gdf_ep.index=pd.to_datetime(gdf_ep.TRIP_END)\n",
    "gdf_ep['hl'] = gdf_ep['TRIP_END'].apply(lambda x: 1 if x in gdf_ep.between_time('18:00', '00:00').TRIP_END else 0).astype(object)\n",
    "gdf_ep.reset_index(inplace=True, drop=True)\n",
    "gdf_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only those cells that are HL\n",
    "gdf_hl_ep = gdf_ep[gdf_ep['hl'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\libpysal\\weights\\weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 67 disconnected components.\n",
      " There are 43 islands with ids: 1, 2, 3, 4, 9, 12, 32, 37, 44, 49, 87, 94, 223, 224, 229, 230, 238, 247, 248, 250, 253, 254, 265, 266, 284, 320, 326, 328, 345, 355, 356, 360, 366, 367, 368, 369, 372, 376, 397, 398, 400, 402, 412.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "### Merge hl cells that are adjacent (touching) to each other \n",
    "\n",
    "# create spatial weights matrix\n",
    "W = libpysal.weights.Queen.from_dataframe(gdf_hl_ep)\n",
    "\n",
    "# get component labels\n",
    "components = W.component_labels\n",
    "\n",
    "gdf_hl_combined_ep = pd.merge(gp.sjoin(\n",
    "    gdf_hl_ep,\n",
    "    gdf_hl_ep.dissolve(by=components)[[\"geometry\"]],\n",
    "    how=\"left\"\n",
    "), gdf_hl_ep.dissolve(by=components)[[\"geometry\"]].reset_index(), left_on=\"index_right\", right_on='index', suffixes=(\"__drop\", \"\")).drop(['index', 'index_right', 'geometry__drop'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hl_combined_ep = pd.merge(gdf_hl_combined_ep, gdf_hl_combined_ep.astype({'geometry': 'string'}).groupby('PERSON_ID')[['geometry']].nunique().reset_index().rename(columns={'geometry': 'CNT_UNIQUE_HL'}), how=\"left\")\n",
    "\n",
    "# using dictionary to convert specific columns\n",
    "convert_dict = {'PERSON_ID': object,\n",
    "                'CNT_UNIQUE_HL': int\n",
    "                }\n",
    " \n",
    "gdf_hl_combined_ep = gdf_hl_combined_ep.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge (concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_combined = pd.concat([gdf_hl_combined_ep, gdf_hl_combined_sp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\libpysal\\weights\\weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 168 disconnected components.\n",
      " There are 104 islands with ids: 7, 9, 35, 160, 212, 217, 291, 292, 353, 354, 355, 365, 366, 368, 370, 373, 378, 385, 386, 392, 394, 400, 408, 409, 689, 702, 703, 707, 982, 983, 984, 985, 1050, 1051, 1059, 1063, 1064, 1065, 1082, 1086, 1095, 1096, 1097, 1099, 1100, 1106, 1107, 1108, 1109, 1110, 1113, 1114, 1115, 1118, 1157, 1158, 1161, 1162, 1163, 1164, 1165, 1171, 1172, 1177, 1179, 1180, 1181, 1185, 1192, 1193, 1206, 1207, 1208, 1209, 1210, 1211, 1214, 1218, 1219, 1220, 1221, 1227, 1228, 1229, 1230, 1231, 1232, 1236, 1238, 1241, 1243, 1244, 1245, 1246, 1249, 1252, 1259, 1260, 1261, 1262, 1263, 1264, 1266, 1270.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "### Merge hl cells that are adjacent (touching) to each other \n",
    "\n",
    "# create spatial weights matrix\n",
    "W = libpysal.weights.Queen.from_dataframe(gp_combined)\n",
    "\n",
    "# get component labels\n",
    "components = W.component_labels\n",
    "\n",
    "# We need to first join and then merge to first get the right index and then actually join the geometry \n",
    "gp_combined = pd.merge(gp.sjoin(\n",
    "    gp_combined,\n",
    "    gp_combined.dissolve(by=components)[[\"geometry\"]],\n",
    "    how=\"left\"\n",
    "), gp_combined.dissolve(by=components)[[\"geometry\"]].reset_index(), left_on=\"index_right\", right_on='index', suffixes=(\"__drop\", \"\")).drop(['index', 'index_right', 'geometry__drop'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_combined = pd.merge(gp_combined.drop('CNT_UNIQUE_HL', axis=1), gp_combined.astype({'geometry': 'string'}).groupby('PERSON_ID')[['geometry']].nunique().reset_index().rename(columns={'geometry': 'CNT_UNIQUE_HL'}), how=\"left\")\n",
    "\n",
    "# using dictionary to convert specific columns\n",
    "convert_dict = {'PERSON_ID': object,\n",
    "                'CNT_UNIQUE_HL': int\n",
    "                }\n",
    " \n",
    "gp_combined = gp_combined.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Frequency'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the number of unique users per HL tile (count of 1 means that the HL has only trips from one unique user, thus perfect clustering)\n",
    "gp_combined.astype({'geometry':'string'}).groupby(['geometry'])[['PERSON_ID']].nunique().PERSON_ID.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique users for which at least one HL was identified\n",
    "gp_combined.PERSON_ID.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique HL tiles: 168\n"
     ]
    }
   ],
   "source": [
    "# Assign ID to HL\n",
    "gp_combined['HL_ID'] = gp_combined.astype({'geometry': 'string'}).groupby('geometry').ngroup()\n",
    "\n",
    "HL_table = gp_combined[['geometry', 'HL_ID']].drop_duplicates()\n",
    "\n",
    "print(f\"Number of unique HL tiles: {len(HL_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match trips with Home Location tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match concatenated trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all start and enpoints of all trajectories with HL tiles\n",
    "# All successfully matched trips will have 0 in the \"matched_sp/ep\" column else NaN\n",
    "matched_sp = gp.sjoin(\n",
    "    trip_sp_gdf_concat, # This data frame contains all SPs of the trips that are at the end of a concatenated trip (end of a concatenated trip)\n",
    "    gp_combined.dissolve()[['geometry']], # Here we do the matching on the dissolved HL tiles since we only want to have one match per point to detect binary whether it is matched at all or not\n",
    "    how=\"left\"\n",
    ").rename(columns={\"index_right\": \"matched_sp\"})\n",
    "\n",
    "matched_ep = gp.sjoin(\n",
    "    trip_ep_gdf_concat, # This data frame contains all EPs of the trips that are at the end of a concatenated trip (end of a concatenated trip)\n",
    "    gp_combined.dissolve()[['geometry']], # same here, see above\n",
    "    how=\"left\"\n",
    ").rename(columns={\"index_right\": \"matched_ep\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Since we here match with the dissolved tile, we also can at max get ONE match per SP since overlapping HL tiles are dissolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge start and endpoints of all trajectories with HL tiles to get HL_IDs for each trip\n",
    "s = gp.sjoin(\n",
    "    trip_sp_gdf_concat,\n",
    "    HL_table,\n",
    "    how=\"right\").drop('index_left', axis=1).dropna()\n",
    "\n",
    "e = gp.sjoin(\n",
    "    trip_ep_gdf_concat, \n",
    "    HL_table, \n",
    "    how=\"right\").drop('index_left', axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched trajectories (concatenated) that do neither start nor end in a HL tile: 106/2656\n",
      "Number of trajectories (concatenated) that start AND end in a HL tile: 1797/2656\n"
     ]
    }
   ],
   "source": [
    "# Get unmatched start and endpoints\n",
    "unmatched_sp_t_ids = matched_sp[matched_sp.matched_sp.isnull()].TRIP_ID.to_list()\n",
    "unmatched_ep_t_ids = matched_ep[matched_ep.matched_ep.isnull()].TRIP_ID.to_list()\n",
    "\n",
    "# Number of unmatched trajectories that do not start or end in an HL tile\n",
    "nr_unmatched = len(full_trips_concat_gdf.query('TRIP_ID_FIRST in @unmatched_sp_t_ids and TRIP_ID_LAST in @unmatched_ep_t_ids'))\n",
    "print(f\"Number of unmatched trajectories (concatenated) that do neither start nor end in a HL tile: {nr_unmatched}/{len(full_trips_concat_gdf)}\")\n",
    "\n",
    "# Get TRIP_IDs of matched start and endpoints\n",
    "matched_sp_t_ids = matched_sp[~matched_sp.matched_sp.isnull()].TRIP_ID.to_list()\n",
    "matched_ep_t_ids = matched_ep[~matched_ep.matched_ep.isnull()].TRIP_ID.to_list()\n",
    "\n",
    "print(f\"Number of trajectories (concatenated) that start AND end in a HL tile: {len(full_trips_concat_gdf.query('TRIP_ID_FIRST in @matched_sp_t_ids and TRIP_ID_LAST in @matched_ep_t_ids'))}/{len(full_trips_concat_gdf)}\")\n",
    "\n",
    "# check whether number of unmatched trajectories plus number of matched trajectories do line up with the total number of trips in data (in this case concatenated trips)\n",
    "assert (full_trips_concat_gdf.query(\"TRIP_ID_FIRST in @s.TRIP_ID or TRIP_ID_LAST in @e.TRIP_ID\").TRIP_ID.nunique() + nr_unmatched) == len(full_trips_concat_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips that match different HL tiles with their SP and EP: 833\n"
     ]
    }
   ],
   "source": [
    "# Merge matched SP and EP to get the HL_IDs for each trip and drop duplicates.\n",
    "HL_table_se_concat = pd.merge(full_trips_concat_gdf, s[['TRIP_ID', 'HL_ID']], left_on=\"TRIP_ID_FIRST\", right_on=\"TRIP_ID\", how=\"left\")\n",
    "HL_table_se_concat = pd.merge(HL_table_se_concat, e[['TRIP_ID', 'HL_ID']], left_on=\"TRIP_ID_LAST\", right_on=\"TRIP_ID\", how=\"left\").drop(['TRIP_ID_y', 'TRIP_ID'], axis=1).rename(columns={'TRIP_ID_x': 'TRIP_ID'})\n",
    "HL_table_se_concat = HL_table_se_concat[['TRIP_ID', 'HL_ID_x', 'HL_ID_y']].set_index('TRIP_ID').stack().droplevel(1).reset_index().rename(columns={0: 'HL_ID'}).drop_duplicates()\n",
    "\n",
    "# Get trips that match different HL tiles with their SP and EP\n",
    "double_assigned_trips = HL_table_se_concat.groupby('TRIP_ID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Get trips that are not assigned to any HL tile\n",
    "unmatched_trips = full_trips_concat_gdf.query('TRIP_ID_FIRST in @unmatched_sp_t_ids and TRIP_ID_LAST in @unmatched_ep_t_ids')[['TRIP_ID']].reset_index(drop=True)\n",
    "unmatched_trips['HL_ID'] = None # Get same format as HL_table_se_concat\n",
    "\n",
    "#HL_table_se_concat['HL_ID'] = HL_table_se_concat['HL_ID_x'].combine_first(HL_table_se_concat['HL_ID_y'])\n",
    "print(f\"Number of trips that match different HL tiles with their SP and EP: {double_assigned_trips.TRIP_ID.nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trips_concat_gdf.query('TRIP_ID in @double_assigned_trips.TRIP_ID').explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>HL_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2735</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3161</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4012</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9660</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11088</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>26440</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>15791</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>14378</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>25904</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>22766</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1717 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TRIP_ID  HL_ID\n",
       "0        2735  112.0\n",
       "1        3161  154.0\n",
       "2        4012  112.0\n",
       "3        9660   50.0\n",
       "6       11088   47.0\n",
       "...       ...    ...\n",
       "3376    26440   55.0\n",
       "3377    15791   55.0\n",
       "3378    14378  133.0\n",
       "3379    25904  163.0\n",
       "3382    22766  141.0\n",
       "\n",
       "[1717 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HL_table_se_concat.groupby('TRIP_ID').filter(lambda x: len(x) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign double matched trips to one unique HL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all double matched trips and compare them to all other trips that are uniquely assigned in their respective potential HLs that they have been matched with. Then take the HL with the single maximum lcss score between the trip under question and any trip of the assigned HL tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   3 out of  10 | elapsed:    1.1s remaining:    2.7s\n",
      "[Parallel(n_jobs=-2)]: Done   5 out of  10 | elapsed:    1.9s remaining:    1.9s\n",
      "[Parallel(n_jobs=-2)]: Done   7 out of  10 | elapsed:    2.7s remaining:    1.1s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:    8.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Get trips that match only one HL tile with their SP and EP\n",
    "uniquely_assigned_trips = HL_table_se_concat.groupby('TRIP_ID').filter(lambda x: len(x) == 1)\n",
    "\n",
    "def compute_lcss_scores(double_assigned_trip):\n",
    "    t_id = double_assigned_trip.TRIP_ID\n",
    "    hl_id = double_assigned_trip.HL_ID\n",
    "\n",
    "    # Create dict to store results for this trip and HL\n",
    "    result_dict = {t_id: {hl_id: list()}}\n",
    "\n",
    "    # Get trajectory linestring for this trip\n",
    "    trip = full_trips_concat_gdf.query(\"TRIP_ID == @t_id\")\n",
    "\n",
    "    # Get trips that are uniquely assigned to this HL\n",
    "    assigned_trips = uniquely_assigned_trips.query(\"HL_ID == @hl_id\")\n",
    "\n",
    "    # Loop through these trips and calc LCSS scores for each of them\n",
    "    for index, assigned_trip in assigned_trips.iterrows():\n",
    "        assigned_t_id = assigned_trip.TRIP_ID\n",
    "\n",
    "        # Skip the calc for the trip with itself\n",
    "        if assigned_t_id == t_id:\n",
    "            continue\n",
    "        # Get trajectory linestring for this trip (here we use the non-concated one since we are considering S and E points separately to match HL with the concated trips afterwards)\n",
    "        a_trip = full_trips_concat_gdf.query(\"TRIP_ID == @assigned_t_id\")\n",
    "\n",
    "        score = LCSS(trip.geometry, a_trip.geometry)\n",
    "\n",
    "        # save scores in list\n",
    "        result_dict[t_id][hl_id].append(score)\n",
    "    return result_dict\n",
    "\n",
    "parallel_scores = Parallel(n_jobs=-2, verbose=10)(delayed(compute_lcss_scores)(double_assigned_trip) for index, double_assigned_trip in double_assigned_trips.iterrows())\n",
    "\n",
    "# Flatten the list of dicts from parallel processing\n",
    "lcss_scores = {}\n",
    "for idx, trip in enumerate(parallel_scores):\n",
    "    for t_id in trip:\n",
    "\n",
    "        # create dict for this trip if not yet existing (it would if another HL this trip was joined with has already been checked)\n",
    "        if t_id not in lcss_scores:\n",
    "            lcss_scores[t_id] = {}\n",
    "\n",
    "        for hl_id in parallel_scores[idx][t_id]:\n",
    "            # create new list for this HL under the trip key\n",
    "            lcss_scores[t_id][hl_id] = parallel_scores[idx][t_id][hl_id]\n",
    "\n",
    "# Get and compare max scores across all matched HL for a trip and assign the HL with the max value of any trip\n",
    "for trip in lcss_scores:\n",
    "    for key in lcss_scores[trip]:\n",
    "        if len(lcss_scores[trip][key]) > 0:\n",
    "            lcss_scores[trip][key] = max(lcss_scores[trip][key])\n",
    "        else:\n",
    "            lcss_scores[trip][key] = 0\n",
    "    \n",
    "    lcss_scores[trip] = max(lcss_scores[trip], key=lcss_scores[trip].get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m HL_table_se_concat \u001b[39m=\u001b[39m HL_table_se_concat\u001b[39m.\u001b[39mdrop_duplicates([\u001b[39m'\u001b[39m\u001b[39mTRIP_ID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHL_ID\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Assert that the number of trips that are matched to a HL tile plus the the nr of trips that are unmatched is equal to the total number of concatenated trips\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(HL_table_se_concat) \u001b[39m+\u001b[39m nr_unmatched \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(full_trips_concat_gdf)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Combine resolved HL HL_table_se_concat with the unmatched trips to get full HL_table for all trips\u001b[39;00m\n\u001b[0;32m      9\u001b[0m HL_table_trips_concat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([HL_table_se_concat, unmatched_trips], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assign resolved scores to se_HL_lookup table and drop duplicates\n",
    "HL_table_se_concat['HL_ID'] = HL_table_se_concat.apply(lambda x: lcss_scores[x['TRIP_ID']] if x['TRIP_ID'] in lcss_scores else x['HL_ID'], axis=1)\n",
    "HL_table_se_concat = HL_table_se_concat.drop_duplicates(['TRIP_ID', 'HL_ID']).reset_index(drop=True)\n",
    "\n",
    "# Assert that the number of trips that are matched to a HL tile plus the the nr of trips that are unmatched is equal to the total number of concatenated trips\n",
    "assert len(HL_table_se_concat) + nr_unmatched == len(full_trips_concat_gdf)\n",
    "\n",
    "# Combine resolved HL HL_table_se_concat with the unmatched trips to get full HL_table for all trips\n",
    "HL_table_trips_concat = pd.concat([HL_table_se_concat, unmatched_trips], ignore_index=True)\n",
    "HL_table_trips_concat.loc[HL_table_trips_concat.HL_ID.isnull(), 'HL_ID'] = 'no_match' # assign HL_ID 'no_match' to unmatched trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block is only needed if we match the individual trip with HL tiles instead of the concatenated. This for loops finds the best-fitting HL for the concatenated trip, given the unique assignments to HLs available to the first and last trips of this chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in full_trips_concat_gdf.iterrows():\n",
    "    t_id = row['TRIP_ID']\n",
    "    t_id_last = row['TRIP_ID_LAST']\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # check if both SP and EP of trip are matched with HL and contained in se_HL table\n",
    "    if t_id in HL_table_se_concat.TRIP_ID.values and t_id_last in HL_table_se_concat.TRIP_ID.values:\n",
    "        s_hl = HL_table_se_concat.query(\"TRIP_ID == @t_id\").HL_ID.iloc[0]\n",
    "        e_hl = HL_table_se_concat.query(\"TRIP_ID == @t_id_last\").HL_ID.iloc[0]\n",
    "        \n",
    "        if s_hl == e_hl:\n",
    "            row['HL_ID'] = s_hl\n",
    "        else:\n",
    "            # Get trips that are currently assigned to sp HL\n",
    "            assigned_trips = HL_table_se_concat.query(\"HL_ID == @s_hl\")\n",
    "            print(s_hl)\n",
    "            print(e_hl)\n",
    "\n",
    "            scores[s_hl] = []\n",
    "\n",
    "            # Loop through these trips and calc LCSS scores for each of them\n",
    "            for index, assigned_trip in assigned_trips.iterrows():\n",
    "                assigned_t_id = assigned_trip.TRIP_ID\n",
    "                # Skip the calc for the trip with itself\n",
    "                if assigned_t_id == t_id:\n",
    "                    continue\n",
    "\n",
    "                # Get trajectory linestring for this trip (here we use the non-concatenated frame since we are considering SP and EP from individual trips)\n",
    "                a_trip = full_trip_gdf.query(\"TRIP_ID == @assigned_t_id\")\n",
    "\n",
    "                score = LCSS(row.geometry, a_trip.geometry)\n",
    "\n",
    "                # save scores in list\n",
    "                scores[s_hl].append(score)\n",
    "\n",
    "            \n",
    "\n",
    "            # Get trips that are currently assigned to ep HL\n",
    "            assigned_trips = HL_table_se_concat.query(\"HL_ID == @e_hl\")\n",
    "\n",
    "            scores[e_hl] = []\n",
    "\n",
    "            # Loop through these trips and calc LCSS scores for each of them\n",
    "            for index, assigned_trip in assigned_trips.iterrows():\n",
    "                assigned_t_id = assigned_trip.TRIP_ID\n",
    "                # Skip the calc for the trip with itself\n",
    "                if assigned_t_id == t_id_last:\n",
    "                    continue\n",
    "                # Get trajectory linestring for this trip (here we use the non-concatenated frame since we are considering SP and EP from individual trips)\n",
    "                a_trip = full_trip_gdf.query(\"TRIP_ID == @assigned_t_id\")\n",
    "\n",
    "                score = LCSS(row.geometry, a_trip.geometry)\n",
    "\n",
    "                # save scores in list\n",
    "                scores[e_hl].append(score)\n",
    "\n",
    "            print(scores)\n",
    "            \n",
    "\n",
    "            for key in scores:\n",
    "                if len(scores[key]) > 0:\n",
    "                    scores[key] = max(scores[key])\n",
    "                else:\n",
    "                    scores[key] = 0\n",
    "            \n",
    "            # get maximum score across all keys and return corresponding key (HL_ID)\n",
    "            row['HL_ID'] = max(scores, key=scores.get)\n",
    "\n",
    "            print(row['HL_ID'])\n",
    "                \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get trajectories that happened during the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTripOverlaps(gdf):\n",
    "    \"\"\"This function calculates whether two trips overlap in time.\n",
    "\n",
    "    Args:\n",
    "        gdf (_type_): The GeoDataFrame containing the trips.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Dictionary with trip IDs as keys and a list of trip IDs that overlap with the key trip as values.\n",
    "    \"\"\"\n",
    "    def getOverlaps(trip_x):\n",
    "        overlap_dict = {}\n",
    "        overlaps = []\n",
    "        ts_x = pd.to_datetime(trip_x['TRIP_START'], format='%Y-%m-%d %H:%M:%S')\n",
    "        te_x = pd.to_datetime(trip_x['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "        i = 0\n",
    "\n",
    "        for index_y, trip_y in gdf.iterrows():\n",
    "            ts_y = pd.to_datetime(trip_y['TRIP_START'], format='%Y-%m-%d %H:%M:%S')\n",
    "            te_y = pd.to_datetime(trip_y['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            if ts_x <= ts_y and te_x >= ts_y and (trip_x.TRIP_ID != trip_y.TRIP_ID): \n",
    "                overlaps.append(trip_y['TRIP_ID'])\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        overlap_dict[trip_x['TRIP_ID']] = overlaps\n",
    "\n",
    "        return overlap_dict\n",
    "\n",
    "    overlap_dicts = Parallel(n_jobs=-2, verbose=10)(delayed(getOverlaps)(trip_x) for index_x, trip_x in gdf.iterrows())\n",
    "\n",
    "    return {k: v for d in overlap_dicts for k, v in d.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   3 out of  10 | elapsed:    4.8s remaining:   11.3s\n",
      "[Parallel(n_jobs=-2)]: Done   5 out of  10 | elapsed:    6.2s remaining:    6.2s\n",
      "[Parallel(n_jobs=-2)]: Done   7 out of  10 | elapsed:    7.1s remaining:    3.0s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:    7.8s finished\n"
     ]
    }
   ],
   "source": [
    "full_trips_concat_gdf_overlap_dict = getTripOverlaps(full_trips_concat_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clustering after HL assignment step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverlappingTrips(traj_id_list):\n",
    "    \"\"\"This function finds the overlapping trips for a list of trajectory IDs.\n",
    "\n",
    "    Args:\n",
    "        traj_id_list (_type_): List of trajectory IDs.\n",
    "    \"\"\"\n",
    "    overlapping_trips = [item for sublist in [full_trips_concat_gdf_overlap_dict[t] for t in traj_id_list] for item in sublist] # we first get a list of lists and then flatten it\n",
    "    return overlapping_trips\n",
    "\n",
    "def findLargestNonSimultaneousSubset(traj_id_list):\n",
    "    \"\"\"This function finds the largest subset of trajectories that are not simultaneous.\n",
    "\n",
    "    Args:\n",
    "        traj_id_list (list): List of trajectory IDs.\n",
    "\n",
    "    Returns:\n",
    "        list: List of trajectory IDs that are not simultaneous.\n",
    "    \"\"\"\n",
    "    len_traj_id_list = len(traj_id_list)\n",
    "\n",
    "    # We create a list of all possible subsets of the trajectory ID list with decreasing length\n",
    "    # We do this iteratively to not overload the memory\n",
    "    for i in range(len_traj_id_list):\n",
    "        # Create a list of all possible subsets of the trajectory ID list with length len_traj_id_list - i\n",
    "        subsets = list(itertools.combinations(traj_id_list, len_traj_id_list - i))\n",
    "    \n",
    "        # Sort the list by length of the subsets\n",
    "        subsets.sort(key=len, reverse=True)\n",
    "\n",
    "        # Loop through the list of subsets\n",
    "        for subset in subsets:\n",
    "            # get all trips that do overlap in time with any of the trips in subset\n",
    "            overlapping_trips = getOverlappingTrips(subset)\n",
    "\n",
    "            # Check if the subset is not simultaneous\n",
    "            if all([t not in overlapping_trips for t in subset]):\n",
    "                # If so, return the subset as list\n",
    "                return list(subset)\n",
    "\n",
    "    # If no subset is found, return an empty list\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 23/107 [06:20<23:10, 16.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# find the largest subset of trips that are not simultaneous\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m non_simultaneous_subset \u001b[39m=\u001b[39m findLargestNonSimultaneousSubset(HL_table_dict[HL])\n\u001b[0;32m     15\u001b[0m \u001b[39m# assign 'no_match' to all trips that are not part of the largest subset\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m trip \u001b[39min\u001b[39;00m HL_table_dict[HL]:\n",
      "Cell \u001b[1;32mIn[92], line 36\u001b[0m, in \u001b[0;36mfindLargestNonSimultaneousSubset\u001b[1;34m(traj_id_list)\u001b[0m\n\u001b[0;32m     33\u001b[0m         overlapping_trips \u001b[39m=\u001b[39m getOverlappingTrips(subset)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# Check if the subset is not simultaneous\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m([t \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m overlapping_trips \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m subset]):\n\u001b[0;32m     37\u001b[0m             \u001b[39m# If so, return the subset as list\u001b[39;00m\n\u001b[0;32m     38\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(subset)\n\u001b[0;32m     40\u001b[0m \u001b[39m# If no subset is found, return an empty list\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[92], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m         overlapping_trips \u001b[39m=\u001b[39m getOverlappingTrips(subset)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# Check if the subset is not simultaneous\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m([t \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m overlapping_trips \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m subset]):\n\u001b[0;32m     37\u001b[0m             \u001b[39m# If so, return the subset as list\u001b[39;00m\n\u001b[0;32m     38\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(subset)\n\u001b[0;32m     40\u001b[0m \u001b[39m# If no subset is found, return an empty list\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This creates the array with clustering IDs after the concatenation step\n",
    "clustering_after_HL = {}\n",
    "HL_table_dict = (HL_table_trips_concat.groupby('HL_ID')\n",
    "       .apply(lambda x: list(dict(x.TRIP_ID).values()))\n",
    "       .to_dict())\n",
    "\n",
    "for index, HL in tqdm(enumerate(HL_table_dict), total=len(HL_table_dict)):\n",
    "    # Skip HL_ID 'no_match', we will assign a clustering ID to these trips later\n",
    "    if HL == 'no_match':\n",
    "        continue\n",
    "\n",
    "    # find the largest subset of trips that are not simultaneous\n",
    "    non_simultaneous_subset = findLargestNonSimultaneousSubset(HL_table_dict[HL])\n",
    "\n",
    "    # assign 'no_match' to all trips that are not part of the largest subset\n",
    "    for trip in HL_table_dict[HL]:\n",
    "        if trip not in non_simultaneous_subset:\n",
    "            HL_table_dict['no_match'].append(trip)\n",
    "\n",
    "    # Loop through all trips that are assigned to this HL_ID and assign the same clustering ID to all of them\n",
    "    for trip in non_simultaneous_subset:\n",
    "        clustering_after_HL[getIndexInList(trip, full_trip_gdf)] = index\n",
    "\n",
    "        # Check if this trip is a concatenated trip and assign the same clustering ID to all trips that are part of the concatenated trip\n",
    "        if trip in trip_concat_dict:\n",
    "            for t in trip_concat_dict[trip]:\n",
    "                clustering_after_HL[getIndexInList(t, full_trip_gdf)] = index\n",
    "\n",
    "# Assign clustering IDs to all 'no_match' trips (these are the trips that were not successfully assigned to any HL_ID)\n",
    "for index, unm_trip in enumerate(HL_table_dict['no_match']):\n",
    "    clustering_after_HL[getIndexInList(unm_trip, full_trip_gdf)] = index + len(HL_table_dict) - 1 # -1 because we don't want to count the 'no_match' HL_ID in the length of HL_table_dict\n",
    "\n",
    "    # Check if this trip is a concatenated trip and assign the same clustering ID to all trips that are part of the concatenated trip\n",
    "    if unm_trip in trip_concat_dict:\n",
    "        for t in trip_concat_dict[unm_trip]:\n",
    "            clustering_after_HL[getIndexInList(t, full_trip_gdf)] = index + len(HL_table_dict) - 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Trips Without Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compares all trips that were not assigned to any HL_ID with all trips that were assigned to a HL_ID and assigns the clustering ID of the trip with the highest LCSS score above a certain threshold\n",
    "SIM_THRESH_FOR_NO_MATCH = 0.5\n",
    "new_cluster_ids = []\n",
    "for unm_trip in tqdm(HL_table_dict['no_match'], total=len(HL_table_dict['no_match'])):\n",
    "    # compute LCSS for all HL_IDs in clustering_after_HL\n",
    "    print(unm_trip)\n",
    "    scores = {}\n",
    "    for hl_id, trips in HL_table_dict.items():\n",
    "        scores[hl_id] = {}\n",
    "        for t in trips:\n",
    "            if t != unm_trip: # don't compare trip with itself\n",
    "                scores[hl_id][t] = LCSS(full_trips_concat_gdf.query(\"TRIP_ID == @unm_trip\").geometry, full_trips_concat_gdf.query(\"TRIP_ID == @t\").geometry)\n",
    "\n",
    "        # find the trip with the highest LCSS score for current HL_ID\n",
    "        max_score = max(scores[hl_id].values())\n",
    "        scores[hl_id] = max_score\n",
    "\n",
    "    max_score_across_hl_id = max(scores.values())\n",
    "    max_score_hl_id = [k for k, v in scores.items() if v == max_score_across_hl_id]\n",
    "    print(max_score_hl_id) #  in theory there could be two trips of different HL_ids with the same LCSS score. This is why this is a list\n",
    "    print(max_score_across_hl_id)\n",
    "\n",
    "    # throw error if length of max_score_hl_id is greater than 1 and the max_score_across_hl_id is above the threshold\n",
    "    if len(max_score_hl_id) > 1 and max_score_across_hl_id > SIM_THRESH_FOR_NO_MATCH:\n",
    "        raise ValueError(\"There are two HL_IDs with the same LCSS score\")\n",
    "    else:\n",
    "        max_score_hl_id = max_score_hl_id[0]\n",
    "\n",
    "    # if the highest LCSS score is above the threshold and the trips that have the highest LCSS score do not overlap in time\n",
    "    if max_score_across_hl_id > SIM_THRESH_FOR_NO_MATCH and unm_trip not in getOverlappingTrips(HL_table_dict[max_score_hl_id]): \n",
    "        # if hl_id of matched hl_id is 'no_match' then create new cluster for the two trips\n",
    "        if max_score_hl_id == 'no_match':\n",
    "            print(\"no match and assign new cluster id\", max(clustering_after_HL.values()) + 1, \"to trips\", unm_trip, max_score_hl_id)\n",
    "            # create new cluster with ID that is one higher than the highest clustering ID\n",
    "            HL_table_dict[max(clustering_after_HL.values()) + 1] = [unm_trip, max_score_hl_id]\n",
    "            new_cluster_ids.append(max(clustering_after_HL.values()) + 1)\n",
    "\n",
    "        # if hl_id of matched hl_id is not 'no_match' then assign the clustering ID of the matched HL_ID to the trip\n",
    "        else:\n",
    "            print(\"existing match and assign cluster id\", max_score_hl_id, \"to trip\", unm_trip)\n",
    "            # Append the trip to the list of trips that are part of the HL_ID with the highest LCSS score\n",
    "            HL_table_dict[max_score_hl_id].append(unm_trip)\n",
    "\n",
    "            # assign the clustering ID of the HL_ID with the highest LCSS score to the trip\n",
    "            # This overwrites the previous clustering ID of the trip that was assigned in the previous step\n",
    "            clustering_after_HL[getIndexInList(unm_trip)] = clustering_after_HL[getIndexInList(HL_table_dict[max_score_hl_id][0])] # This is a list of length one as asserted above\n",
    "\n",
    "            # Check if this trip is a concatenated trip and assign the same clustering ID to all trips that are part of the concatenated trip\n",
    "            if unm_trip in trip_concat_dict:\n",
    "                for t in trip_concat_dict[unm_trip]:\n",
    "                    clustering_after_HL[getIndexInList(t)] = clustering_after_HL[getIndexInList(HL_table_dict[max_score_hl_id][0])] # This is a list of length one as asserted above\n",
    "    else:\n",
    "        # if trip overlaps in time with other trips in the same HL_ID\n",
    "        if unm_trip in getOverlappingTrips(HL_table_dict[max_score_hl_id]):\n",
    "            print(\"trip overlaps in time with other trips in the same HL_ID\")\n",
    "        \n",
    "        \n",
    "\n",
    "# Assign clustering IDs to all trips that are part of a new cluster\n",
    "for cluster_id in new_cluster_ids:\n",
    "    for trip in HL_table_dict[cluster_id]:\n",
    "        clustering_after_HL[getIndexInList(trip)] = cluster_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = full_trips_concat_gdf.query(\"TRIP_ID == 993106\").explore(color='purple')\n",
    "\n",
    "hl_trips = HL_table_dict[58]\n",
    "\n",
    "full_trips_concat_gdf.query(\"TRIP_ID in @hl_trips\").explore(m = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique clusters: 277\n"
     ]
    }
   ],
   "source": [
    "clustering_HL = list(dict(sorted(clustering_HL.items())).values())\n",
    "\n",
    "print(f\"Number of unique clusters: {len(set(clustering_HL))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering results after concatenation step:\n",
      "\n",
      "Clustering results after HL matching step:\n",
      "Accuracy@1: 0.963\n",
      "Precision: 0.935\n",
      "Recall: 0.950\n",
      "F1: 0.942\n",
      "Homogeneity: 0.961\n",
      "Completeness: 0.737\n",
      "V-measure: 0.834\n",
      "Rand index: 0.981\n",
      "ARI: 0.557\n",
      "MI: 3.560\n",
      "NMI: 0.834\n",
      "AMI: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\Bened\\Documents\\Git\\Master-Thesis\\env\\lib\\site-packages\\geopandas\\geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "print(\"Clustering results after concatenation step:\")\n",
    "#evaluate(clustering_concat)\n",
    "print(\"\\nClustering results after HL matching step:\")\n",
    "evaluate(clustering_HL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "543daf12f525df94f20bbdd448da69881f98a71c963c44c9c7818e0113666227"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

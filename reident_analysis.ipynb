{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e578aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import skmob\n",
    "from skmob.measures.individual import radius_of_gyration, number_of_locations\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from shapely import Point\n",
    "from statannotations.Annotator import Annotator\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'figure.figsize':(10,5), 'figure.dpi':200})\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "freemove = gp.read_file(\"data/freemove/freemove_clustering_tfidf.geojson\", geometry='geometry')\n",
    "freemove_private = gp.read_file(\"data/freemove/freemove_private_clustering_tfidf.geojson\", geometry='geometry')\n",
    "freemove_500tessellation = gp.read_file(\"data/freemove/freemove_500tessellation_clustering_tfidf.geojson\", geometry='geometry')\n",
    "freemove_private_500tessellation = gp.read_file(\"data/freemove/freemove_private_500tessellation_clustering_tfidf.geojson\", geometry='geometry')\n",
    "\n",
    "geolife = gp.read_file(\"data/geolife/geolife_clustering_tfidf.geojson\", geometry='geometry')\n",
    "geolife_private = gp.read_file(\"data/geolife/geolife_private_clustering_tfidf.geojson\", geometry='geometry')\n",
    "geolife_500tessellation = gp.read_file(\"data/geolife/geolife_500tessellation_clustering_tfidf.geojson\", geometry='geometry')\n",
    "geolife_private_500tessellation = gp.read_file(\"data/geolife/geolife_private_500tessellation_clustering_tfidf.geojson\", geometry='geometry')\n",
    "\n",
    "\n",
    "clustering_results = {\n",
    "    'freemove': freemove,\n",
    "    'freemove_private': freemove_private,\n",
    "    'freemove_500tessellation': freemove_500tessellation,\n",
    "    'freemove_private_500tessellation': freemove_private_500tessellation,\n",
    "    'geolife': geolife,\n",
    "    'geolife_private': geolife_private,\n",
    "    'geolife_500tessellation': geolife_500tessellation,\n",
    "    'geolife_private_500tessellation': geolife_private_500tessellation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b1eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.847\n",
      "Completeness: 0.511\n",
      "V-measure: 0.638\n",
      "Rand index: 0.968\n",
      "ARI: 0.266\n",
      "MI: 3.134\n",
      "NMI: 0.638\n",
      "AMI: 0.421\n",
      "Cluster accuracy: 0.317\n"
     ]
    }
   ],
   "source": [
    "attack.evaluate(geolife.clustering_HL.tolist(), geolife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next dataset:  freemove\n",
      "Nr of users to evaluate:  67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8858f1d8e8a47f0a5f87b7c46ba94a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of users to evaluate:  54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3a52e8df5f49f7a7759ee9c706a399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_perf_scores(clustering_result_data, uid, n_points):\n",
    "    # get for random points from this user's trajectories\n",
    "    coords = [list(x.coords) for x in clustering_result_data.query('PERSON_ID == @uid').geometry.tolist()]\n",
    "    coords = [item for sublist in coords for item in sublist]\n",
    "    rand_points = random.choices(coords, k=n_points)\n",
    "\n",
    "    # find all trajectories where these points are part of and get unique cluster ids\n",
    "    clustering_result_data['contains_rand_points'] = clustering_result_data.geometry.apply(lambda x: 1 if len(set(x.coords).intersection(set(rand_points))) > 0 else 0)\n",
    "    cluster_ids = clustering_result_data.query('contains_rand_points == 1').clustering_HL.unique()\n",
    "\n",
    "    # get trips of cluster and the true nr of trips of this user\n",
    "    cluster_trips = clustering_result_data.query('clustering_HL in @cluster_ids')\n",
    "    n_cluster_trips = len(cluster_trips)\n",
    "    n_cluster_user_trips = len(cluster_trips.query('PERSON_ID == @uid'))\n",
    "    n_user_trips = len(clustering_result_data.query('PERSON_ID == @uid'))\n",
    "\n",
    "    # calculate precision, recall, mean\n",
    "    precision = n_cluster_user_trips/n_cluster_trips\n",
    "    recall = n_cluster_user_trips/n_user_trips\n",
    "    pr_mean = (precision + recall)/2\n",
    "    f_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "    return precision, recall, pr_mean, f_score\n",
    "\n",
    "\n",
    "def evaluate_attack(clustering_result_data, n_draws_per_user, nr_points):\n",
    "    eval_scores = {'user_id': [], 'precision': [], 'recall': [], 'pr_mean': [], 'f_score': [], 'nr_points': []}\n",
    "    \n",
    "    # Find user ids that have at least N_POINTS + 1 trips\n",
    "    user_ids = clustering_result_data.groupby('PERSON_ID').TRIP_ID.nunique().reset_index().query('TRIP_ID > @nr_points').PERSON_ID.unique()\n",
    "    print('Nr of users to evaluate: ', user_ids.size)\n",
    "    \n",
    "    # only evaluate user that have at least N_POINTS/2 + 1 trips\n",
    "    for uid in tqdm(user_ids):\n",
    "    \n",
    "        precision_scores, recall_scores, pr_mean_scores, f_scores = [], [], [], []\n",
    "        for i in range(n_draws_per_user):\n",
    "            p, r, m, f = compute_perf_scores(clustering_result_data, uid, nr_points)\n",
    "            precision_scores.append(p)\n",
    "            recall_scores.append(r)\n",
    "            pr_mean_scores.append(m)\n",
    "            f_scores.append(f)\n",
    "\n",
    "        eval_scores['user_id'].extend([uid] * len(precision_scores))\n",
    "        eval_scores['precision'].extend(precision_scores)\n",
    "        eval_scores['recall'].extend(recall_scores)\n",
    "        eval_scores['pr_mean'].extend(pr_mean_scores)\n",
    "        eval_scores['f_score'].extend(f_scores)\n",
    "        eval_scores['nr_points'].extend([nr_points] * len(precision_scores))\n",
    "\n",
    "    return pd.DataFrame.from_dict(eval_scores)\n",
    "\n",
    "scores = []\n",
    "for name, data in clustering_results.items():\n",
    "    print(\"Next dataset: \", name)\n",
    "    scores_points = []\n",
    "    for nr_p in [1,4,10]:\n",
    "        scores_points.append(evaluate_attack(data, n_draws_per_user=100, nr_points=nr_p))\n",
    "    scores.append(pd.concat(scores_points))\n",
    "    \n",
    "# scores = pd.concat(scores, keys=list(clustering_results.keys())).reset_index(0, names=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b64c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat(scores, keys=list(clustering_results.keys())).reset_index(0, names=\"data\")\n",
    "\n",
    "scores.to_csv('reident_scores.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd5dca",
   "metadata": {},
   "source": [
    "# Constructing Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3767e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(gdf, tessellation):\n",
    "    gdf = gdf.to_crs(4326)\n",
    "    gdf['START_POINT'] = gdf.geometry.apply(lambda x: Point(x.coords[0]))\n",
    "    gdf['END_POINT'] = gdf.geometry.apply(lambda x: Point(x.coords[-1]))\n",
    "    \n",
    "    sp = gdf[['PERSON_ID', 'TRIP_ID', 'TRIP_START', 'START_POINT', 'clustering_HL']].copy()\n",
    "    ep = gdf[['PERSON_ID', 'TRIP_ID', 'TRIP_END', 'END_POINT', 'clustering_HL']].copy()\n",
    "\n",
    "\n",
    "    sp['lat'] = sp.START_POINT.apply(lambda x: x.y)\n",
    "    sp['lng'] = sp.START_POINT.apply(lambda x: x.x)\n",
    "    ep['lat'] = ep.END_POINT.apply(lambda x: x.y)\n",
    "    ep['lng'] = ep.END_POINT.apply(lambda x: x.x)\n",
    "\n",
    "    sp = sp.rename(columns={'TRIP_START': 'datetime'})\n",
    "    ep = ep.rename(columns={'TRIP_END': 'datetime'})\n",
    "\n",
    "    sp.drop('START_POINT', axis=1, inplace=True)\n",
    "    ep.drop('END_POINT', axis=1, inplace=True)\n",
    "\n",
    "    points = pd.concat([sp, ep])\n",
    "    points = gp.GeoDataFrame(points, geometry=gp.points_from_xy(points.lng, points.lat, crs='epsg:4326'))\n",
    "    \n",
    "    points = gp.sjoin(points, tessellation, predicate='within', how='left').drop('index_right', axis=1)\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "def get_location_entropy(tile_id, mapped_points_gdf):\n",
    "    assert isinstance(tile_id, str)\n",
    "    t_trips = mapped_points_gdf.query(\"tile_id == @tile_id\")\n",
    "    le = 0\n",
    "    c_l = len(t_trips)\n",
    "    for p in t_trips.clustering_HL:\n",
    "        c_l_u = len(t_trips.query('clustering_HL == @p'))\n",
    "        p_u_l =  c_l_u/ c_l\n",
    "        le += p_u_l * math.log(p_u_l)\n",
    "    return -le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5fd74874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freemove\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30dd0690eedf4a5c92d65ee074ae99c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freemove_private\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833fca11ad4b43eabea4dbf854b5936d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freemove_500tessellation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5d6885573b4b2b9d4480766c3400c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freemove_private_500tessellation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302a4c43200049ab8e1992791c25f99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entropy freemove\n",
    "i = 0\n",
    "tessellation_freemove = gp.read_file(\"data/freemove/tessellation_freemove_500.geojson\", geometry='geometry').to_crs(4326)\n",
    "for name, data in clustering_results.items():\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(name)\n",
    "    \n",
    "    # Get random entropy\n",
    "    re = get_points(data, tessellation_freemove).groupby('PERSON_ID').apply(lambda g: math.log2(g.tile_id.nunique())).reset_index().rename(columns={0: 'rand_entropy', 'PERSON_ID': 'user_id'})\n",
    "    scores[i] = pd.merge(scores[i], re, how='left')\n",
    "    \n",
    "    # Get average location entropy\n",
    "    mapped_points_gdf = get_points(data, tessellation_freemove)\n",
    "    mapped_points_gdf['ale'] = mapped_points_gdf.tile_id.progress_apply(lambda x: get_location_entropy(x, mapped_points_gdf))\n",
    "    scores[i] = pd.merge(scores[i], mapped_points_gdf[['PERSON_ID', 'ale']].groupby('PERSON_ID').mean().reset_index().rename(columns={'PERSON_ID': 'user_id'}), how='left')\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "868c712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy GeoLife\n",
    "i = 0\n",
    "tessellation_geolife = gp.read_file(\"data/geolife/tessellation_geolife_500.geojson\", geometry='geometry').to_crs(4326)\n",
    "for name, data in clustering_results.items():\n",
    "    if i < 4:\n",
    "        i += 1\n",
    "        continue\n",
    "    print(name)\n",
    "    # Get random entropy\n",
    "    re = get_points(data, tessellation_geolife).groupby('PERSON_ID').apply(lambda g: math.log2(g.tile_id.nunique())).reset_index().rename(columns={0: 'rand_entropy', 'PERSON_ID': 'user_id'})\n",
    "    scores[i] = pd.merge(scores[i], re, how='left')\n",
    "    \n",
    "    # Get average location entropy\n",
    "    mapped_points_gdf = get_points(data, tessellation_geolife)\n",
    "    mapped_points_gdf['ale'] = mapped_points_gdf.tile_id.progress_apply(lambda x: get_location_entropy(x, mapped_points_gdf))\n",
    "    scores[i] = pd.merge(scores[i], mapped_points_gdf[['PERSON_ID', 'ale']].groupby('PERSON_ID').mean().reset_index().rename(columns={'PERSON_ID': 'user_id'}), how='left')\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_trips per user\n",
    "for index, (name, data) in enumerate(clustering_results.items()):\n",
    "    print(name)\n",
    "    scores[index] = pd.merge(scores[index], data.groupby('PERSON_ID').TRIP_ID.nunique().reset_index().rename(columns={'TRIP_ID': 'n_trips', 'PERSON_ID': 'user_id'}), how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30ba26b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dec796d6da45be8a90dc34866af8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Radius of gyration freemove\n",
    "freemove_raw_points = gp.read_file(\"data/freemove/freemove_raw_point.geojson\", geometry='geometry')\n",
    "freemove_raw_points = freemove_raw_points.to_crs(4326)\n",
    "freemove_raw_points['lon'] = freemove_raw_points.geometry.apply(lambda x: x.coords[0][0])\n",
    "freemove_raw_points['lat'] = freemove_raw_points.geometry.apply(lambda x: x.coords[0][1])\n",
    "tdf_freemove = skmob.TrajDataFrame(freemove_raw_points, latitude='lat', longitude='lon', datetime='time', user_id='user')\n",
    "rg = radius_of_gyration(tdf_freemove)\n",
    "\n",
    "for data in scores[:4]:\n",
    "    data = pd.merge(data, rg.rename(columns={'uid': 'user_id'}), how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a43fa0b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tdf_geolife' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Radius of gyration freemove\u001b[39;00m\n\u001b[1;32m      2\u001b[0m geolife_raw_points \u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mread_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/geolife/geolife_raw_point.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m, geometry\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m geolife_raw_points \u001b[38;5;241m=\u001b[39m \u001b[43mtdf_geolife\u001b[49m\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;241m4326\u001b[39m)\n\u001b[1;32m      4\u001b[0m geolife_raw_points[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m geolife_raw_points\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mcoords[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m geolife_raw_points[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m geolife_raw_points\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mcoords[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tdf_geolife' is not defined"
     ]
    }
   ],
   "source": [
    "# Radius of gyration freemove\n",
    "geolife_raw_points = gp.read_file(\"data/geolife/geolife_raw_point.geojson\", geometry='geometry')\n",
    "geolife_raw_points = geolife_raw_points.to_crs(4326)\n",
    "geolife_raw_points['lon'] = geolife_raw_points.geometry.apply(lambda x: x.coords[0][0])\n",
    "geolife_raw_points['lat'] = geolife_raw_points.geometry.apply(lambda x: x.coords[0][1])\n",
    "tdf_geolife = skmob.TrajDataFrame(geolife_raw_points, latitude='lat', longitude='lon', datetime='time', user_id='user')\n",
    "rg = radius_of_gyration(tdf_geolife)\n",
    "\n",
    "# for data in scores[4:]:\n",
    "#     data = pd.merge(data, rg.rename(columns={'uid': 'user_id'}), how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22606cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0,1,2,3,4,5,6,7][4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean hour of day \n",
    "for index, (name, data) in enumerate(clustering_results.items()):\n",
    "    print(name)\n",
    "    data['hour_of_day'] = pd.to_datetime(data.TRIP_START).dt.hour - 1\n",
    "    scores[index] = pd.merge(scores[index], data[['PERSON_ID', 'hour_of_day']].groupby('PERSON_ID').mean().reset_index().rename(columns={'PERSON_ID': 'user_id'}), how='left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac95068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import attack\n",
    "import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff41bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import libpysal\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import Point\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "sys.stdout = open('logfile', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6af6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "gdf = gp.read_file('Experiments/Clustering_assessment_heuristic/full_trip_HL_clustering.geojson').to_crs(4326)\n",
    "\n",
    "# format clusering_HL columns as object\n",
    "gdf['clustering_HL'] = gdf['clustering_HL'].astype('object')\n",
    "\n",
    "gdf['START_POINT'] = gdf.geometry.apply(lambda x: Point(x.coords[0]))\n",
    "gdf['END_POINT'] = gdf.geometry.apply(lambda x: Point(x.coords[-1]))\n",
    "\n",
    "gdf = gdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8259521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(gdf, tessellation):\n",
    "    sp = gdf[['PERSON_ID', 'TRIP_ID', 'TRIP_START', 'START_POINT', 'clustering_HL']].copy()\n",
    "    ep = gdf[['PERSON_ID', 'TRIP_ID', 'TRIP_END', 'END_POINT', 'clustering_HL']].copy()\n",
    "\n",
    "\n",
    "    sp['lat'] = sp.START_POINT.apply(lambda x: x.y)\n",
    "    sp['lng'] = sp.START_POINT.apply(lambda x: x.x)\n",
    "    ep['lat'] = ep.END_POINT.apply(lambda x: x.y)\n",
    "    ep['lng'] = ep.END_POINT.apply(lambda x: x.x)\n",
    "\n",
    "    sp = sp.rename(columns={'TRIP_START': 'datetime'})\n",
    "    ep = ep.rename(columns={'TRIP_END': 'datetime'})\n",
    "\n",
    "    sp.drop('START_POINT', axis=1, inplace=True)\n",
    "    ep.drop('END_POINT', axis=1, inplace=True)\n",
    "\n",
    "    points = pd.concat([sp, ep])\n",
    "    points = gp.GeoDataFrame(points, geometry=gp.points_from_xy(points.lng, points.lat, crs='epsg:4326'))\n",
    "    \n",
    "    points = gp.sjoin(points, tessellation, predicate='within', how='left').drop('index_right', axis=1)\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be91c235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/skmob/tessellation/tilers.py:117: ShapelyDeprecationWarning: The 'cascaded_union()' function is deprecated. Use 'unary_union()' instead.\n",
      "  base_shape = gpd.GeoSeries(cascaded_union(polygons), crs=base_shape.crs)\n"
     ]
    }
   ],
   "source": [
    "from skmob.tessellation import tilers\n",
    "import skmob\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "MIN_LNG, MIN_LAT, MAX_LNG, MAX_LAT = 12.562133, 52.099718, 14.129426, 52.803108  # raw_points_gdf.geometry.total_bounds\n",
    "\n",
    "TILE_DIAMETER_IN_METERS = (\n",
    "    800  # approximately. For h3 the most appropriate resolution is found\n",
    ")\n",
    "TILE_TYPE = \"squared\"  # other option: \"squared\", \"h3_tessellation\"\n",
    "####################################################################\n",
    "\n",
    "base_shape = gp.GeoDataFrame(\n",
    "    index=[0],\n",
    "    crs=4326,\n",
    "    geometry=[\n",
    "        Polygon(\n",
    "            zip(\n",
    "                [MIN_LNG, MAX_LNG, MAX_LNG, MIN_LNG],\n",
    "                [MIN_LAT, MIN_LAT, MAX_LAT, MAX_LAT],\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "tessellation = tilers.tiler.get(\n",
    "    TILE_TYPE,\n",
    "    base_shape=base_shape,\n",
    "    meters=TILE_DIAMETER_IN_METERS,\n",
    ")#.drop('H3_INDEX', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dad92d",
   "metadata": {},
   "source": [
    "print(len(tessellation))\n",
    "print('nr unique', tessellation.tile_ID.nunique())\n",
    "\n",
    "tessellation.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a4dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_location_entropy(tile_id, mapped_points_gdf):\n",
    "    assert isinstance(tile_id, str)\n",
    "    t_trips = mapped_points_gdf.query(\"tile_ID == @tile_id\")\n",
    "    le = 0\n",
    "    c_l = len(t_trips)\n",
    "    for p in t_trips.clustering_HL:\n",
    "        c_l_u = len(t_trips.query('clustering_HL == @p'))\n",
    "        p_u_l =  c_l_u/ c_l\n",
    "        le += p_u_l * math.log(p_u_l)\n",
    "    return -le\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2befdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_loc_frequency(row, loc_frequencies, points):\n",
    "    \n",
    "    tl_ID = str(row.tile_ID)\n",
    "    cl_ID = row.clustering_HL\n",
    "    \n",
    "    cl_loc_freqs = loc_frequencies.query('clustering_HL == @cl_ID')\n",
    "        \n",
    "    tf = row.freq/cl_loc_freqs.freq.sum()\n",
    "    \n",
    "    idf = math.log(points.clustering_HL.nunique()/(points.query('tile_ID == @tl_ID').clustering_HL.nunique() + 1))\n",
    "    \n",
    "    tf_idf = tf * idf\n",
    "    \n",
    "    return tf_idf\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6283a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_similarity(cl_i, cl_j, loc_frequencies, points):\n",
    "    overlap_tile_ids = set(loc_frequencies.query(\"clustering_HL == @cl_i\").tile_ID).intersection(set(loc_frequencies.query(\"clustering_HL == @cl_j\").tile_ID))\n",
    "    nr_overlapping_tiles = len(overlap_tile_ids)\n",
    "    loc_sim = 0\n",
    "    \n",
    "    if nr_overlapping_tiles == 0:\n",
    "        return loc_sim\n",
    "    for t in overlap_tile_ids:\n",
    "        cl_i_freq = loc_frequencies[(loc_frequencies['clustering_HL'] == cl_i) & (loc_frequencies['tile_ID'] == t)].tf_idf.iloc[0]\n",
    "        cl_j_freq = loc_frequencies[(loc_frequencies['clustering_HL'] == cl_j) & (loc_frequencies['tile_ID'] == t)].tf_idf.iloc[0]\n",
    "        \n",
    "        loc_sim += points.query(\"tile_ID == @t\").location_entropy.iloc[0] * min(cl_i_freq, cl_j_freq)\n",
    "    return loc_sim/nr_overlapping_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5561fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_n(M, n=10):\n",
    "    # (score, i, j)\n",
    "    max_n = [(0,0,0)] * n\n",
    "    \n",
    "    for i in range(M.shape[0]):\n",
    "        for j in range(M.shape[1]):\n",
    "            if not all(x[0] > M[i,j] for x in max_n):\n",
    "                index_smallest_val = max_n.index(sorted(max_n, reverse=False)[0])\n",
    "                max_n[index_smallest_val] = (M[i,j], i, j) # score must be first for sorting later\n",
    "    \n",
    "    print('Nr of links that are equal to one of max values: ', sum([sum(M == max_val[0]).sum() for max_val in max_n]))\n",
    "    return max_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0af3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nr_overlaps(cl_i, cl_j, gdf):\n",
    "    nr_overlaps = 0\n",
    "    \n",
    "    gdf['TRIP_START'] = pd.to_datetime(gdf['TRIP_START'], format='%Y-%m-%d %H:%M:%S')\n",
    "    gdf['TRIP_END'] = pd.to_datetime(gdf['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    cl_i_trips = gdf.query(\"clustering_HL == @cl_i\").copy()\n",
    "    cl_j_trips = gdf.query(\"clustering_HL == @cl_j\").copy()\n",
    "    \n",
    "    for i, t_i in cl_i_trips.iterrows():\n",
    "        t_i_s_x = t_i\n",
    "        t_i_e_x = pd.to_datetime(t_i['TRIP_END'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        for i, t_j in cl_j_trips.iterrows():\n",
    "            if ((t_i.TRIP_START <= t_j.TRIP_START and t_i.TRIP_END >= t_j.TRIP_START) or (t_j.TRIP_START <= t_i.TRIP_START and t_j.TRIP_END >= t_i.TRIP_START))  and (t_i.TRIP_ID != t_j.TRIP_ID):\n",
    "                print('Overlapping trips in two clusters:', cl_i, cl_j, 'Trips: ', t_i.TRIP_ID, t_j.TRIP_ID)\n",
    "                nr_overlaps += 1\n",
    "                \n",
    "    return nr_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1621646",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl_id in gdf.clustering_HL.unique():\n",
    "    if find_nr_overlaps(cl_id,cl_id, gdf) > 0:\n",
    "        print(cl_id)\n",
    "        print(\"error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f0adacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results_recursive_search(clustering, gdf):\n",
    "    ground_truth = attack.getGroundTruth(gdf)\n",
    "    \n",
    "    # Write all clustering metrics of evaluate() to csv and add columns for parameters\n",
    "    result_dicts = []\n",
    "    result_dict = {}\n",
    "    result_dict['Homogeneity'] = metrics.homogeneity_score(ground_truth, clustering)\n",
    "    result_dict['Completeness'] = metrics.completeness_score(ground_truth, clustering)\n",
    "    result_dict['V-measure'] = metrics.v_measure_score(ground_truth, clustering)\n",
    "    result_dict['Rand index'] = metrics.rand_score(ground_truth, clustering)\n",
    "    result_dict['ARI'] = metrics.adjusted_rand_score(ground_truth, clustering)\n",
    "    result_dict['MI'] = metrics.mutual_info_score(ground_truth, clustering)\n",
    "    result_dict['NMI'] = metrics.normalized_mutual_info_score(ground_truth, clustering)\n",
    "    result_dict['AMI'] = metrics.adjusted_mutual_info_score(ground_truth, clustering)\n",
    "    result_dict['Cluster accuracy'] = attack.cluster_acc(ground_truth, clustering)\n",
    "    result_dicts.append(result_dict)\n",
    "\n",
    "    df = pd.DataFrame(result_dicts)\n",
    "\n",
    "    # Add column with date and time\n",
    "    df['Date'] = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    df['Time'] = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile('results_recursive_search.csv')\n",
    "\n",
    "    # Write to csv (append)\n",
    "    if not file_exists:\n",
    "        df.to_csv('results_recursive_search.csv', mode='a', header=True, index=False)\n",
    "    else:\n",
    "        df.to_csv('results_recursive_search.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8fe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▎                                                                 | 493/2750 [00:12<01:32, 24.35it/s]"
     ]
    }
   ],
   "source": [
    "iteration = 1\n",
    "\n",
    "while True:\n",
    "    print('\\nNext Iteration: ', iteration)\n",
    "    # Get new points gdf matched with tessellation that contains updated clustering column\n",
    "    points = get_points(gdf, tessellation)\n",
    "    \n",
    "    # Compute location entropy for latest clustering\n",
    "    points['location_entropy'] = points.tile_ID.progress_apply(lambda x: get_location_entropy(x, points))\n",
    "    points['location_entropy'] = points.location_entropy.apply(lambda x: 1 - (x - points.location_entropy.min())/(points.location_entropy.max() - points.location_entropy.min()))\n",
    "    points.location_entropy.plot.hist(bins=30, title=\"Inverse location_entropy distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Update location frequencies and loc frequency-inverse cluster frequency\n",
    "    loc_frequencies = points.groupby(['clustering_HL', 'tile_ID'])[['TRIP_ID']].nunique().reset_index().sort_values('clustering_HL').rename(columns={'TRIP_ID': 'freq'})\n",
    "    loc_frequencies['tf_idf'] = loc_frequencies.apply(lambda x: get_inverse_loc_frequency(x, loc_frequencies, points), axis=1)\n",
    "    loc_frequencies.tf_idf.plot.hist(bins=50, title=\"Tf-Idf distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Recalculate similarities between latest clusters\n",
    "    M = np.zeros((loc_frequencies.clustering_HL.nunique(), loc_frequencies.clustering_HL.nunique()))\n",
    "    for i, cl_i in tqdm(enumerate(loc_frequencies.clustering_HL.unique()), total=loc_frequencies.clustering_HL.nunique()):\n",
    "        for j, cl_j in enumerate(loc_frequencies.clustering_HL.unique()):\n",
    "            if i == j:\n",
    "                continue\n",
    "            M[i, j] = get_location_similarity(cl_i, cl_j, loc_frequencies, points)\n",
    "    np.fill_diagonal(M, 0)\n",
    "    \n",
    "    # Select n best links and assign new clustering IDs\n",
    "    print('New links of clusters:')\n",
    "    max_n =  get_max_n(M)\n",
    "    # Adjust scores based on overlapping trips in the two candidate clusters\n",
    "    print(max_n)\n",
    "    max_n = [(t[0] * 1/(find_nr_overlaps(loc_frequencies.clustering_HL.unique()[t[1]], loc_frequencies.clustering_HL.unique()[t[2]], gdf) + 1), t[1], t[2]) for t in max_n]\n",
    "    print(max_n)\n",
    "    \n",
    "    # Take the 5 best fitting clusters\n",
    "    for i, tup in enumerate(sorted(max_n, reverse=True)[:5]):\n",
    "        cl_1 = loc_frequencies.clustering_HL.unique()[tup[1]]\n",
    "        cl_2 = loc_frequencies.clustering_HL.unique()[tup[2]]\n",
    "        print(cl_1, cl_2)\n",
    "        # Assign new clustering ids (merge clustering ids)\n",
    "        gdf['clustering_HL'] = gdf.clustering_HL.apply(lambda x: cl_1 if x == cl_2 else x)\n",
    "    \n",
    "    # Evaluate result of iteration\n",
    "    attack.evaluate(gdf.clustering_HL.tolist(), gdf)\n",
    "    store_results_recursive_search(gdf.clustering_HL.tolist(), gdf)\n",
    "    \n",
    "    # Check break condition\n",
    "    if gdf.clustering_HL.nunique() <= 72:\n",
    "        print(\"K clusters reached! Done!\")\n",
    "        break\n",
    "        \n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ebe1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
